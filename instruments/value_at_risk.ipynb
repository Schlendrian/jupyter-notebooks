{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvacon\n",
    "import pyvacon.marketdata.testdata as mkt_testdata\n",
    "import pyvacon.tools.enums as enums\n",
    "import pyvacon.marketdata.plot as mkt_plot\n",
    "import pyvacon.models.plot as model_plot\n",
    "import pyvacon.models.tools as model_tools\n",
    "import pyvacon.analytics as analytics\n",
    "import pyvacon.tools.converter as converter\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch, Rectangle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.transforms as mtransforms\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime as dt\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# use ipynb to import function definitions from another notebook\n",
    "import ipynb\n",
    "from ipynb.fs.defs.ir_shock_scenarios import getShockedDiscountCurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# We define some constants which we'll use repeatedly throughout this notebook.\n",
    "notebook_is_draft = True\n",
    "color_main = 'tab:blue'\n",
    "color_highlight = 'tab:orange'\n",
    "color_graphblue = \"#4e79a7\"\n",
    "color_histmarker = \"#4e79a7\"\n",
    "color_histmarkerborder = \"White\"\n",
    "grid_alpha = 0.4\n",
    "default_daycounter_type = enums.DayCounter.ACTACT\n",
    "default_interpolation_type = enums.InterpolationType.LINEAR\n",
    "default_extrapolation_type = enums.ExtrapolationType.NONE\n",
    "default_plotly_scatter_mode = 'lines'\n",
    "if notebook_is_draft:\n",
    "    default_sample_size_MC = 100\n",
    "else:\n",
    "    default_sample_size_MC = 10000\n",
    "refdate = dt.datetime(year = 2019, month = 12, day = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Same for functions\n",
    "def get_default_title_dict(title_text):\n",
    "    return dict(\n",
    "        text = title_text,\n",
    "        y = 0.9,\n",
    "        x = 0.475,\n",
    "        xanchor = 'center',\n",
    "        yanchor = 'top')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## Value at risk\n",
    "Value at risk (VaR) is a measure for the risk in a portfolio of financial assets. Given a time horizon of $n$ days and a confidence level $\\alpha$, the VaR is the loss of value, which has the probability $\\alpha$ not to be exceeded within the next $n$ days. In other words, the VaR is the $\\alpha$-quantile of the distribution of loss in the value of a portfolio other the next $n$ days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different methods for estimating the value at risk can be put into two major categories: Those using analytical models and those using simulations.\n",
    "\n",
    "The goal of **analytical** methods is to define a probability distribution, which approximates the actual probability distribution of the portfolio value. One can then write down a closed formula for the value at risk.\n",
    "\n",
    "**Simulation**-based methods simulate the change in value over the next $n$ days and use the resulting relative frequency distribution to 'read off' the value at risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical simulation\n",
    "A very popular way of simulating changes in value uses past market data to estimate what will happen in the future. To do so, we first have to identify all market variables affecting the portfolio value. Then we collect data on how these variables moved over the past $k+n$ days. This allows us to calculate $k$ historical scenarios of what can happen in $n$ days. Assuming that the market will behave in the future as it did in the past, we can compute the portfolio value in each of these scenarios. This provides us with a relative frequency distribution, which we then use to determine the value at risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo simulation\n",
    "Monte Carlo simulation is similar to historical simulation in the sense that we also\n",
    "- generate a set of market scenarios,\n",
    "- compute the value of our portfolio in each of these scenarios and\n",
    "- use the resulting relative frequency distribution to determine the value at risk.\n",
    "\n",
    "They differ in the method for generating market scenarios: Instead of historical data, Monte Carlo simulation uses randomly generated movements of all relevant market variables. This requires more work (for example, you first have to develop a model for the market movements), but also comes with more flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook\n",
    "We are going to look at two rather simple portfolios, one containing only a single bond and one containing a swap in addition to that bond. Their values can be computed by summing over all discounted future cash flows of said bond (and swap). Therefore, the only market variables affecting the portfolio values are the interest rates we use to determine the discount factors.\n",
    "We will use both historical and Monte Carlo simulation to obtain interest rate scenarios. Based on these scenarios, we are going to determine the value at risk for both portfolios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historical simulation\n",
    "## Historical data\n",
    "We choose to discount future cash flows using EONIA interest rate curves. We have historical data from every business day of 2018 and 2019 available to us. The data includes the actual over-night rates plus forward rates for various maturities. We'll load the data for maturities of 1 day, 1-11 months and 1-10 years.\n",
    "\n",
    "*Note: These interest rates are not zero-coupon rates, but we are currently using them as is. Bootstrapping is still on the TODO list.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# load test data from an Excel file\n",
    "xl = pd.ExcelFile('TestDaten.xlsx')\n",
    "#print(xl.sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     2
    ]
   },
   "outputs": [],
   "source": [
    "# import data into pandas dataframe\n",
    "data_EONIA = xl.parse('EONIA')\n",
    "data_EONIA = pd.DataFrame(data_EONIA,\n",
    "                       columns = [\n",
    "                           'Stichtag',\n",
    "                           '1t',\n",
    "                           '1m',\n",
    "                           '2m',\n",
    "                           '3m',\n",
    "                           '4m',\n",
    "                           '5m',\n",
    "                           '6m',\n",
    "                           '7m',\n",
    "                           '8m',\n",
    "                           '9m',\n",
    "                           '10m',\n",
    "                           '11m',\n",
    "                           '1j',\n",
    "                           '2j',\n",
    "                           '3j',\n",
    "                           '4j',\n",
    "                           '5j',\n",
    "                           '6j',\n",
    "                           '7j',\n",
    "                           '8j',\n",
    "                           '9j',\n",
    "                           '10j'\n",
    "                       ])\n",
    "\n",
    "\n",
    "# convert Excel dates to a more useful format and add them to the data frame as a new column\n",
    "data_EONIA['datum'] = pd.TimedeltaIndex(data_EONIA['Stichtag'], unit='d') + dt.datetime(1899, 12, 30)\n",
    "#display(data_EONIA.head(5))\n",
    "#display(data_EONIA.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we'll need them later, we store the selected maturities in the form of year fractions and dates (relative to our reference date defined above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# maturities in years\n",
    "sampling_points_EONIA_yf = [1/365] # 1 day\n",
    "sampling_points_EONIA_yf.extend( (np.arange(11)+1)/12 ) # 1 to 11 months\n",
    "sampling_points_EONIA_yf.extend(np.arange(10)+1) # 1 to 10 years\n",
    "#print(sampling_points_EONIA_yf)\n",
    "\n",
    "# compute the maturity dates (using the reference date defined above)\n",
    "sampling_points_EONIA_dates = []\n",
    "sampling_points_EONIA_dates.append(refdate + dt.timedelta(days = 1))\n",
    "for i in range(11):\n",
    "    sampling_points_EONIA_dates.append(refdate + dt.timedelta(days = (i+1)*30))\n",
    "for i in range(10):\n",
    "    year = refdate.year + i + 1\n",
    "    month = refdate.month\n",
    "    day = refdate.day\n",
    "    sampling_points_EONIA_dates.append(\n",
    "        dt.datetime(year = year, month = month , day = day)\n",
    "    )\n",
    "#print(sampling_points_EONIA_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario generation\n",
    "As mentioned in the introduction, our goal is to use historical data to simulate how much the relevant market variables might change from now to $n$ days from now.\n",
    "\n",
    "### Example 1\n",
    "Let's assume that $n=1$. In that case, we are asking how much a given market variable can change from one business day to the next.\n",
    "We assume that our historical data is ordered by date ascending and numbered consecutively, starting at 1. If $v_i$ denotes the value of the market variable on day $i$, then we can compute change scenarios in the following way.\n",
    "\n",
    "| Scenario | From | To | Absolute change | Relative change |\n",
    "| :---: | :---: | :---: | :---: | :---: |\n",
    "| 1 | Day 1 | Day 2 | $d_1 = v_2 - v_1$ | $q_1 = \\frac{v_2}{v_1}$ |\n",
    "| 2 | Day 2 | Day 3 | $d_2 = v_3 - v_2$ | $q_2 = \\frac{v_3}{v_2}$ |\n",
    "| 3 | Day 3 | Day 4 | $d_3 = v_4 - v_3$ | $q_3 = \\frac{v_4}{v_3}$ |\n",
    "| 4 | Day 4 | Day 5 | $d_4 = v_5 - v_4$ | $q_4 = \\frac{v_5}{v_4}$ |\n",
    "| ... | ||||\n",
    "\n",
    "After we compute these change scenarios (or shift scenarios), we can apply them to the current value $v$ of the market variable to obtain market scenarios: We can either add the absolute changes to the current value...\n",
    "\n",
    "| Scenario | Value of market variable |\n",
    "| :---: | :---: | \n",
    "| 1 | $v + d_1$ | \n",
    "| 2 | $v + d_2$ | \n",
    "| 3 | $v + d_3$ | \n",
    "| 4 | $v + d_4$ |\n",
    "| ... | |\n",
    "\n",
    "... or multiply the current value by the relative changes\n",
    "\n",
    "| Scenario | Value of market variable |\n",
    "| :---: | :---: | \n",
    "| 1 | $v \\cdot q_1$ |\n",
    "| 2 | $v \\cdot q_2$ | \n",
    "| 3 | $v \\cdot q_3$ | \n",
    "| 4 | $v \\cdot q_4$ |\n",
    "| ... | |\n",
    "\n",
    "Which of these approaches you choose should depend on the considered market variable. In the case of interest rates, it turns out that using absolute changes produces more realistic scenarios than using relative changes.\n",
    "\n",
    "### Example 2\n",
    "Note that, since we have one data point for every business day, the way we computed the change scenarios in Example 1 seemed very natural. If we now let $n=10$, we have to think about it more carefully. Consider the following two approaches.\n",
    "\n",
    "*Approach 1*\n",
    "\n",
    "We compute the change in value from day $i$ to day $i+10$ for **all days** where that is possible.\n",
    "\n",
    "| Scenario | From | To | Absolute Change | Relative Change |\n",
    "| :---: | :---: | :---: | :---: | :---: |\n",
    "| 1 | Day 1 | Day 11 | $v_{11} - v_1$ | $\\frac{v_{11}}{v_1}$ |\n",
    "| 2 | Day 2 | Day 12 | $v_{12} - v_2$ | $\\frac{v_{12}}{v_2}$ |\n",
    "| 3 | Day 3 | Day 13 | $v_{13} - v_3$ | $\\frac{v_{13}}{v_3}$ |\n",
    "| 4 | Day 4 | Day 14 | $v_{14} - v_4$ | $\\frac{v_{14}}{v_4}$ |\n",
    "| ... | ||||\n",
    "\n",
    "You'll find that this leads to significant **overlap in the time frames** (From -> To) behind the scenarios. The time frames of scenario 2 and scenario 4, for example, overlap in days 4 to 12. This introduces **correlation** between the scenarios.\n",
    "\n",
    "Remark: This is an example of **autocorrelation**. In the context of time series, autocorrelation is a measure of the similarity between values of one and the same variable at different points in time. In our example that variable is the change in interest rates in the last 10 days. By choosing overlapping time frames, each data point has an influence on multiple values in the time series. This leads to correlation between the values. The bigger the overlap, the stronger the correlation.\n",
    "\n",
    "\n",
    "*Approach 2*\n",
    "\n",
    "To avoid this effect, we can choose the time frames such that they have less or no overlap.\n",
    "\n",
    "| Scenario | From | To | Absolute Change | Relative Change |\n",
    "| :---: | :---: | :---: | :---: | :---: |\n",
    "| 1 | Day 1 | Day 11 | $v_{11} - v_{1}$ | $\\frac{v_{11}}{v_1}$ |\n",
    "| 2 | Day 11 | Day 21 | $v_{21} - v_{11}$ | $\\frac{v_{21}}{v_{11}}$ |\n",
    "| 3 | Day 21 | Day 31 | $v_{31} - v_{21}$ | $\\frac{v_{31}}{v_{21}}$ |\n",
    "| 4 | Day 31 | Day 41 | $v_{41} - v_{31}$ | $\\frac{v_{41}}{v_{31}}$ |\n",
    "| ... | ||||\n",
    "\n",
    "As a consequence, we end up with only about **a tenth the number of scenarios** we had in Approach 1. Of course, we can try to get more data, but that can be expensive or simply not possible (especially, if you consider time frames spanning a whole year, as is often the case in practice). Furthermore, one can argue that data becomes less relevant the further it reaches into the past. \n",
    "\n",
    "\n",
    "### What we do in this notebook\n",
    "The following code is generic in the sense that you can freely choose the time horizon $n$ and whether you want the scenarios to be computed using absolute or relative changes. However, the amount of overlap in the time frames can currently not be controlled.\n",
    "\n",
    "Assuming that our historical data is ordered by date ascending and numbered consecutively, let $n$ be the selected time horizon in days, $v$ be the current value of a market variable and $v_i$ the value it had on date $i$. Then we'll compute the value $s_i$ of the market variable in the $i$-th scenario as either\n",
    "$$s_i = v + (v_{i+n} - v_i)$$\n",
    "or\n",
    "$$s_i = v \\cdot \\frac{v_{i+n}}{v_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timehorizon = 1 # number of business days\n",
    "scenario_construction_type = 'absolute' # absolute or relative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: You can change these parameters to your liking and rerun the code to see the effects. You can use this to verify that the interest rate scenarios generated by applying relative changes can be a bit unrealistic.*\n",
    "\n",
    "We now compute scenarios using both approaches. Afterwards, we choose which set of scenarios we're actually going to use (based on the constant defined above). We assume that the latest EONIA curve available to us is the same as the current curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# restrict to the columns containing interest rates\n",
    "data_EONIA_rates_only = pd.DataFrame(\n",
    "                            data_EONIA,\n",
    "                            columns = data_EONIA.columns[~data_EONIA.columns.isin(['Stichtag','datum'])]\n",
    "                        )\n",
    "\n",
    "# save the current market data in a pandas.series\n",
    "data_EONIA_current = data_EONIA_rates_only.iloc[0,:]\n",
    "#display(data_EONIA_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Compute absolute and relative changes\n",
    "\n",
    "# Copy the data frame structure\n",
    "data_scenarios_absolute = pd.DataFrame().reindex_like(data_EONIA_rates_only)\n",
    "data_scenarios_relative = pd.DataFrame().reindex_like(data_EONIA_rates_only)\n",
    "\n",
    "# Compute the values\n",
    "n = timehorizon\n",
    "\n",
    "for i in range(len(data_EONIA_rates_only.index) - n):\n",
    "    for col in data_EONIA_rates_only.columns:\n",
    "        data_scenarios_absolute.iloc[i + n, :][col] = data_EONIA_current[col] + data_EONIA_rates_only.iloc[i, :][col] - data_EONIA_rates_only.iloc[i + n, :][col]\n",
    "        if data_EONIA_rates_only.iloc[i + n, :][col] != 0:\n",
    "            data_scenarios_relative.iloc[i + n, :][col] = data_EONIA_current[col] * data_EONIA_rates_only.iloc[i, :][col] / data_EONIA_rates_only.iloc[i + n, :][col]\n",
    "\n",
    "            \n",
    "# Remove the rows containing NaN (i.e. the first n rows and those where we divided by 0)\n",
    "data_scenarios_absolute = data_scenarios_absolute.dropna()\n",
    "data_scenarios_relative = data_scenarios_relative.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Decide which scenarios to use in the rest of the notebook\n",
    "if scenario_construction_type == 'relative':\n",
    "    data_scenarios = data_scenarios_relative\n",
    "if scenario_construction_type == 'absolute':\n",
    "    data_scenarios = data_scenarios_absolute\n",
    "\n",
    "# data_scenarios.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Scenarios\n",
    "To get a sense of how different the generated scenarios are from the current data, we plot all of them and highlight the ones that are (in a certain sense) the 'most distant'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "# Compute the 'distances' of all scenarios to the current EONIA curve and sort them by that distance\n",
    "\n",
    "considered_columns = [\n",
    "    '1t',\n",
    "#    '1m',\n",
    "#    '2m',\n",
    "   '3m',\n",
    "#    '4m',\n",
    "#    '5m',\n",
    "   '6m',\n",
    "#    '7m',\n",
    "#    '8m',\n",
    "#    '9m',\n",
    "#    '10m',\n",
    "#    '11m',\n",
    "   '1j',\n",
    "   '2j',\n",
    "   '3j',\n",
    "   '4j',\n",
    "   '5j',\n",
    "   '6j',\n",
    "   '7j',\n",
    "   '8j',\n",
    "   '9j',\n",
    "   '10j'\n",
    "]\n",
    "diffs = data_scenarios[considered_columns] - data_EONIA_current[considered_columns]\n",
    "distances = [ np.linalg.norm(row, ord = 2) for index, row in diffs.iterrows() ]\n",
    "data_scenarios_with_dist = data_scenarios.copy()\n",
    "#print(data_scenarios_with_dist)\n",
    "data_scenarios_with_dist['dist'] = distances\n",
    "#print(distances)\n",
    "data_scenarios_with_dist.sort_values(by = 'dist', ascending = False, inplace=True)\n",
    "data_scenarios_with_dist = data_scenarios_with_dist.reset_index(drop=True)\n",
    "data_scenarios_without_dist = data_scenarios_with_dist.drop('dist', axis=1)\n",
    "#print(data_scenarios_with_dist)\n",
    "#print(data_scenarios_with_dist.iloc[0:10])\n",
    "\n",
    "# We'll highlight the 'most distant' scenarios in a different color in the plot below\n",
    "indeces_most_distant = data_scenarios_with_dist.index.isin([0,1,2,3])\n",
    "\n",
    "# We'll use this later\n",
    "maxdist_hist = max(distances)\n",
    "\n",
    "# clean up\n",
    "# diffs.describe()\n",
    "del diffs\n",
    "del distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the scenarios\n",
    "\n",
    "# use matplotlib\n",
    "if False:\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    ax = fig.gca()\n",
    "\n",
    "    color_current = 'w'\n",
    "    color_bulk = 'k'\n",
    "    color_maxdist = 'tab:blue'\n",
    "    ax.plot(sampling_points_EONIA_yf, data_EONIA_current, '.-', label = 'current EONIA curve', color = color_current, zorder = 20)\n",
    "    ax.plot(sampling_points_EONIA_yf, data_scenarios_without_dist[~indeces_most_distant].transpose(), '.-', label = 'other scenarios', color = color_bulk, zorder = 15, alpha=0.05)\n",
    "    ax.plot(sampling_points_EONIA_yf, data_scenarios_without_dist[indeces_most_distant].transpose(), '.-', label = 'extreme scenarios', color = color_maxdist, zorder = 15, alpha=1)\n",
    "\n",
    "    plt.xlabel('Expiry (in years)')\n",
    "    plt.ylabel('Interest rate (in percent)')\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    legend_elements = [\n",
    "        Patch(facecolor=color_current, edgecolor='gainsboro', label='current EONIA curve'),\n",
    "        Patch(facecolor=color_maxdist, label='extreme scenarios'),\n",
    "        Patch(facecolor=color_bulk, label='other scenarios')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# use plotly\n",
    "else:\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # We'll highlight the 'most distant' scenarios in a different color in the plot below\n",
    "    highlighted_indeces = [0,1,2,3]\n",
    "\n",
    "    showlegend = True\n",
    "    for i in data_scenarios_without_dist.index:\n",
    "        if i not in highlighted_indeces:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x = sampling_points_EONIA_yf,\n",
    "                y = data_scenarios_without_dist.iloc[i],\n",
    "                name = 'other scenarios',\n",
    "                legendgroup = 'other scenarios',\n",
    "                mode = default_plotly_scatter_mode,\n",
    "                showlegend = showlegend,\n",
    "                line=dict(color=\"Black\"),\n",
    "                opacity = 0.08\n",
    "            ))\n",
    "            showlegend = False\n",
    "\n",
    "    showlegend = True\n",
    "    for i in highlighted_indeces:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x = sampling_points_EONIA_yf,\n",
    "            y = data_scenarios_without_dist.iloc[i],\n",
    "            name = 'extreme scenarios',\n",
    "            legendgroup = 'extreme scenarios',\n",
    "            mode = default_plotly_scatter_mode,\n",
    "            showlegend = showlegend,\n",
    "            line=dict(color=color_graphblue)\n",
    "        ))\n",
    "        showlegend = False\n",
    "\n",
    "    fig.add_trace(go.Scatter(x = sampling_points_EONIA_yf, y = data_EONIA_current, name = 'current EONIA curve', mode = default_plotly_scatter_mode, line=dict(color=\"LightCyan\"),))\n",
    "\n",
    "\n",
    "    fig.update_layout(\n",
    "        showlegend=True,\n",
    "        xaxis = dict(title_text = \"Expiry (in years)\"),\n",
    "        yaxis = dict(title_text = \"Interest rate (in percent)\"),\n",
    "        legend = dict(traceorder='reversed'),\n",
    "        title=get_default_title_dict(\"Historical scenarios\")\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case that the scenarios were constructed using relative historical changes, you'll probably find that some of them are rather extreme. To get a better understanding of why they are, we take a closer look at the scenarios containing the highest and the lowest interest rates found in any scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if scenario_construction_type == 'relative':\n",
    "    # print(data_scenarios.min())\n",
    "    # print(data_scenarios.idxmin())\n",
    "    # print(data_scenarios.min().min())\n",
    "    # print(data_scenarios.min().idxmin())\n",
    "    # print(imin)\n",
    "\n",
    "    imin = data_scenarios.idxmin()[data_scenarios.min().idxmin()]\n",
    "    imax = data_scenarios.idxmax()[data_scenarios.max().idxmax()]\n",
    "\n",
    "    display(\n",
    "        pd.DataFrame({\n",
    "            'Current': data_EONIA_current,\n",
    "            'imin': data_EONIA_rates_only.loc[imin,:],\n",
    "            'imin - n': data_EONIA_rates_only.loc[imin - timehorizon,:],\n",
    "            'Scenario (imin)': data_scenarios.loc[imin,:],\n",
    "            'imax': data_EONIA_rates_only.loc[imax,:],\n",
    "            'imax - n': data_EONIA_rates_only.loc[imax - timehorizon,:],\n",
    "            'Scenario (imax)': data_scenarios.loc[imax,:]\n",
    "        }).head(len(data_EONIA_current))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define a function that handles plotting the scenarios\n",
    "def PlotMonteCarloScenarios(monteCarloScenarios, useMatplotlib = True):\n",
    "    \n",
    "    if useMatplotlib:\n",
    "        fig = plt.figure(figsize=(16,8))\n",
    "        ax = fig.gca()\n",
    "        color_current = 'w'\n",
    "        color_bulk = 'k'\n",
    "        ax.plot(sampling_points_EONIA_yf, data_EONIA_current, '.-', color = color_current, zorder = 20)\n",
    "        ax.plot(sampling_points_EONIA_yf, monteCarloScenarios.transpose(), '.-', color = color_bulk, zorder = 15, alpha=20/len(monteCarloScenarios.index))\n",
    "\n",
    "        plt.xlabel('Expiry (in years)')\n",
    "        plt.ylabel('Interest rate (in percent)')\n",
    "\n",
    "        legend_elements = [\n",
    "            Patch(facecolor=color_current, edgecolor='gainsboro', label='current EONIA curve'),\n",
    "            Patch(facecolor=color_bulk, label='Monte Carlo scenarios')\n",
    "        ]\n",
    "        plt.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "        plt.show()\n",
    "    else:\n",
    "        fig = go.Figure()\n",
    "\n",
    "        showlegend = True\n",
    "        opacity = max(0.001, min(1, 25/len(monteCarloScenarios.index)))\n",
    "        for i in monteCarloScenarios.index:\n",
    "            if i not in highlighted_indeces:\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x = sampling_points_EONIA_yf,\n",
    "                    y = monteCarloScenarios.iloc[i],\n",
    "                    name = 'other scenarios',\n",
    "                    legendgroup = 'other scenarios',\n",
    "                    mode = default_plotly_scatter_mode,\n",
    "                    showlegend = showlegend,\n",
    "                    line=dict(color=\"Black\"),\n",
    "                    opacity = opacity\n",
    "                ))\n",
    "                showlegend = False\n",
    "\n",
    "        fig.add_trace(go.Scatter(x = sampling_points_EONIA_yf, y = data_EONIA_current, name = 'current EONIA curve', mode = default_plotly_scatter_mode, line=dict(color=\"LightCyan\"),))\n",
    "\n",
    "\n",
    "        fig.update_layout(\n",
    "            showlegend=True,\n",
    "            xaxis = dict(title_text = \"Expiry (in years)\"),\n",
    "            yaxis = dict(title_text = \"Interest rate (in percent)\"),\n",
    "            legend = dict(traceorder='reversed'),\n",
    "            title={\n",
    "                'text': \"Monte Carlo scenarios\",\n",
    "                'y':0.9,\n",
    "                'x':0.475,\n",
    "                'xanchor': 'center',\n",
    "                'yanchor': 'top'}\n",
    "        )\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple simulation\n",
    "We apply a randomized parallel shift to the current interest rate curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Generate scenarios\n",
    "n_sims_simpleMC = 2000#default_sample_size_MC\n",
    "np.random.seed(7001)\n",
    "random_shifts = np.random.normal(0, 0.02, n_sims_simpleMC)\n",
    "data_scenarios_random_shift = pd.DataFrame().reindex_like(data_EONIA_rates_only)\n",
    "data_scenarios_random_shift = data_scenarios_random_shift.iloc[0:0]\n",
    "\n",
    "for x in random_shifts:\n",
    "    data_scenarios_random_shift = data_scenarios_random_shift.append(data_EONIA_current + x, ignore_index=True)\n",
    "\n",
    "# data_scenarios_random_shift.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotMonteCarloScenarios(data_scenarios_random_shift)\n",
    "# PlotMonteCarloScenarios(data_scenarios_random_shift, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly picking historical data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using random distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Generate scenarios\n",
    "n_picks_dist = default_sample_size_MC\n",
    "np.random.seed(7002)\n",
    "random_dists = np.random.normal(0, maxdist_hist/3, n_picks_dist)\n",
    "random_dists = [abs(d) for d in random_dists]\n",
    "\n",
    "data_scenarios_random_pick_dist = pd.DataFrame().reindex_like(data_scenarios_with_dist)\n",
    "data_scenarios_random_pick_dist = data_scenarios_random_pick_dist.iloc[0:0]\n",
    "\n",
    "for d in random_dists:\n",
    "    data_scenarios_random_pick_dist = data_scenarios_random_pick_dist.append(\n",
    "        data_scenarios_with_dist.iloc[(data_scenarios_with_dist['dist']-d).abs().argsort()[:1]],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    \n",
    "#print(maxdist_hist)\n",
    "#data_scenarios_random_pick_dist.describe()\n",
    "\n",
    "data_scenarios_random_pick_dist = data_scenarios_random_pick_dist.drop('dist', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# PlotMonteCarloScenarios(data_scenarios_random_pick_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using random indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Generate scenarios\n",
    "n_picks_indeces = default_sample_size_MC\n",
    "np.random.seed(7003)\n",
    "num_hist_scenarios = len(data_scenarios_without_dist.index)\n",
    "random_indeces= []\n",
    "\n",
    "while len(random_indeces) < n_picks_indeces:\n",
    "    sample = math.floor(abs(np.random.normal(0, num_hist_scenarios/1)))\n",
    "    if sample >= 0 and sample < num_hist_scenarios:\n",
    "        random_indeces.append(sample)\n",
    "\n",
    "data_scenarios_random_pick_indeces = pd.DataFrame().reindex_like(data_scenarios_without_dist)\n",
    "data_scenarios_random_pick_indeces = data_scenarios_random_pick_indeces.iloc[0:0]\n",
    "\n",
    "for i in random_indeces:\n",
    "    data_scenarios_random_pick_indeces = data_scenarios_random_pick_indeces.append(\n",
    "        data_scenarios_without_dist.iloc[num_hist_scenarios-i-1],\n",
    "        ignore_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PlotMonteCarloScenarios(data_scenarios_random_pick_indeces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using buckets\n",
    "Instead of sampling from all historical interest rate curves available to us, we now divide them into buckets and choose one representative curve for each of them. Then, we take random samples from the set of buckets, instead of the entire set of historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Create buckets\n",
    "data_scenarios_buckets = pd.DataFrame().reindex_like(data_scenarios_without_dist)\n",
    "data_scenarios_buckets = data_scenarios_buckets.iloc[0:0]\n",
    "\n",
    "# The first bucket contains the current curve itself, i.e. the curve with distance 0\n",
    "data_scenarios_buckets = data_scenarios_buckets.append(\n",
    "    data_EONIA_current,\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "n_hist_buckets_max = 60\n",
    "# Then we increase the distance in steps of maxdist/n\n",
    "k = len(data_scenarios_with_dist.index)-1 # data_scenarios_with_dist is ordered by dist descending\n",
    "k_old = np.NaN\n",
    "for i in range(1, n_hist_buckets_max-1):\n",
    "    while data_scenarios_with_dist[\"dist\"][k] < i * maxdist_hist/n_hist_buckets_max:\n",
    "        k -= 1;\n",
    "#     print(k+1)\n",
    "    if k_old != k:\n",
    "        data_scenarios_buckets = data_scenarios_buckets.append(\n",
    "            data_scenarios_without_dist.iloc[k+1],\n",
    "            ignore_index=True\n",
    "        )\n",
    "    k_old = k\n",
    "\n",
    "# The last bucket is represented by the curve, which is the 'most distant' to the current curve\n",
    "data_scenarios_buckets = data_scenarios_buckets.append(\n",
    "    data_scenarios_without_dist.iloc[0],\n",
    "    ignore_index=True\n",
    ")\n",
    "# print(len(data_scenarios_buckets.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Generate scenarios\n",
    "n_picks_buckets = default_sample_size_MC\n",
    "np.random.seed(7004)\n",
    "\n",
    "data_scenarios_random_buckets = pd.DataFrame().reindex_like(data_scenarios_without_dist)\n",
    "data_scenarios_random_buckets = data_scenarios_random_buckets.iloc[0:0]\n",
    "\n",
    "while len(data_scenarios_random_buckets.index) < n_picks_buckets:\n",
    "    sample = math.floor(abs(np.random.normal(0, len(data_scenarios_buckets.index)/2)))\n",
    "    if sample >= 0 and sample < len(data_scenarios_buckets.index):\n",
    "        data_scenarios_random_buckets = data_scenarios_random_buckets.append(\n",
    "            data_scenarios_buckets.iloc[sample],\n",
    "            ignore_index=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PlotMonteCarloScenarios(data_scenarios_random_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation via short rate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# todo\n",
    "cir = analytics.CIRModel('CIR_Test', refdate, 1.0,0.05,0.20,0.025)\n",
    "cir_dc = model_tools.compute_yieldcurve(cir, refdate,sampling_points_EONIA_dates)\n",
    "hull_white = analytics.HullWhiteModel('HW_Test', refdate, 1, 0.1, cir_dc)\n",
    "\n",
    "\n",
    "sim_dates = converter.createPTimeList(refdate, sampling_points_EONIA_dates)\n",
    "refdate_LTime = converter.getLTime(refdate)\n",
    "n_sims = 100\n",
    "n_steps_per_year = 200\n",
    "max_num_threads = 2\n",
    "\n",
    "cir_lab = analytics.ModelLab(cir, refdate_LTime)\n",
    "cir_lab.simulate(sim_dates, n_sims, n_steps_per_year, max_num_threads)\n",
    "\n",
    "hw_lab = analytics.ModelLab(hull_white, refdate_LTime)\n",
    "hw_lab.simulate(sim_dates, n_sims, n_steps_per_year, max_num_threads)\n",
    "\n",
    "sampling_points_EONIA_datediffdays = [math.ceil(365*yf) for yf in sampling_points_EONIA_yf]\n",
    "# dates = converter.createPTimeList(refdate, sampling_points_EONIA_datediffdays)\n",
    "\n",
    "for i in range(n_sims):\n",
    "    cir_lab.setFromSimulatedValues(cir, 1, i)  \n",
    "    dc = model_tools.compute_yieldcurve(cir, sim_dates[0], sampling_points_EONIA_datediffdays)    \n",
    "    mkt_plot.curve(dc, sim_dates, sim_dates[0], True, '', False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# todo\n",
    "sampling_points_EONIA_datediffdays = [math.ceil(365*yf) for yf in sampling_points_EONIA_yf]\n",
    "refdate_sim = sampling_points_EONIA_dates[0]\n",
    "dates = converter.createPTimeList(refdate_sim, sampling_points_EONIA_datediffdays)\n",
    "\n",
    "for i in range(n_sims):\n",
    "    cir_lab.setFromSimulatedValues(cir, 1, i)  \n",
    "    dc = model_tools.compute_yieldcurve(cir, refdate_sim, sampling_points_EONIA_datediffdays)    \n",
    "    mkt_plot.curve(dc, dates, refdate_sim, True, '', False)\n",
    "    \n",
    "\n",
    "# for i in range(n_sims):\n",
    "#     hw_lab.setFromSimulatedValues(hull_white, 1, i)  \n",
    "#     dc = model_tools.compute_yieldcurve(hull_white, sim_times[0], dc_dates_p)    \n",
    "#     mkt_plot.curve(dc, sim_dates, sim_times[0], True, '', False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple portfolio\n",
    "To keep things simple, we start off with a portfolio containing only one fixed coupon bond with the following specifications:\n",
    " - It was issued on 2019/12/30\n",
    " - It has a maturity of 10 years\n",
    " - Its principal is 100€\n",
    " - It pays a 5€ coupon every year\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#define_bond\n",
    "maturity = 10\n",
    "principal = 100.0\n",
    "coupon_rate = 0.05\n",
    "maturity_date = dt.datetime(year = refdate.year + maturity, month = refdate.month, day = refdate.day)\n",
    "#print(refdate)\n",
    "#print(maturity_date)\n",
    "\n",
    "# Generate the coupon payment schedule as a vector of datetimes\n",
    "coupon_dates = []\n",
    "for i in range(maturity):\n",
    "    coupon_dates.append(dt.datetime(year = refdate.year + i + 1, month = refdate.month, day = refdate.day))\n",
    "#print(coupon_dates)\n",
    "coupon_rates = [coupon_rate]*len(coupon_dates)\n",
    "coupon_payments = [coupon_rate*principal]*len(coupon_dates)\n",
    "\n",
    "# We now use these specifications to define a fixed coupon bond\n",
    "fixed_coupon_bond = pyvacon.instruments.BondSpecification('Fixed_Coupon', 'DBK', enums.SecuritizationLevel.NONE, 'EUR',\n",
    "    maturity_date, refdate, principal, default_daycounter_type, coupon_dates, coupon_rates, '', [], [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current value of this portfolio can be computed by simply summing over all discounted future cash flows. Therefore, the only market variables affecting this value are the interest rates we use to determine the discount factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Credit Spread and Portfolio Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define the pricer, we're going to use to price our bond\n",
    "pricing_data_simple = pyvacon.pricing.BondPricingData()\n",
    "pricing_data_simple.param = pyvacon.pricing.BondPricingParameter()\n",
    "pricing_data_simple.param.useJLT = False\n",
    "pricing_data_simple.pricingRequest = pyvacon.pricing.PricingRequest()\n",
    "pricing_data_simple.pricingRequest.setCleanPrice(True)\n",
    "pricing_data_simple.pricer = 'BondPricer'\n",
    "pricing_data_simple.spec = fixed_coupon_bond\n",
    "\n",
    "valdate = refdate # + dt.timedelta(days = timehorizon)\n",
    "pricing_data_simple.valDate = valdate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We are currently not taking portfolio aging into account: In the computations below, we are using the reference date as valuation date. That is, we look at the effects our shift scenarios would have on the value of our portfolio, if they were to happen instantaneously (instead of over the next $n$ days)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Credit Spread\n",
    "**Credit spread** is the difference in yield between two investments of similar maturities, but different credit qualities. It can be interpreted as the risk premium for one investment over the other.\n",
    "\n",
    "Given the low EONIA rates, the bond we defined above currently has a much higher yield than a hypothetical bond paying EONIA rates on the same principal. Pricing the bond using discount factors based on EONIA rates would grossly overestimate its value. That is, the price we compute would be a lot higher than the bond's actual market value. To avoid this, we determine the constant shift we need to apply to the interest rates used for discounting in order for our price to equal the market value of the bond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Use the current EONIA rates + a constant rate to compute the price of the fixed coupon bond\n",
    "# Vary the constant rate and repeat until the value of the bond is right about the same as its principal\n",
    "creditspread = coupon_rate * 100 # in basis points\n",
    "stepsize = coupon_rate * 100 # the initial step size used to vary the interest rate\n",
    "spreads = []\n",
    "values = []\n",
    "for k in range(20):\n",
    "    # create DC defined by the scenario\n",
    "    dsc_fac = analytics.vectorDouble()\n",
    "    spreadScenario = data_EONIA_current + creditspread;\n",
    "    for i in range(len(spreadScenario)):\n",
    "        dsc_fac.append(math.exp(-spreadScenario.iloc[i]/100*i)) # t = i years  # market data is given in basis points -> /100  \n",
    "            \n",
    "    discountCurve = analytics.DiscountCurve('dc_linear', refdate, sampling_points_EONIA_dates, dsc_fac, enums.DayCounter.ACTACT, enums.InterpolationType.LINEAR, enums.ExtrapolationType.NONE)\n",
    "    pricing_data_simple.discountCurve = discountCurve\n",
    "    \n",
    "    results = pyvacon.pricing.price(pricing_data_simple)\n",
    "    \n",
    "    values.append(results.getPrice())\n",
    "    spreads.append(creditspread)\n",
    "    \n",
    "    if values[k] > principal:\n",
    "        creditspread += stepsize\n",
    "    else:\n",
    "        creditspread -= stepsize\n",
    "    stepsize /= 2\n",
    "\n",
    "#print(spreads)\n",
    "#print(values)\n",
    "#print(creditspread)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The credit spread is {{round(creditspread, 3)}}%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Compute the current value without taking the credit spread into account\n",
    "\n",
    "# Create DC defined by the scenario\n",
    "dsc_fac = analytics.vectorDouble()\n",
    "for i in range(len(data_EONIA_current)):\n",
    "        dsc_fac.append(math.exp(-data_EONIA_current.iloc[i]/100*i)) # t = i years  # market data is given in basis points -> /100  \n",
    "\n",
    "discountCurve = analytics.DiscountCurve('dc_linear', refdate, sampling_points_EONIA_dates, dsc_fac, default_daycounter_type, default_interpolation_type, default_extrapolation_type)\n",
    "pricing_data_simple.discountCurve = discountCurve\n",
    "results = pyvacon.pricing.price(pricing_data_simple)\n",
    "current_value_without_credit_spread = results.getPrice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Compute the current value\n",
    "\n",
    "# Create DC defined by the scenario\n",
    "dsc_fac = analytics.vectorDouble()\n",
    "for i in range(len(data_EONIA_current)):\n",
    "        dsc_fac.append(math.exp(-(data_EONIA_current + creditspread).iloc[i]/100*i)) # t = i years  # market data is given in basis points -> /100  \n",
    "\n",
    "discountCurve = analytics.DiscountCurve('dc_linear', refdate, sampling_points_EONIA_dates, dsc_fac, default_daycounter_type, default_interpolation_type, default_extrapolation_type)\n",
    "pricing_data_simple.discountCurve = discountCurve\n",
    "results = pyvacon.pricing.price(pricing_data_simple)\n",
    "currentPriceDirty = results.getPrice()\n",
    "currentPriceClean = results.getCleanPrice()\n",
    "#print(currentPriceDirty)\n",
    "#print(currentPriceClean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking into account the credit spread we computed above, we arrive at a current value of {{round(currentPriceDirty, 6)}}, which closely matches the actual market value of 100. If we didn't take the credit spread into account, we would arrive at a value of {{round(current_value_without_credit_spread, 2)}}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Portfolio Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Choose the scenarios that we're going to use\n",
    "data_scenarios = data_scenarios_absolute\n",
    "#data_scenarios = data_scenarios_random_buckets\n",
    "compare_scenarios = True\n",
    "#data_scenarios_compare = data_scenarios_random_pick_indeces\n",
    "#data_scenarios_compare = data_scenarios_random_pick_dist\n",
    "data_scenarios_compare = data_scenarios_random_shift\n",
    "#data_scenarios_compare = data_scenarios_random_buckets\n",
    "#data_scenarios_compare = data_scenarios_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Compute the price of the fixed coupon bond at the valuation date defined above\n",
    "# Repeat for every scenario\n",
    "def ComputeValuesOfSimplePortfolio(refdate, sampling_points_dates, data_scenarios, creditspread, daycounter_type, interpolation_type, extrapolation_type):\n",
    "    results_dirty = []\n",
    "    results_clean = []\n",
    "    for index, scenario in data_scenarios.iterrows():\n",
    "        # add the credit spread we computed for our bond\n",
    "        scenario = scenario + creditspread\n",
    "\n",
    "        # create DC defined by the scenario\n",
    "        dsc_fac = analytics.vectorDouble()\n",
    "        for i in range(len(scenario)):\n",
    "                dsc_fac.append(math.exp(-scenario.iloc[i]/100*i)) # t = i years  # market data is given in basis points -> /100  \n",
    "\n",
    "        discountCurve = analytics.DiscountCurve('dc_linear', refdate, sampling_points_dates, dsc_fac, daycounter_type, interpolation_type, extrapolation_type)\n",
    "        pricing_data_simple.discountCurve = discountCurve\n",
    "\n",
    "        results = pyvacon.pricing.price(pricing_data_simple)\n",
    "        results_dirty.append(results.getPrice())\n",
    "        results_clean.append(results.getCleanPrice())\n",
    "        #print(pricing_data_simple.spec.getObjectId() + ', dirty price: ' + str(results.getPrice()) + \",  clean price: \" + str(results.getCleanPrice()))\n",
    "    return [results_dirty, results_clean]\n",
    "    \n",
    "\n",
    "results_dirty, results_clean = ComputeValuesOfSimplePortfolio(refdate, sampling_points_EONIA_dates, data_scenarios, creditspread, default_daycounter_type, default_interpolation_type, default_extrapolation_type)\n",
    "    \n",
    "if compare_scenarios:\n",
    "    results_dirty_compare, results_clean_compare = ComputeValuesOfSimplePortfolio(refdate, sampling_points_EONIA_dates, data_scenarios_compare, creditspread, default_daycounter_type, default_interpolation_type, default_extrapolation_type)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# In case we want to compare scenarios, we determine the minimal and maximal x-values to display in upcoming plots\n",
    "if compare_scenarios:\n",
    "    xmin = min(min(results_dirty), min(results_dirty_compare))\n",
    "    xmax = max(max(results_dirty), max(results_dirty_compare))\n",
    "    \n",
    "    #minIndex = results_dirty.index(min(results_dirty))\n",
    "    #print(minIndex)\n",
    "    #print(results_dirty[minIndex])\n",
    "    #print(data_scenarios.iloc[minIndex])\n",
    "\n",
    "    #maxIndex = results_dirty.index(max(results_dirty))\n",
    "    #print(maxIndex)\n",
    "    #print(results_dirty[maxIndex])\n",
    "    #print(data_scenarios.iloc[maxIndex])\n",
    "\n",
    "    #results_series = pd.Series(results_dirty)\n",
    "    #display(results_series.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot pricing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define a function that plots the histrograms\n",
    "def plotHistogram(\n",
    "    data,\n",
    "    binsstart,\n",
    "    binsend,\n",
    "    nbins,\n",
    "    title_xaxis,\n",
    "    markercolor = color_histmarker,\n",
    "    bordercolor = color_histmarkerborder\n",
    "):\n",
    "    xbins = dict(start = binsstart, end = binsend, size = (binsend-binsstart)/nbins)\n",
    "    marker=dict(\n",
    "        color=markercolor,\n",
    "        line = dict(color = bordercolor, width = 1)\n",
    "    )\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Histogram(x = data, xbins = xbins, marker = marker))\n",
    "\n",
    "    fig.update_layout(\n",
    "        showlegend=False,\n",
    "        xaxis = dict(title_text = title_xaxis, range = [binsstart, binsend]),\n",
    "        yaxis = dict(title_text = \"Number of occurences\")\n",
    "    #     ,title={\n",
    "    #         'text': \"Historical scenarios\",\n",
    "    #         'y':0.9,\n",
    "    #         'x':0.475,\n",
    "    #         'xanchor': 'center',\n",
    "    #         'yanchor': 'top'}\n",
    "    )\n",
    "\n",
    "    fig.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Histogramm of the pricing results\n",
    "plotHistogram(\n",
    "    data = results_dirty,\n",
    "    binsstart = xmin,\n",
    "    binsend = xmax,\n",
    "    nbins = 60,\n",
    "    title_xaxis = \"Portfolio value\"\n",
    ")\n",
    "\n",
    "if compare_scenarios:\n",
    "    plotHistogram(\n",
    "        data = results_dirty_compare,\n",
    "        binsstart = xmin,\n",
    "        binsend = xmax,\n",
    "        nbins = 60,\n",
    "        title_xaxis = \"Portfolio value\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Histogramm of the changes/differences in value\n",
    "valDiffsDirty = np.asarray([res - currentPriceDirty for res in results_dirty])\n",
    "binsstart_simple = xmin - currentPriceDirty\n",
    "binsend_simple = xmax - currentPriceDirty\n",
    "plotHistogram(\n",
    "    data = valDiffsDirty,\n",
    "    binsstart = binsstart_simple,\n",
    "    binsend = binsend_simple,\n",
    "    nbins = 60,\n",
    "    title_xaxis = \"Change in portfolio value\"\n",
    ")\n",
    "\n",
    "\n",
    "if compare_scenarios:\n",
    "    valDiffsDirty_compare = np.asarray([res - currentPriceDirty for res in results_dirty_compare])\n",
    "    plotHistogram(\n",
    "        data = valDiffsDirty_compare,\n",
    "        binsstart = binsstart_simple,\n",
    "        binsend = binsend_simple,\n",
    "        nbins = 60,\n",
    "        title_xaxis = \"Change in portfolio value\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Value at Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# determine quantile\n",
    "valDiffsDirty = (-1)*np.sort((-1)*valDiffsDirty)\n",
    "quantile = 0.99\n",
    "#print(np.quantile(valDiffsDirty, 1-quantile, interpolation='higher')) # apparently always uses ascending order\n",
    "\n",
    "# Compute the number of the entry corresponding to the quantile defined above\n",
    "quantileIndex = np.ceil(len(valDiffsDirty)*quantile).astype(int)\n",
    "#print(quantileIndex)\n",
    "#print(quantile * len(valDiffsDirty))\n",
    "\n",
    "# To get the index of this entry, we have to subtract 1\n",
    "quantileIndex -= 1\n",
    "\n",
    "# Check correctness\n",
    "#print('--------------')\n",
    "#print(valDiffsDirty[quantileIndex-1])\n",
    "#print((quantileIndex)/len(valDiffsDirty))\n",
    "#print('--------------')\n",
    "#print(valDiffsDirty[quantileIndex])\n",
    "#print((quantileIndex + 1)/len(valDiffsDirty))\n",
    "#print('--------------')\n",
    "#print(valDiffsDirty[quantileIndex+1])\n",
    "#print((quantileIndex + 2)/len(valDiffsDirty))\n",
    "#print('--------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot cummulative relative frequencies of loss of portfolio value\n",
    "losses = -valDiffsDirty\n",
    "\n",
    "marker=dict(\n",
    "    color=color_histmarker\n",
    ")\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x = losses, nbinsx = len(losses)*2, marker = marker, cumulative_enabled = True, histnorm = 'probability'))\n",
    "\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    xaxis = dict(title_text = \"Loss of portfolio value\"),\n",
    "    yaxis = dict(title_text = \"Cumulative relative frequency\")\n",
    "#     ,title={\n",
    "#         'text': \"Historical scenarios\",\n",
    "#         'y':0.9,\n",
    "#         'x':0.475,\n",
    "#         'xanchor': 'center',\n",
    "#         'yanchor': 'top'}\n",
    ")\n",
    "\n",
    "fig.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a probability of {{round((quantileIndex + 1)/len(valDiffsDirty)*100, 3)}}% the value of our portfolio is not going to shrink by more than {{round(-1 * valDiffsDirty[quantileIndex], 4)}} in the next {{timehorizon}} day(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended portfolio\n",
    "## Add a swap\n",
    "We swap the fixed coupon payments for interest payments based on EONIA rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define the swap scpecification\n",
    "startdates = [refdate]\n",
    "startdates.extend(coupon_dates[0:len(coupon_dates)-1])\n",
    "#startdates = converter.createPTimeList(refdate, startdates)\n",
    "\n",
    "enddates = coupon_dates\n",
    "#enddates = converter.createPTimeList(enddates, startdates)\n",
    "\n",
    "#print(startdates)\n",
    "#print(enddates)\n",
    "\n",
    "paydates = enddates\n",
    "resetdates = startdates\n",
    "\n",
    "notionals = analytics.vectorDouble()\n",
    "notionals.append(principal)\n",
    "\n",
    "fixedleg = analytics.IrFixedLegSpecification(coupon_rate, notionals, startdates, enddates, paydates,'EUR', default_daycounter_type)\n",
    "\n",
    "floatleg = analytics.IrFloatLegSpecification(notionals, resetdates, startdates, enddates,\n",
    "                                    paydates,'EUR', 'test_udl', default_daycounter_type, \n",
    "                                    0)\n",
    "                                    #creditspread/100) # spread is given in basis points\n",
    "\n",
    "ir_swap = analytics.InterestRateSwapSpecification('TEST_SWAP', 'DBK', enums.SecuritizationLevel.COLLATERALIZED, 'EUR',\n",
    "                                           converter.getLTime(paydates[-1]), fixedleg, floatleg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recompute the value of our portfolio in all scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Specify all data we need to price the swap\n",
    "ir_swap_pricing_data = analytics.InterestRateSwapPricingData()\n",
    "\n",
    "pay_leg_pricing_data = analytics.InterestRateSwapLegPricingData()\n",
    "pay_leg_pricing_data.spec = ir_swap.getPayLeg()\n",
    "pay_leg_pricing_data.fxRate = 1.0\n",
    "pay_leg_pricing_data.weight = -1.0\n",
    "\n",
    "rec_leg_pricing_data = analytics.InterestRateSwapFloatLegPricingData()\n",
    "rec_leg_pricing_data.spec = ir_swap.getReceiveLeg()\n",
    "rec_leg_pricing_data.fxRate = 1.0\n",
    "rec_leg_pricing_data.weight = 1.0\n",
    "\n",
    "ir_swap_pricing_data.pricer = 'InterestRateSwapPricer'\n",
    "ir_swap_pricing_data.pricingRequest = analytics.PricingRequest()\n",
    "ir_swap_pricing_data.valDate = converter.getLTime(refdate)\n",
    "ir_swap_pricing_data.setCurr('EUR')\n",
    "ir_swap_pricing_data.addLegData(pay_leg_pricing_data)\n",
    "ir_swap_pricing_data.addLegData(rec_leg_pricing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Compute the price of our portfolio\n",
    "# Repeat for every scenario\n",
    "results_dirty = []\n",
    "results_clean = []\n",
    "for index, scenario in data_scenarios.iterrows():\n",
    "    # add the credit spread we computed for our bond\n",
    "    \n",
    "    # create DC defined by the scenario\n",
    "    factorsEONIA = analytics.vectorDouble()\n",
    "    factorsWithSpread = analytics.vectorDouble()\n",
    "    for i in range(len(scenario)):\n",
    "        factorsEONIA.append(math.exp(-scenario.iloc[i]/100*i)) # t = i years  # market data is given in basis points -> /100  \n",
    "        factorsWithSpread.append(math.exp(-(scenario.iloc[i] + creditspread)/100*i)) # t = i years  # market data is given in basis points -> /100  \n",
    "            \n",
    "    dcEONIA = analytics.DiscountCurve('dc_linear', refdate, sampling_points_EONIA_dates, factorsEONIA, default_daycounter_type, default_interpolation_type, default_extrapolation_type)\n",
    "    dcWithSpread   = analytics.DiscountCurve('dc_linear_spread', refdate, sampling_points_EONIA_dates, factorsWithSpread, default_daycounter_type, default_interpolation_type, default_extrapolation_type)\n",
    "    \n",
    "    pricing_data_simple.discountCurve = dcEONIA # dcWithSpread\n",
    "    pay_leg_pricing_data.discountCurve = dcEONIA\n",
    "    rec_leg_pricing_data.discountCurve = dcEONIA\n",
    "    rec_leg_pricing_data.fixingCurve = dcEONIA\n",
    "    \n",
    "    prBond = pyvacon.pricing.price(pricing_data_simple)\n",
    "    prSwap = analytics.price(ir_swap_pricing_data)\n",
    "    dirty = prBond.getPrice() + prSwap.getPrice()\n",
    "    clean = prBond.getCleanPrice() + prSwap.getCleanPrice()\n",
    "    results_dirty.append(dirty)\n",
    "    results_clean.append(clean)\n",
    "    #print(pricing_data_simple.spec.getObjectId() + ', dirty price: ' + str(results.getPrice()) + \",  clean price: \" + str(results.getCleanPrice()))\n",
    "#print(results_dirty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the current value as a reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define a discount curve based on the current EONIA rates\n",
    "factorsEONIA = analytics.vectorDouble()\n",
    "factorsWithSpread = analytics.vectorDouble()\n",
    "for i in range(len(data_EONIA_current)):\n",
    "    factorsEONIA.append(math.exp(-data_EONIA_current.iloc[i]/100*i)) # t = i years  # market data is given in basis points -> /100  \n",
    "    factorsWithSpread.append(math.exp(-(data_EONIA_current.iloc[i] + creditspread)/100*i)) # t = i years  # market data is given in basis points -> /100  \n",
    "    \n",
    "dcEONIA = analytics.DiscountCurve('dc_linear', refdate, sampling_points_EONIA_dates, factorsEONIA, default_daycounter_type, default_interpolation_type, default_extrapolation_type)\n",
    "dcWithSpread = analytics.DiscountCurve('dc_linear_spread', refdate, sampling_points_EONIA_dates, factorsWithSpread, default_daycounter_type, default_interpolation_type, default_extrapolation_type)\n",
    "\n",
    "pricing_data_simple.discountCurve = dcEONIA # dcWithSpread\n",
    "pay_leg_pricing_data.discountCurve = dcEONIA\n",
    "rec_leg_pricing_data.discountCurve = dcEONIA \n",
    "rec_leg_pricing_data.fixingCurve = dcEONIA\n",
    "\n",
    "# compute portfolio value\n",
    "prBond = pyvacon.pricing.price(pricing_data_simple)\n",
    "prSwap = analytics.price(ir_swap_pricing_data)\n",
    "#print(prSwap.getPrice())\n",
    "#print(prBond.getPrice())\n",
    "currentValueBond = prBond.getPrice()\n",
    "currentValueSwap = prSwap.getPrice()\n",
    "currentValue = prBond.getPrice() + prSwap.getPrice()\n",
    "#print(currentValueSwap)\n",
    "#print(currentValueBond)\n",
    "#print(currentValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the pricing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Histogramm of the changes/differences in value\n",
    "valDiffsDirty = np.asarray([res - currentValue for res in results_dirty])\n",
    "plotHistogram(\n",
    "    data = valDiffsDirty,\n",
    "    binsstart = binsstart_simple,\n",
    "    binsend = binsend_simple,\n",
    "    nbins = 60,\n",
    "    title_xaxis = 'Change in portfolio value'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the swap we added to our portfolio cancels out any market risk, setting the value at risk to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interest Rate Shock Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters for the shock scenarios\n",
    "\n",
    "shockParams = pd.DataFrame({'Currency': [], 'Parallel': [], 'Short': [], 'Long': []})\n",
    "shockParams = shockParams.append({'Currency': 'EUR', 'Parallel': 200, 'Short': 250, 'Long': 100}, ignore_index = True)\n",
    "shockParams = shockParams.append({'Currency': 'GBP', 'Parallel': 250, 'Short': 300, 'Long': 150}, ignore_index = True)\n",
    "shockParams = shockParams.append({'Currency': 'USD', 'Parallel': 200, 'Short': 300, 'Long': 150}, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the change in value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Compute the price of our portfolio\n",
    "# Repeat for every scenario\n",
    "results_dirty = []\n",
    "results_clean = []\n",
    "results_dirty_bondonly = []\n",
    "results_clean_bondonly = []\n",
    "\n",
    "currency = 'EUR'\n",
    "parallel = shockParams.loc[shockParams['Currency'] == currency].loc[0]['Parallel']\n",
    "short = shockParams.loc[shockParams['Currency'] == currency].loc[0]['Short']\n",
    "long = shockParams.loc[shockParams['Currency'] == currency].loc[0]['Long']\n",
    "    \n",
    "shockScenarios = ['ParallelUp', 'ParallelDown', 'ShortUp', 'ShortDown', 'LongUp', 'LongDown', 'Flatten', 'Steepen']\n",
    "\n",
    "for shockScenario in shockScenarios:\n",
    "    dcEONIA = getShockedDiscountCurve(\n",
    "        'dc_linear',\n",
    "        refdate,\n",
    "        sampling_points_EONIA_dates,\n",
    "        data_EONIA_current*100,\n",
    "        default_daycounter_type,\n",
    "        default_interpolation_type,\n",
    "        default_extrapolation_type,\n",
    "        shockScenario,\n",
    "        parallel,\n",
    "        short,\n",
    "        long\n",
    "    )\n",
    "    \n",
    "#     dcWithSpread = getShockedDiscountCurve(\n",
    "#          'dc_linear_spread',\n",
    "#          refdate,\n",
    "#          sampling_points_EONIA_dates,\n",
    "#          data_EONIA_current + creditspread,\n",
    "#          default_daycounter_type,\n",
    "#          default_interpolation_type,\n",
    "#          default_extrapolation_type,\n",
    "#          shockScenario,\n",
    "#          parallel/100,\n",
    "#          short/100,\n",
    "#          long/100\n",
    "#      )\n",
    "    \n",
    "    pricing_data_simple.discountCurve = dcEONIA # dcWithSpread\n",
    "    pay_leg_pricing_data.discountCurve = dcEONIA\n",
    "    rec_leg_pricing_data.discountCurve = dcEONIA\n",
    "    rec_leg_pricing_data.fixingCurve = dcEONIA\n",
    "    \n",
    "    prBond = pyvacon.pricing.price(pricing_data_simple)\n",
    "    prSwap = analytics.price(ir_swap_pricing_data)\n",
    "    dirty = prBond.getPrice() + prSwap.getPrice()\n",
    "    clean = prBond.getCleanPrice() + prSwap.getCleanPrice()\n",
    "    results_dirty.append(dirty)\n",
    "    results_clean.append(clean)\n",
    "    results_dirty_bondonly.append(prBond.getPrice())\n",
    "    results_clean_bondonly.append(prBond.getCleanPrice())\n",
    "    #print(pricing_data_simple.spec.getObjectId() + ', dirty price: ' + str(results.getPrice()) + \",  clean price: \" + str(results.getCleanPrice()))\n",
    "#print(results_dirty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the change in value (comparison between our entire portfolio and the bond on its own)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Bar plot of the changes/differences in value\n",
    "valDiffsDirty = np.asarray([res - currentValue for res in results_dirty])\n",
    "valDiffsDirtyBondOnly = np.asarray([res - currentValueBond for res in results_dirty_bondonly])\n",
    "\n",
    "y_shockscenarios_simple = valDiffsDirtyBondOnly/currentValueBond*100\n",
    "ymin = min(y_shockscenarios_simple)\n",
    "ymax = max(y_shockscenarios_simple)\n",
    "ydiff = abs(ymax-ymin)\n",
    "rangemin = ymin - ydiff/10\n",
    "rangemax = ymax + ydiff/10\n",
    "\n",
    "marker=dict(\n",
    "    color=color_histmarker,\n",
    "    line = dict(color = color_histmarker, width = 1)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Plot results for the simple portfolio\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=shockScenarios, y=y_shockscenarios_simple, marker = marker))\n",
    "\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    xaxis = dict(title_text = \"Shock Scenario\"),\n",
    "    yaxis = dict(title_text = \"Change in portfolio value [%]\", range = [rangemin, rangemax])\n",
    "    ,title=get_default_title_dict(\"Simple Portfolio\")\n",
    ")\n",
    "\n",
    "fig.show()  \n",
    "\n",
    "\n",
    "\n",
    "# Plot results for the extended portfolio\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=shockScenarios, y=valDiffsDirty/currentValue*100, marker = marker))\n",
    "\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    xaxis = dict(title_text = \"Shock Scenario\"),\n",
    "    yaxis = dict(title_text = \"Change in portfolio value [%]\", range = [rangemin, rangemax])\n",
    "    ,title=get_default_title_dict(\"Extended Portfolio\")\n",
    ")\n",
    "\n",
    "fig.show()  \n",
    "\n",
    "\n",
    "del y_shockscenarios_simple, ymin, ymax, ydiff, rangemin, rangemax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bootstrap zero-coupon interest rates\n",
    "- 'Einheit' der Zinssaetze mit an die Funktionen uebergeben (?) (dezimal, percent, basispoint)\n",
    "- Das Wort 'current' als Beschreibung für die jüngste in den Marktdaten vorhandene EONIA-Kurve ueberdenken\n",
    "- Actually order the historical data by date ascending (instead of descending) to avoid confusion\n",
    "- Illustrate the definition of VaR\n",
    "- Portfolio mit zwei Bonds unterschiedlicher Laufzeiten untersuchen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "268.25px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
