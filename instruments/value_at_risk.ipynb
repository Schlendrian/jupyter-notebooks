{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/rivacon_frontmark_combined_header.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "from enum import Enum, unique\n",
    "\n",
    "import pyvacon\n",
    "import pyvacon.marketdata.testdata as mkt_testdata\n",
    "import pyvacon.tools.enums as enums\n",
    "import pyvacon.marketdata.bootstrapping as bootstr\n",
    "import pyvacon.marketdata.plot as mkt_plot\n",
    "import pyvacon.models.plot as model_plot\n",
    "import pyvacon.models.tools as model_tools\n",
    "import pyvacon.analytics as analytics\n",
    "import pyvacon.tools.converter as converter\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch, Rectangle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.transforms as mtransforms\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime as dt\n",
    "import math\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from scipy import stats\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "from IPython.display import display_html\n",
    "from IPython.display import HTML\n",
    "# # Apply some CSS to this notebook\n",
    "# HTML(\"\"\"\n",
    "# <style>\n",
    "# //css\n",
    "# </style>\n",
    "# \"\"\")\n",
    "\n",
    "# use ipynb to import function definitions from another notebook\n",
    "import ipynb\n",
    "from ipynb.fs.defs.ir_shock_scenarios import getShockedDiscountCurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# We define some constants which we'll use repeatedly throughout this notebook\n",
    "notebook_is_draft = True\n",
    "color_main = 'tab:blue'\n",
    "color_highlight = 'tab:orange'\n",
    "color_graphblue = 'rgb(78, 121, 167)'#'\"#4e79a7\"\n",
    "color_graphblue_5 = 'rgba(78, 121, 167, 0.5)'\n",
    "color_histmarker = \"#4e79a7\"\n",
    "color_histmarkerborder = \"White\"\n",
    "grid_alpha = 0.4\n",
    "default_daycounter_type = enums.DayCounter.ACT365_FIXED\n",
    "default_daycounter = analytics.DayCounter(default_daycounter_type)\n",
    "default_interpolation_type = enums.InterpolationType.HAGAN_DF\n",
    "default_extrapolation_type = enums.ExtrapolationType.NONE\n",
    "default_plotly_scatter_mode = 'lines'\n",
    "if notebook_is_draft:\n",
    "    default_sample_size_MC = 500 # about the same sample size we have available for historical simulation\n",
    "else:\n",
    "    default_sample_size_MC = 10000\n",
    "refdate = dt.datetime(year = 2019, month = 12, day = 30)\n",
    "\n",
    "@unique\n",
    "class UnitInterestRate(Enum):\n",
    "    DECIMAL = 1\n",
    "    PERCENT = 2\n",
    "    BASISPOINTS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     18,
     47,
     78,
     106
    ]
   },
   "outputs": [],
   "source": [
    "# Same for functions\n",
    "def get_default_title_dict(title_text):\n",
    "    return dict(\n",
    "        text = title_text,\n",
    "        yref = 'container',\n",
    "        y = 0.9,\n",
    "        xref = 'paper',\n",
    "        x = 0.5,\n",
    "        xanchor = 'center',\n",
    "        yanchor = 'top')\n",
    "\n",
    "def getDates(refdate, year_fractions):\n",
    "    return [refdate + dt.timedelta(int(round(year_fractions[i]*365.25))) for i in range(len(year_fractions))]\n",
    "\n",
    "def getDiscountFactors(\n",
    "    interest_rates,\n",
    "    year_fractions,\n",
    "    unit_interest_rates\n",
    "):\n",
    "    if len(interest_rates) != len(year_fractions):\n",
    "        raise Exception('You must supply an equal number of interest rates and year fractions!')\n",
    "    \n",
    "    if unit_interest_rates == UnitInterestRate.DECIMAL:\n",
    "        factor_unit = 1\n",
    "    elif unit_interest_rates == UnitInterestRate.PERCENT:\n",
    "        factor_unit = 1/100\n",
    "    elif unit_interest_rates == UnitInterestRate.BASISPOINTS:\n",
    "        factor_unit = 1/(100*100)\n",
    "    else:\n",
    "        raise Exception('Value for parameter \\'unit_interest_rates\\' needs to be one of the values supplied by the Enum class \\'UnitInterestRate\\'!')\n",
    "        \n",
    "    dsc_fac = analytics.vectorDouble()\n",
    "    for i in range(len(interest_rates)):\n",
    "        dsc_fac.append(math.exp(-interest_rates[i]*factor_unit*year_fractions[i]))  \n",
    "        \n",
    "    return dsc_fac\n",
    "\n",
    "\n",
    "def getDiscountCurve(\n",
    "    object_name,\n",
    "    refdate,\n",
    "    dates,\n",
    "    interest_rates,\n",
    "    unit_interest_rates,\n",
    "    daycounter_type = default_daycounter_type,\n",
    "    interpolation_type = default_interpolation_type,\n",
    "    extrapolation_type = default_extrapolation_type\n",
    "):\n",
    "    daycounter = analytics.DayCounter(daycounter_type)\n",
    "    \n",
    "    year_fractions = []\n",
    "    for i in range(len(dates)):\n",
    "        year_fractions.append(daycounter.yf(refdate, dates[i]))\n",
    "    \n",
    "    dsc_fac = getDiscountFactors(\n",
    "        interest_rates,\n",
    "        year_fractions,\n",
    "        unit_interest_rates\n",
    "    )\n",
    "    \n",
    "    discountCurve = analytics.DiscountCurve(\n",
    "        object_name,\n",
    "        refdate,\n",
    "        dates,\n",
    "        dsc_fac,\n",
    "        daycounter_type,\n",
    "        interpolation_type,\n",
    "        extrapolation_type\n",
    "    )\n",
    "    \n",
    "    return discountCurve\n",
    "\n",
    "\n",
    "def getInterestRatesFromDiscountCurve(\n",
    "    discount_curve,\n",
    "    refdate,\n",
    "    dates,\n",
    "    unit_interest_rates\n",
    "):\n",
    "    if unit_interest_rates == UnitInterestRate.DECIMAL:\n",
    "        factor_unit = 1\n",
    "    elif unit_interest_rates == UnitInterestRate.PERCENT:\n",
    "        factor_unit = 100\n",
    "    elif unit_interest_rates == UnitInterestRate.BASISPOINTS:\n",
    "        factor_unit = 100*100\n",
    "    else:\n",
    "        raise Exception('Value for parameter \\'unit_interest_rates\\' needs to be one of the values supplied by the Enum class \\'UnitInterestRate\\'!')\n",
    "    \n",
    "    daycounter = analytics.DayCounter(discount_curve.getDayCounterType())\n",
    "    interest_rates = [ (-1) * math.log(discount_curve.value(dates[i], refdate)) / daycounter.yf(dates[i], refdate) * factor_unit for i in range(len(dates))]\n",
    "    return interest_rates\n",
    "\n",
    "\n",
    "def getBootstrappedData(\n",
    "    raw_data,\n",
    "    column_refdate,\n",
    "    columns_maturity,\n",
    "    maturities_yf,\n",
    "    quotes_template,\n",
    "    curve_name,\n",
    "    daycounter_type,\n",
    "    unit_interest_rates,\n",
    "#     interpolation_type = default_interpolation_type,\n",
    "#     extrapolation_type = default_extrapolation_type,\n",
    "    discount_curves = None #,\n",
    "#     basis_curves = None # TODO\n",
    "):\n",
    "    if unit_interest_rates == UnitInterestRate.DECIMAL:\n",
    "        factor_unit = 1\n",
    "    elif unit_interest_rates == UnitInterestRate.PERCENT:\n",
    "        factor_unit = 100\n",
    "    elif unit_interest_rates == UnitInterestRate.BASISPOINTS:\n",
    "        factor_unit = 100*100\n",
    "    else:\n",
    "        raise Exception('Value for parameter \\'unit_interest_rates\\' needs to be one of the values supplied by the Enum class \\'UnitInterestRate\\'!')\n",
    "    \n",
    "    series_bootstrapped = []\n",
    "        \n",
    "    for i in range(0, len(raw_data.index)):\n",
    "        row = raw_data[columns_maturity].iloc[i]\n",
    "\n",
    "        # Copy the template and insert actual quotes\n",
    "        quotes = quotes_template.copy(deep = True)\n",
    "        quotes['Quote'] = row\n",
    "        quotes['Quote'] = quotes['Quote'].apply(lambda x: x/factor_unit) # raw data is given in unit_interest_rates\n",
    "\n",
    "        # set up curve parameters for bootstrapping algorithm\n",
    "        refdate_curve = raw_data.iloc[i][column_refdate]\n",
    "        \n",
    "        if isinstance(refdate_curve, pd.Timestamp):\n",
    "            refdate_curve_dt = refdate_curve.to_pydatetime()\n",
    "        else:\n",
    "            refdate_curve_dt = refdate_curve\n",
    "            \n",
    "        if not ( isinstance(refdate_curve_dt, dt.datetime) or isinstance(refdate_curve_dt, dt.date) ):\n",
    "            raise Exception('The given reference dates need to be of type datetime.date, datetime.datetime or pandas.Timestamp')\n",
    "            \n",
    "        dates_curve = getDates(refdate_curve_dt, maturities_yf)\n",
    "        \n",
    "        if not isinstance(discount_curves, pd.DataFrame):\n",
    "            discount_curve = None\n",
    "        else:\n",
    "#             if column_refdate not in discount_curves.columns:\n",
    "#                 raise Exception('The DataFrame containing discount curves needs to supply the ref')\n",
    "#             query = discount_curves[discount_curves['Analytics.DiscountCurve'].getRefDate() == refdate_curve]\n",
    "            query = discount_curves[discount_curves[column_refdate] == refdate_curve]\n",
    "            if query.empty:\n",
    "                discount_curve = None\n",
    "                # Throw error? Show warning?\n",
    "            elif len(query.index) > 1:\n",
    "                raise Exception('More than one discount curve was provided for reference date ' + refdate_curve_dt.strftime(\"%d-%m-%Y\") + '. I don\\'t know which one to use.')\n",
    "            else:\n",
    "                discount_curve = query['Analytics.DiscountCurve'].iloc[0]\n",
    "        \n",
    "        basis_curve = None # TODO\n",
    "        \n",
    "        curve_spec =  {\n",
    "            'refDate': refdate_curve_dt, \n",
    "            'curveName': curve_name + refdate_curve_dt.strftime(\"%d-%m-%Y\"),\n",
    "            'dayCount': daycounter_type,\n",
    "            'calendar': analytics.SimpleHolidayCalendar('GER_HOL'),\n",
    "            'discountCurve': discount_curve,\n",
    "            'basisCurve': basis_curve#,\n",
    "#             'InterpolationType': interpolation_type,\n",
    "#             'ExtrapolationType': extrapolation_type\n",
    "        }\n",
    "\n",
    "        # bootstrap the curve     \n",
    "        curve_bootstrapped = bootstr.bootstrap_curve(quotes, curve_spec)\n",
    "\n",
    "        # the resulting curve is given as a discount curve\n",
    "        # -> compute the zero rates for the given maturities\n",
    "        df = analytics.vectorDouble()\n",
    "        dc = analytics.DayCounter(daycounter_type)\n",
    "        curve_bootstrapped.value(df, refdate_curve_dt, dates_curve)\n",
    "        data_bootstrapped = [-math.log(df[i])/dc.yf(refdate_curve_dt, dates_curve[i]) for i in range(len(df))]\n",
    "        data_bootstrapped = [factor_unit*x for x in data_bootstrapped] # convert back to unit_interest_rates\n",
    "\n",
    "        # put the data into a series and store it in a list\n",
    "        series = pd.Series(data = [refdate_curve, curve_bootstrapped] + data_bootstrapped, index = [column_refdate, 'Analytics.DiscountCurve'] + columns_maturity)\n",
    "        series_bootstrapped.append(series)\n",
    "                \n",
    "    return pd.DataFrame(data = series_bootstrapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## Value at risk\n",
    "**Value at risk (VaR)** is a measure for the risk in a portfolio of financial assets. Given a time horizon of $n$ days and a confidence level $\\alpha$, the VaR is the loss of value that has the probability $\\alpha$ not to be exceeded within the next $n$ days. In other words, the VaR is the $\\alpha$-quantile of the distribution of loss in the value of a portfolio over the next $n$ days.\n",
    "\n",
    "The different methods for estimating the value at risk can be put into two major categories: Those using analytical models and those using simulations.\n",
    "\n",
    "The goal of **analytical** methods is to define a probability distribution, which approximates the actual probability distribution of the portfolio value. One can then write down a closed formula for the value at risk.\n",
    "\n",
    "**Simulation**-based methods simulate the change in value over the next $n$ days and use the resulting relative frequency distribution to 'read off' the value at risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     8
    ]
   },
   "outputs": [],
   "source": [
    "# Illustrate VaR using probability density of a normal distribution\n",
    "def illustrateVaRNormalDistPDF(\n",
    "    xmin,\n",
    "    xmax,\n",
    "    alpha,\n",
    "    npoints = 100, # number of data points used to draw the density function\n",
    "    mean = 0,\n",
    "    std = 1\n",
    "):\n",
    "    normal_dist = stats.norm(mean,std)\n",
    "    x_VaR = normal_dist.ppf(0.945)\n",
    "    rangey = [0-0.03, normal_dist.pdf(mean) + 0.035]\n",
    "\n",
    "    x_pdf = [xmin + i*(xmax - xmin)/(npoints - 1) for i in range(npoints)]\n",
    "    y_pdf = [normal_dist.pdf(x) for x in x_pdf]#[math.exp(-x*x/2)/math.sqrt(2*math.pi) for x in x_density]\n",
    "\n",
    "    x_alpha_annotation = x_VaR + 0.2*(xmax - x_VaR)\n",
    "    x_fill_shape = [x_VaR + i*(xmax - x_VaR)/(npoints - 1) for i in range(npoints)]\n",
    "    y_fill_shape = [normal_dist.pdf(x) for x in x_fill_shape]\n",
    "\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x = x_fill_shape,\n",
    "        y = y_fill_shape,\n",
    "        mode = 'lines',\n",
    "        showlegend = False,\n",
    "        fill = 'tozeroy',\n",
    "        marker = dict(color = color_graphblue)\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x = x_pdf,\n",
    "        y = y_pdf,\n",
    "        name = 'probability density',\n",
    "        mode = 'lines',\n",
    "        showlegend = True,\n",
    "        line=dict(color=\"Black\"),\n",
    "        opacity = 1\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        showlegend=False,\n",
    "        title = get_default_title_dict('Probability density')\n",
    "\n",
    "        ,xaxis = dict(\n",
    "            title_text = \"Loss\"\n",
    "            ,showticklabels = False\n",
    "        )\n",
    "        ,yaxis = dict(\n",
    "            title_text = \"Frequency\"\n",
    "            ,showticklabels = False\n",
    "            ,range = rangey\n",
    "        )\n",
    "        \n",
    "        ,annotations = [\n",
    "            dict(\n",
    "                xref = 'x',\n",
    "                x = x_VaR,\n",
    "                yref = 'paper',\n",
    "                y = -0.065,\n",
    "                text = 'VaR',\n",
    "                showarrow = False\n",
    "            )\n",
    "\n",
    "            ,dict(\n",
    "                xref = 'x',\n",
    "                x = x_alpha_annotation,\n",
    "                yref = 'y',\n",
    "                y = 0.5*normal_dist.pdf(x_alpha_annotation),\n",
    "                text = '$1-\\\\alpha$',\n",
    "                showarrow = False,\n",
    "                font = dict(\n",
    "                    size = 18\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        ,shapes = [\n",
    "            dict(\n",
    "                type = 'line',\n",
    "                xref = 'x',\n",
    "                x0 = x_VaR,\n",
    "                x1 = x_VaR,\n",
    "                yref = 'y',\n",
    "                y0 = rangey[0],\n",
    "                y1 = normal_dist.pdf(x_VaR),\n",
    "                line = dict(\n",
    "                    color = color_histmarker\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "illustrateVaRNormalDistPDF(\n",
    "    xmin = -3,\n",
    "    xmax = 3,\n",
    "    alpha = 0.945\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     8
    ]
   },
   "outputs": [],
   "source": [
    "# Illustrate VaR using cumulative distribution function of a normal distribution\n",
    "def illustrateVaRNormalDistCDF(\n",
    "    xmin,\n",
    "    xmax,\n",
    "    alpha,\n",
    "    npoints = 100, # number of data points used to draw the density function\n",
    "    mean = 0,\n",
    "    std = 1\n",
    "):\n",
    "    normal_dist = stats.norm(mean,std)\n",
    "    x_VaR = normal_dist.ppf(0.945)\n",
    "\n",
    "    x_cdf = [xmin + i*(xmax - xmin)/(npoints - 1) for i in range(npoints)]\n",
    "    y_cdf = [normal_dist.cdf(x) for x in x_cdf]\n",
    "    rangey = [-0.05, 1.05]\n",
    "\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x = x_cdf,\n",
    "        y = y_cdf,\n",
    "        name = 'cumulative distribution',\n",
    "        mode = 'lines',\n",
    "        line=dict(color=\"Black\"),\n",
    "        opacity = 1\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        showlegend=False,\n",
    "        title = get_default_title_dict('Cumulative probability distribution')\n",
    "\n",
    "        ,xaxis = dict(\n",
    "            title_text = \"Loss\"\n",
    "            ,showticklabels = False\n",
    "        )\n",
    "\n",
    "        ,yaxis = dict(\n",
    "            title_text = \"Cumulative probability\",\n",
    "            range = rangey\n",
    "        )\n",
    "\n",
    "        ,shapes = [\n",
    "\n",
    "            # parallel to x, through alpha\n",
    "            dict(\n",
    "                type = 'line',\n",
    "                xref = 'x',\n",
    "                x0 = xmin,\n",
    "                x1 = x_VaR,\n",
    "                yref = 'y',\n",
    "                y0 = alpha,\n",
    "                y1 = alpha,\n",
    "                line = dict(\n",
    "                    color = color_graphblue\n",
    "                )\n",
    "            ),\n",
    "\n",
    "            # parallel to y, through VaR\n",
    "            dict(\n",
    "                type = 'line',\n",
    "                xref = 'x',\n",
    "                x0 = x_VaR,\n",
    "                x1 = x_VaR,\n",
    "                yref = 'y',\n",
    "                y0 = rangey[0],\n",
    "                y1 = alpha,\n",
    "                line = dict(\n",
    "                    color = color_histmarker\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "\n",
    "\n",
    "        ,annotations = [\n",
    "            dict(\n",
    "                xref = 'x',\n",
    "                x = x_VaR,\n",
    "                yref = 'paper',\n",
    "                y = -0.065,\n",
    "                text = 'VaR',\n",
    "                showarrow = False\n",
    "            )\n",
    "\n",
    "            ,dict(\n",
    "                xref = 'paper',\n",
    "                x = -0.02,\n",
    "                yref = 'y',\n",
    "                y = alpha,\n",
    "                text = '$\\\\alpha$',\n",
    "                showarrow = False,\n",
    "                font = dict(\n",
    "                    size = 15\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "illustrateVaRNormalDistCDF(\n",
    "    xmin = -3,\n",
    "    xmax = 3,\n",
    "    alpha = 0.945\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected shortfall\n",
    "By definition, VaR does not provide any information about the tail of the loss distribution. In fact, it is indifferent to the shape of the distribution beyond the chosen confidence level. For that reason, one often uses another risk measure in combination with VaR: The **expected shortfall (ES)**, also referred to as **conditional value at risk**, is the expected loss given that the loss exceeds VaR. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical simulation\n",
    "A very popular way of simulating changes in value uses past market data to estimate what will happen in the future. To do so, we first have to identify all market variables affecting the portfolio value. Then we collect data on how these variables moved over the past $k+n$ days. This allows us to calculate $k$ historical scenarios of what can happen in $n$ days. Assuming that the market will behave in the future as it did in the past, we can compute the portfolio value in each of these scenarios. This provides us with a relative frequency distribution, which we then use to determine value at risk and expected shortfall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo simulation\n",
    "Monte Carlo simulation is similar to historical simulation in the sense that we also\n",
    "- generate a set of market scenarios,\n",
    "- compute the value of our portfolio in each of these scenarios and\n",
    "- use the resulting relative frequency distribution to determine the value at risk.\n",
    "\n",
    "They differ in the method for generating market scenarios: Instead of historical data, Monte Carlo simulation uses randomly generated movements of all relevant market variables. This requires more work (for example, you first have to develop a model for the market movements), but also comes with more flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook\n",
    "We are going to look at two rather simple portfolios, one containing only a single bond and one containing a swap in addition to that bond. Their values can be computed by summing over all discounted future cash flows of said bond (and swap). Therefore, the only market variables affecting the portfolio values are the interest rates we use to determine the discount factors.\n",
    "We will use both historical and Monte Carlo simulation to obtain interest rate scenarios. Based on these scenarios, we are going to determine value at risk and expected shortfall for both portfolios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historical simulation\n",
    "## Historical data\n",
    "We choose to discount future cash flows using EONIA interest rate curves. We have historical data from every business day of 2018 and 2019 available to us. The data includes rates for various maturities. We'll load the data for maturities of 1 day, 1-11 months and 1-10 years.\n",
    "\n",
    "*Note: These interest rates are not zero-coupon rates. We will determine the zero rates in [the bootstrapping section below](#bootstrapping).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# load test data from an Excel file\n",
    "xl = pd.ExcelFile('TestDaten.xlsx')\n",
    "#print(xl.sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     5,
     29,
     44
    ]
   },
   "outputs": [],
   "source": [
    "# import data into pandas dataframe\n",
    "data_EONIA_raw = xl.parse('EONIA')\n",
    "data_EUR3M_raw = xl.parse('EUR3M')\n",
    "data_EUR6M_raw = xl.parse('EUR6M')\n",
    "del xl\n",
    "columns_maturity_EONIA = [\n",
    "   '1D',\n",
    "   '1M',\n",
    "   '2M',\n",
    "   '3M',\n",
    "   '4M',\n",
    "   '5M',\n",
    "   '6M',\n",
    "   '7M',\n",
    "   '8M',\n",
    "   '9M',\n",
    "   '10M',\n",
    "   '11M',\n",
    "   '1Y',\n",
    "   '2Y',\n",
    "   '3Y',\n",
    "   '4Y',\n",
    "   '5Y',\n",
    "   '6Y',\n",
    "   '7Y',\n",
    "   '8Y',\n",
    "   '9Y',\n",
    "   '10Y'\n",
    "]\n",
    "columns_maturity_EUR3M = [\n",
    "   '3M',\n",
    "   '6M',\n",
    "   '9M',\n",
    "   '1Y',\n",
    "   '2Y',\n",
    "   '3Y',\n",
    "   '4Y',\n",
    "   '5Y',\n",
    "   '6Y',\n",
    "   '7Y',\n",
    "   '8Y',\n",
    "   '9Y',\n",
    "   '10Y'\n",
    "]\n",
    "columns_maturity_EUR6M = [\n",
    "   '6M',\n",
    "   '1Y',\n",
    "   '2Y',\n",
    "   '3Y',\n",
    "   '4Y',\n",
    "   '5Y',\n",
    "   '6Y',\n",
    "   '7Y',\n",
    "   '8Y',\n",
    "   '9Y',\n",
    "   '10Y'\n",
    "]\n",
    "\n",
    "data_EONIA_raw = pd.DataFrame(data_EONIA_raw, columns = ['RefDate'] + columns_maturity_EONIA)\n",
    "data_EUR3M_raw = pd.DataFrame(data_EUR3M_raw, columns = ['RefDate'] + columns_maturity_EUR3M)\n",
    "data_EUR6M_raw = pd.DataFrame(data_EUR6M_raw, columns = ['RefDate'] + columns_maturity_EUR6M)\n",
    "# print(['RefDate'] + columns_maturity_EONIA)\n",
    "\n",
    "# convert Excel dates to a more useful format\n",
    "data_EONIA_raw['RefDate'] = pd.TimedeltaIndex(data_EONIA_raw['RefDate'], unit='d') + dt.datetime(1899, 12, 30)\n",
    "data_EUR3M_raw['RefDate'] = pd.TimedeltaIndex(data_EUR3M_raw['RefDate'], unit='d') + dt.datetime(1899, 12, 30)\n",
    "data_EUR6M_raw['RefDate'] = pd.TimedeltaIndex(data_EUR6M_raw['RefDate'], unit='d') + dt.datetime(1899, 12, 30)\n",
    "#display(data_EONIA.head(5))\n",
    "#display(data_EONIA.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we'll need them later, we store the selected maturities in the form of year fractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# maturities in years\n",
    "maturities_EONIA_yf = [1/365] # 1 day\n",
    "maturities_EONIA_yf.extend( (np.arange(11)+1)/12 ) # 1 to 11 months\n",
    "maturities_EONIA_yf.extend(np.arange(10)+1) # 1 to 10 years\n",
    "\n",
    "maturities_EUR3M_yf = [3/12, 6/12, 9/12] # 3, 6 and 9 months\n",
    "maturities_EUR3M_yf.extend(np.arange(10)+1) # 1 to 10 years\n",
    "\n",
    "maturities_EUR6M_yf = [6/12] # 6 months\n",
    "maturities_EUR6M_yf.extend(np.arange(10)+1) # 1 to 10 years\n",
    "\n",
    "\n",
    "maturities_EONIA_dates = getDates(refdate, maturities_EONIA_yf)\n",
    "maturities_EUR3M_dates = getDates(refdate, maturities_EUR3M_yf)\n",
    "maturities_EUR6M_dates = getDates(refdate, maturities_EUR6M_yf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bootstrapping'></a>\n",
    "### Bootstrapping\n",
    "#### EONIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# The pyvacon bootstrap algorithm needs the quotes of the curve we want to boostrap to be provided\n",
    "# via a DataFrame that has a particular structure. We will now build such a DataFrame with all quotes\n",
    "# set to NaN. We will insert actual quotes before bootstrapping.\n",
    "ncols = len(columns_maturity_EONIA)\n",
    "dfQuotes_EONIA_template = pd.DataFrame(data = columns_maturity_EONIA, index = columns_maturity_EONIA, columns = ['Maturity'])\n",
    "tenors = ['1D', '1M', '2M', '3M', '4M', '5M', '6M', '7M', '8M', '9M', '10M', '11M'] + 10 * ['1Y']\n",
    "dfQuotes_EONIA_template['Instrument'] = ['DEPOSIT'] + ( (ncols-1)*['OIS'] )\n",
    "dfQuotes_EONIA_template['Quote'] = ncols*['NaN']# will be replaced with actual data later\n",
    "dfQuotes_EONIA_template['Currency'] = ncols*['EUR']\n",
    "dfQuotes_EONIA_template['UnderlyingIndex'] = ncols*['EONIA']\n",
    "dfQuotes_EONIA_template['UnderlyingTenor'] = tenors\n",
    "dfQuotes_EONIA_template['UnderlyingPaymentFrequency'] = tenors\n",
    "dfQuotes_EONIA_template['BasisIndex'] = ncols*['NaN']\n",
    "dfQuotes_EONIA_template['BasisTenor'] = ncols*['NaN']\n",
    "dfQuotes_EONIA_template['BasisPaymentFrequency'] = ncols*['NaN']\n",
    "dfQuotes_EONIA_template['PaymentFrequencyFixed'] = tenors\n",
    "dfQuotes_EONIA_template['DayCountFixed'] = ncols*['Act360']\n",
    "dfQuotes_EONIA_template['DayCountFloat'] = ncols*['Act360']\n",
    "dfQuotes_EONIA_template['DayCountBasis'] = ncols*['NaN']\n",
    "dfQuotes_EONIA_template['RollConventionFixed'] = ncols*['ModifiedFollowing']\n",
    "dfQuotes_EONIA_template['RollConventionFloat'] = ncols*['ModifiedFollowing']\n",
    "dfQuotes_EONIA_template['RollConventionBasis'] = ncols*['NaN']\n",
    "dfQuotes_EONIA_template['SpotLag'] = ['0D'] + ((ncols-1)*['2D'])\n",
    "del ncols\n",
    "# dfQuotes_EONIA_template.head(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# bootstrap the data and put it into a DataFrame\n",
    "data_EONIA_bootstrapped = getBootstrappedData(\n",
    "    data_EONIA_raw,\n",
    "    'RefDate',\n",
    "    columns_maturity_EONIA,\n",
    "    maturities_EONIA_yf,\n",
    "    dfQuotes_EONIA_template,\n",
    "    'EONIA',\n",
    "    enums.DayCounter.ACT360,\n",
    "    UnitInterestRate.PERCENT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# compare raw and bootstrapped curves\n",
    "dc = analytics.DayCounter(enums.DayCounter.ACT360)\n",
    "if(notebook_is_draft):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x = maturities_EONIA_yf, y = data_EONIA_raw[columns_maturity_EONIA].iloc[1], name = 'raw', mode = default_plotly_scatter_mode))\n",
    "    fig.add_trace(go.Scatter(x = maturities_EONIA_yf, y = data_EONIA_bootstrapped[columns_maturity_EONIA].iloc[1], name = 'bootstrapped', mode = default_plotly_scatter_mode))\n",
    "\n",
    "#     fig.add_trace(go.Scatter(x = maturities_EONIA_yf, y = data_EONIA_raw[columns_maturity_EONIA].iloc[12], name = 'raw', mode = default_plotly_scatter_mode))\n",
    "#     fig.add_trace(go.Scatter(x = maturities_EONIA_yf, y = data_EONIA_bootstrapped[columns_maturity_EONIA].iloc[12], name = 'bootstrapped', mode = default_plotly_scatter_mode))\n",
    "\n",
    "\n",
    "    fig.update_layout(\n",
    "        showlegend=True,\n",
    "        xaxis = dict(title_text = \"Expiry (in years)\"),\n",
    "        yaxis = dict(title_text = \"Interest rate (in percent)\")\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "#del series_bootstrapped\n",
    "del dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the bootstrapped data from here on out\n",
    "data_EONIA = data_EONIA_bootstrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EURIBOR 3M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# do the same for EUR3M\n",
    "ncols = len(columns_maturity_EUR3M)\n",
    "dfQuotes_EUR3M_template = pd.DataFrame(data = columns_maturity_EUR3M, index = columns_maturity_EUR3M, columns = ['Maturity'])\n",
    "dfQuotes_EUR3M_template['Instrument'] = ['DEPOSIT'] + ( (ncols-1)*['IRS'] )\n",
    "dfQuotes_EUR3M_template['Quote'] = ncols*['NaN']# will be replaced with actual data later\n",
    "dfQuotes_EUR3M_template['Currency'] = ncols*['EUR']\n",
    "dfQuotes_EUR3M_template['UnderlyingIndex'] = ncols*['EURIBOR']\n",
    "dfQuotes_EUR3M_template['UnderlyingTenor'] = ncols*['3M']\n",
    "dfQuotes_EUR3M_template['UnderlyingPaymentFrequency'] = ncols*['3M']\n",
    "dfQuotes_EUR3M_template['BasisIndex'] = ncols*['NaN']\n",
    "dfQuotes_EUR3M_template['BasisTenor'] = ncols*['NaN']\n",
    "dfQuotes_EUR3M_template['BasisPaymentFrequency'] = ncols*['NaN']\n",
    "dfQuotes_EUR3M_template['PaymentFrequencyFixed'] = ['3M', '6M', '9M'] + ( (ncols-3)*['1Y'] )\n",
    "dfQuotes_EUR3M_template['DayCountFixed'] = ncols*['Act360']\n",
    "dfQuotes_EUR3M_template['DayCountFloat'] = ncols*['Act360']\n",
    "dfQuotes_EUR3M_template['DayCountBasis'] = ncols*['NaN']\n",
    "dfQuotes_EUR3M_template['RollConventionFixed'] = ncols*['ModifiedFollowing']\n",
    "dfQuotes_EUR3M_template['RollConventionFloat'] = ncols*['ModifiedFollowing']\n",
    "dfQuotes_EUR3M_template['RollConventionBasis'] = ncols*['NaN']\n",
    "dfQuotes_EUR3M_template['SpotLag'] = ncols*['2D']\n",
    "del ncols\n",
    "# dfQuotes_EUR3M_template.head(999)\n",
    "\n",
    "# bootstrap the data and put it into a DataFrame\n",
    "data_EUR3M_bootstrapped = getBootstrappedData(\n",
    "    data_EUR3M_raw,\n",
    "    'RefDate',\n",
    "    columns_maturity_EUR3M,\n",
    "    maturities_EUR3M_yf,\n",
    "    dfQuotes_EUR3M_template,\n",
    "    'EUR3M',\n",
    "    enums.DayCounter.ACT360,\n",
    "    UnitInterestRate.PERCENT,\n",
    "    data_EONIA_bootstrapped[['RefDate', 'Analytics.DiscountCurve']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# compare raw and bootstrapped curves\n",
    "dc = analytics.DayCounter(enums.DayCounter.ACT360)\n",
    "if(notebook_is_draft):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x = maturities_EUR3M_yf, y = data_EUR3M_raw[columns_maturity_EUR3M].iloc[1], name = 'raw', mode = default_plotly_scatter_mode))\n",
    "    fig.add_trace(go.Scatter(x = maturities_EUR3M_yf, y = data_EUR3M_bootstrapped[columns_maturity_EUR3M].iloc[1], name = 'bootstrapped', mode = default_plotly_scatter_mode))\n",
    "\n",
    "#     fig.add_trace(go.Scatter(x = maturities_EUR3M_yf, y = data_EUR3M_raw[columns_maturity_EUR3M].iloc[12], name = 'raw', mode = default_plotly_scatter_mode))\n",
    "#     fig.add_trace(go.Scatter(x = maturities_EUR3M_yf, y = data_EUR3M_bootstrapped[columns_maturity_EUR3M].iloc[12], name = 'bootstrapped', mode = default_plotly_scatter_mode))\n",
    "\n",
    "    fig.update_layout(\n",
    "        showlegend=True,\n",
    "        xaxis = dict(title_text = \"Expiry (in years)\"),\n",
    "        yaxis = dict(title_text = \"Interest rate (in percent)\")\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "#del series_bootstrapped\n",
    "del dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the bootstrapped data from here on out\n",
    "data_EUR3M = data_EUR3M_bootstrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EURIBOR 6M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# do the same for EUR6M\n",
    "ncols = len(columns_maturity_EUR6M)\n",
    "dfQuotes_EUR6M_template = pd.DataFrame(data = columns_maturity_EUR6M, index = columns_maturity_EUR6M, columns = ['Maturity'])\n",
    "dfQuotes_EUR6M_template['Instrument'] = 0*['DEPOSIT'] + ( (ncols-0)*['IRS'] )#ncols*['IRS']#\n",
    "dfQuotes_EUR6M_template['Quote'] = ncols*['NaN']# will be replaced with actual data later\n",
    "dfQuotes_EUR6M_template['Currency'] = ncols*['EUR']\n",
    "dfQuotes_EUR6M_template['UnderlyingIndex'] = ncols*['EURIBOR']\n",
    "dfQuotes_EUR6M_template['UnderlyingTenor'] = ncols*['6M']\n",
    "dfQuotes_EUR6M_template['UnderlyingPaymentFrequency'] = ncols*['6M']\n",
    "dfQuotes_EUR6M_template['BasisIndex'] = ncols*['NaN']\n",
    "dfQuotes_EUR6M_template['BasisTenor'] = ncols*['NaN']\n",
    "dfQuotes_EUR6M_template['BasisPaymentFrequency'] = ncols*['NaN']\n",
    "dfQuotes_EUR6M_template['PaymentFrequencyFixed'] = ['6M'] + ( (ncols-1)*['1Y'] )\n",
    "dfQuotes_EUR6M_template['DayCountFixed'] = ncols*['Act360']\n",
    "dfQuotes_EUR6M_template['DayCountFloat'] = ncols*['Act360']\n",
    "dfQuotes_EUR6M_template['DayCountBasis'] = ncols*['NaN']\n",
    "dfQuotes_EUR6M_template['RollConventionFixed'] = ncols*['ModifiedFollowing']\n",
    "dfQuotes_EUR6M_template['RollConventionFloat'] = ncols*['ModifiedFollowing']\n",
    "dfQuotes_EUR6M_template['RollConventionBasis'] = ncols*['NaN']\n",
    "dfQuotes_EUR6M_template['SpotLag'] = ncols*['2D']\n",
    "del ncols\n",
    "# dfQuotes_EUR6M_template.head(999)\n",
    "\n",
    "# bootstrap the data and put it into a DataFrame\n",
    "data_EUR6M_bootstrapped = getBootstrappedData(\n",
    "    data_EUR6M_raw,\n",
    "    'RefDate',\n",
    "    columns_maturity_EUR6M,\n",
    "    maturities_EUR6M_yf,\n",
    "    dfQuotes_EUR6M_template,\n",
    "    'EUR6M',\n",
    "    enums.DayCounter.ACT360,\n",
    "    UnitInterestRate.PERCENT,\n",
    "    data_EONIA_bootstrapped[['RefDate', 'Analytics.DiscountCurve']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# compare raw and bootstrapped curves\n",
    "dc = analytics.DayCounter(enums.DayCounter.ACT360)\n",
    "if(notebook_is_draft):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x = maturities_EUR6M_yf, y = data_EUR6M_raw[columns_maturity_EUR6M].iloc[1], name = 'raw', mode = default_plotly_scatter_mode))\n",
    "    fig.add_trace(go.Scatter(x = maturities_EUR6M_yf, y = data_EUR6M_bootstrapped[columns_maturity_EUR6M].iloc[1], name = 'bootstrapped', mode = default_plotly_scatter_mode))\n",
    "\n",
    "#     fig.add_trace(go.Scatter(x = maturities_EUR6M_yf, y = data_EUR6M_raw[columns_maturity_EUR6M].iloc[12], name = 'raw', mode = default_plotly_scatter_mode))\n",
    "#     fig.add_trace(go.Scatter(x = maturities_EUR6M_yf, y = data_EUR6M_bootstrapped[columns_maturity_EUR6M].iloc[12], name = 'bootstrapped', mode = default_plotly_scatter_mode))\n",
    "\n",
    "    fig.update_layout(\n",
    "        showlegend=True,\n",
    "        xaxis = dict(title_text = \"Expiry (in years)\"),\n",
    "        yaxis = dict(title_text = \"Interest rate (in percent)\")\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "#del series_bootstrapped\n",
    "del dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the bootstrapped data from here on out\n",
    "data_EUR6M = data_EUR6M_bootstrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences in basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# define a function that computes the differences in interest rates for common reference dates and maturities\n",
    "def computeBasisSpreads(\n",
    "    data1,\n",
    "    data2,\n",
    "    columns_maturity1,\n",
    "    columns_maturity2,\n",
    "    column_refdate1,\n",
    "    column_refdate2\n",
    "):\n",
    "    common_dates = [d for d in data1[column_refdate1].tolist() if d in data2[column_refdate2].tolist()]\n",
    "\n",
    "    series = []\n",
    "\n",
    "    for adate in common_dates:\n",
    "        row1 = data1[data1[column_refdate1] == adate][columns_maturity1]\n",
    "        row2 = data2[data2[column_refdate2] == adate][columns_maturity2]\n",
    "        \n",
    "        if len(row1.index) != 1:\n",
    "            raise Exception('Found an unexpected number of rows in data set number 1 for reference date ' + adate.strftime(\"%d-%m-%Y\") + '. Expected 1, found ' + str(len(row1.index)) + '.')\n",
    "\n",
    "        if len(row2.index) != 1:\n",
    "            raise Exception('Found an unexpected number of rows in data set number 2 for reference date ' + adate.strftime(\"%d-%m-%Y\") + '. Expected 1, found ' + str(len(row2.index)) + '.')\n",
    "\n",
    "        common_maturities = [c for c in row1.columns if c in row2.columns]\n",
    "        diff = row2[common_maturities].iloc[0] - row1[common_maturities].iloc[0]\n",
    "\n",
    "        series.append(pd.Series(data = [adate]+diff.tolist(), index = [column_refdate1] + diff.index.tolist()))\n",
    "\n",
    "    return pd.DataFrame(data = series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EUR3M-EONIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreads_EUR3M_EONIA = computeBasisSpreads(\n",
    "    data1 = data_EUR3M_bootstrapped,\n",
    "    data2 = data_EONIA_bootstrapped,\n",
    "    columns_maturity1 = columns_maturity_EUR3M,\n",
    "    columns_maturity2 = columns_maturity_EONIA,\n",
    "    column_refdate1 = 'RefDate',\n",
    "    column_refdate2 = 'RefDate'\n",
    ")\n",
    "columns_maturity_EUR3M_EONIA = [c for c in columns_maturity_EUR3M if c in columns_maturity_EONIA]\n",
    "maturities_EUR3M_EONIA_yf = [yf for yf in maturities_EUR3M_yf if yf in maturities_EONIA_yf] # TODO: find better way of doing this\n",
    "maturities_EUR3M_EONIA_dates = getDates(refdate, maturities_EUR3M_EONIA_yf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EUR3M-EUR6M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreads_EUR3M_EUR6M = computeBasisSpreads(\n",
    "    data1 = data_EUR3M_bootstrapped,\n",
    "    data2 = data_EUR6M_bootstrapped,\n",
    "    columns_maturity1 = columns_maturity_EUR3M,\n",
    "    columns_maturity2 = columns_maturity_EUR6M,\n",
    "    column_refdate1 = 'RefDate',\n",
    "    column_refdate2 = 'RefDate'\n",
    ")\n",
    "columns_maturity_EUR3M_EUR6M = [c for c in columns_maturity_EUR3M if c in columns_maturity_EUR6M]\n",
    "maturities_EUR3M_EUR6M_yf = [yf for yf in maturities_EUR3M_yf if yf in maturities_EUR6M_yf] # TODO: find better way of doing this\n",
    "maturities_EUR3M_EUR6M_dates = getDates(refdate, maturities_EUR3M_EUR6M_yf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario generation\n",
    "As mentioned in the introduction, our goal is to use historical data to simulate how much the relevant market variables might change from now to $n$ days from now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "Let's assume that $n=1$. In that case, we are asking how much a given market variable can change from one business day to the next.\n",
    "We assume that our historical data is ordered by date ascending and numbered consecutively, starting at 1. If $v_i$ denotes the value of the market variable on day $i$, then we can compute change scenarios in the following way.\n",
    "\n",
    "| Scenario | From | To | Absolute change | Relative change |\n",
    "| :---: | :---: | :---: | :---: | :---: |\n",
    "| 1 | Day 1 | Day 2 | $d_1 = v_2 - v_1$ | $q_1 = \\frac{v_2}{v_1}$ |\n",
    "| 2 | Day 2 | Day 3 | $d_2 = v_3 - v_2$ | $q_2 = \\frac{v_3}{v_2}$ |\n",
    "| 3 | Day 3 | Day 4 | $d_3 = v_4 - v_3$ | $q_3 = \\frac{v_4}{v_3}$ |\n",
    "| 4 | Day 4 | Day 5 | $d_4 = v_5 - v_4$ | $q_4 = \\frac{v_5}{v_4}$ |\n",
    "| ... | ||||\n",
    "\n",
    "After we compute these change scenarios (or shift scenarios), we can apply them to the current value $v$ of the market variable to obtain market scenarios: We can either add the absolute changes to the current value...\n",
    "\n",
    "| Scenario | Value of market variable |\n",
    "| :---: | :---: | \n",
    "| 1 | $v + d_1$ | \n",
    "| 2 | $v + d_2$ | \n",
    "| 3 | $v + d_3$ | \n",
    "| 4 | $v + d_4$ |\n",
    "| ... | |\n",
    "\n",
    "... or multiply the current value by the relative changes\n",
    "\n",
    "| Scenario | Value of market variable |\n",
    "| :---: | :---: | \n",
    "| 1 | $v \\cdot q_1$ |\n",
    "| 2 | $v \\cdot q_2$ | \n",
    "| 3 | $v \\cdot q_3$ | \n",
    "| 4 | $v \\cdot q_4$ |\n",
    "| ... | |\n",
    "\n",
    "Which of these approaches you choose should depend on the considered market variable. In the case of interest rates, it turns out that using absolute changes produces more realistic scenarios than using relative changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "Note that, since we have one data point for every business day, the way we computed the change scenarios in Example 1 seemed very natural. If we now let $n=10$, we have to think about it more carefully. Consider the following two approaches.\n",
    "\n",
    "*Approach 1*\n",
    "\n",
    "We compute the change in value from day $i$ to day $i+10$ for **all days** where that is possible.\n",
    "\n",
    "| Scenario | From | To | Absolute Change | Relative Change |\n",
    "| :---: | :---: | :---: | :---: | :---: |\n",
    "| 1 | Day 1 | Day 11 | $v_{11} - v_1$ | $\\frac{v_{11}}{v_1}$ |\n",
    "| 2 | Day 2 | Day 12 | $v_{12} - v_2$ | $\\frac{v_{12}}{v_2}$ |\n",
    "| 3 | Day 3 | Day 13 | $v_{13} - v_3$ | $\\frac{v_{13}}{v_3}$ |\n",
    "| 4 | Day 4 | Day 14 | $v_{14} - v_4$ | $\\frac{v_{14}}{v_4}$ |\n",
    "| ... | ||||\n",
    "\n",
    "You'll find that this leads to significant **overlap in the time frames** (From -> To) behind the scenarios. The time frames of scenario 2 and scenario 4, for example, overlap in days 4 to 12. This introduces **correlation** between the scenarios.\n",
    "\n",
    "Remark: This is an example of **autocorrelation**. In the context of time series, autocorrelation is a measure of the similarity between values of one and the same variable at different points in time. In our example that variable is the change in interest rates in the last 10 days. By choosing overlapping time frames, each data point has an influence on multiple values in the time series. This leads to correlation between the values. The bigger the overlap, the stronger the correlation.\n",
    "\n",
    "\n",
    "*Approach 2*\n",
    "\n",
    "To avoid this effect, we can choose the time frames such that they have less or no overlap.\n",
    "\n",
    "| Scenario | From | To | Absolute Change | Relative Change |\n",
    "| :---: | :---: | :---: | :---: | :---: |\n",
    "| 1 | Day 1 | Day 11 | $v_{11} - v_{1}$ | $\\frac{v_{11}}{v_1}$ |\n",
    "| 2 | Day 11 | Day 21 | $v_{21} - v_{11}$ | $\\frac{v_{21}}{v_{11}}$ |\n",
    "| 3 | Day 21 | Day 31 | $v_{31} - v_{21}$ | $\\frac{v_{31}}{v_{21}}$ |\n",
    "| 4 | Day 31 | Day 41 | $v_{41} - v_{31}$ | $\\frac{v_{41}}{v_{31}}$ |\n",
    "| ... | ||||\n",
    "\n",
    "As a consequence, we end up with only about **a tenth the number of scenarios** we had in Approach 1. Of course, we can try to get more data, but that can be expensive or simply not possible (especially, if you consider time frames spanning a whole year, as is often the case in practice). Furthermore, one can argue that data becomes less relevant the further it reaches into the past. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we do in this notebook\n",
    "The following code is generic in the sense that you can freely choose the time horizon $n$ and whether you want the scenarios to be computed using absolute or relative changes. However, the amount of overlap in the time frames can currently not be controlled.\n",
    "\n",
    "Assuming that our historical data is ordered by date ascending and numbered consecutively, let $n$ be the selected time horizon in days, $v$ be the current value of a market variable and $v_i$ the value it had on date $i$. Then we'll compute the value $s_i$ of the market variable in the $i$-th scenario as either\n",
    "$$s_i = v + (v_{i+n} - v_i)$$\n",
    "or\n",
    "$$s_i = v \\cdot \\frac{v_{i+n}}{v_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timehorizon = 1 # number of business days\n",
    "scenario_construction_type = 'absolute' # absolute or relative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: You can change these parameters to your liking and rerun the code to see the effects. You can use this to verify that the interest rate scenarios generated by applying relative changes can be a bit unrealistic.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     7
    ]
   },
   "outputs": [],
   "source": [
    "# define a function that computes the absolute or relative differences\n",
    "def computeScenarios(\n",
    "    data,\n",
    "    column_refdate,\n",
    "    columns_maturity,\n",
    "    ndays,\n",
    "    absolute = True\n",
    "):\n",
    "    # Sort by refdate descending\n",
    "    data_sorted = data.sort_values(column_refdate, ascending = False)\n",
    "    \n",
    "    # Restrict to the columns containing interest rates\n",
    "    data_rates_only = pd.DataFrame(data = data_sorted, columns = columns_maturity)\n",
    "    \n",
    "    # Copy the data frame structure\n",
    "    data_scenarios = pd.DataFrame().reindex_like(data_rates_only)\n",
    "    \n",
    "    # Compute the absolute or relative changes over n days\n",
    "    for i in range(len(data_scenarios.index)-ndays):\n",
    "        if absolute:\n",
    "            data_scenarios.iloc[i+ndays, :] = data_rates_only.iloc[i, :] - data_rates_only.iloc[i+ndays, :]\n",
    "        else:\n",
    "            if data_rates_only.iloc[i+ndays, :] != 0:\n",
    "                data_scenarios.iloc[i+ndays, :] = data_rates_only.iloc[i, :] / data_rates_only.iloc[i+ndays, :]\n",
    "    \n",
    "    # Remove the rows containing NaN (i.e. the first n rows and those where we divided by 0)\n",
    "    return data_scenarios.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     5
    ]
   },
   "outputs": [],
   "source": [
    "# define a function that applies the scenarios to given interest rates\n",
    "def applyScenarios(\n",
    "    current,\n",
    "    scenarios,\n",
    "    absolute = True\n",
    "):\n",
    "    if not isinstance(current, pd.Series):\n",
    "        raise Exception('The current data needs to be provided in a pandas.Series.')\n",
    "    if not isinstance(scenarios, pd.DataFrame):\n",
    "        raise Exception('Scenarios need to be provided in a pandas.DataFrame.')\n",
    "    if not set(current.index) == set(scenarios.columns):\n",
    "        raise Exception('The names of the columns of the current data and the scenario data need to match.')\n",
    "    \n",
    "    applied = pd.DataFrame().reindex_like(scenarios)\n",
    "    for i in applied.index:\n",
    "        if absolute:\n",
    "            applied.loc[i] = current + scenarios.loc[i]\n",
    "        else:\n",
    "            applied.loc[i] = current * scenarios.loc[i]\n",
    "    return applied\n",
    "\n",
    "\n",
    "# applyScenarios(\n",
    "#     pd.Series(data = [1,2,3,4,5]),\n",
    "#     pd.DataFrame(data = [\n",
    "#         [1,1,1,1,1],\n",
    "#         [2,2,2,2,2],\n",
    "#         [1,2,3,4,5],\n",
    "#         [0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "#     ]),\n",
    "#     False\n",
    "# ).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EONIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# EONIA\n",
    "# save the current market data in a pandas.series\n",
    "data_EONIA_current = data_EONIA.iloc[0,:][columns_maturity_EONIA]\n",
    "\n",
    "# compute and apply scenarios\n",
    "data_scenarios_EONIA_diff = computeScenarios(\n",
    "    data_EONIA,\n",
    "    'RefDate',\n",
    "    columns_maturity_EONIA,\n",
    "    timehorizon,\n",
    "    absolute = (scenario_construction_type == 'absolute')\n",
    ")\n",
    "\n",
    "data_scenarios_EONIA = applyScenarios(\n",
    "    data_EONIA_current,\n",
    "    data_scenarios_EONIA_diff,\n",
    "    (scenario_construction_type == 'absolute')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EUR3M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EUR3M\n",
    "# save the current market data in a pandas.series\n",
    "data_EUR3M_current = data_EUR3M.iloc[0,:][columns_maturity_EUR3M]\n",
    "\n",
    "# compute and apply scenarios\n",
    "data_scenarios_EUR3M_diff = computeScenarios(\n",
    "    data_EUR3M,\n",
    "    'RefDate',\n",
    "    columns_maturity_EUR3M,\n",
    "    timehorizon,\n",
    "    absolute = (scenario_construction_type == 'absolute')\n",
    ")\n",
    "\n",
    "data_scenarios_EUR3M = applyScenarios(\n",
    "    data_EUR3M_current,\n",
    "    data_scenarios_EUR3M_diff,\n",
    "    (scenario_construction_type == 'absolute')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EUR6M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# EUR6M\n",
    "# save the current market data in a pandas.series\n",
    "data_EUR6M_current = data_EUR6M.iloc[0,:][columns_maturity_EUR6M]\n",
    "\n",
    "# compute and apply scenarios\n",
    "data_scenarios_EUR6M_diff = computeScenarios(\n",
    "    data_EUR6M,\n",
    "    'RefDate',\n",
    "    columns_maturity_EUR6M,\n",
    "    timehorizon,\n",
    "    absolute = (scenario_construction_type == 'absolute')\n",
    ")\n",
    "\n",
    "data_scenarios_EUR6M = applyScenarios(\n",
    "    data_EUR6M_current,\n",
    "    data_scenarios_EUR6M_diff,\n",
    "    (scenario_construction_type == 'absolute')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EUR3M-EONIA-spreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# EUR3M-EONIA-spreads\n",
    "# save the current market data in a pandas.series\n",
    "spreads_EUR3M_EONIA_current = spreads_EUR3M_EONIA.iloc[0,:][columns_maturity_EUR3M_EONIA]\n",
    "\n",
    "# compute and apply scenarios\n",
    "spreads_scenarios_EUR3M_EONIA_diff = computeScenarios(\n",
    "    spreads_EUR3M_EONIA,\n",
    "    'RefDate',\n",
    "    columns_maturity_EUR3M_EONIA,\n",
    "    timehorizon,\n",
    "    absolute = (scenario_construction_type == 'absolute')\n",
    ")\n",
    "\n",
    "spreads_scenarios_EUR3M_EONIA = applyScenarios(\n",
    "    spreads_EUR3M_EONIA_current,\n",
    "    spreads_scenarios_EUR3M_EONIA_diff,\n",
    "    (scenario_construction_type == 'absolute')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Check consistency of EUR3M-EONIA spreads\n",
    "diff = data_scenarios_EONIA[columns_maturity_EUR3M_EONIA] - (data_scenarios_EUR3M[columns_maturity_EUR3M_EONIA] + spreads_scenarios_EUR3M_EONIA[columns_maturity_EUR3M_EONIA])\n",
    "display(diff.describe())\n",
    "del diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EUR3M-EUR6M-spreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# EUR3M-EUR6M-spreads\n",
    "# save the current market data in a pandas.series\n",
    "spreads_EUR3M_EUR6M_current = spreads_EUR3M_EUR6M.iloc[0,:][columns_maturity_EUR3M_EUR6M]\n",
    "\n",
    "# compute and apply scenarios\n",
    "spreads_scenarios_EUR3M_EUR6M_diff = computeScenarios(\n",
    "    spreads_EUR3M_EUR6M,\n",
    "    'RefDate',\n",
    "    columns_maturity_EUR3M_EUR6M,\n",
    "    timehorizon,\n",
    "    absolute = (scenario_construction_type == 'absolute')\n",
    ")\n",
    "\n",
    "spreads_scenarios_EUR3M_EUR6M = applyScenarios(\n",
    "    spreads_EUR3M_EUR6M_current,\n",
    "    spreads_scenarios_EUR3M_EUR6M_diff,\n",
    "    (scenario_construction_type == 'absolute')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Check consistency of EUR3M-EUR6M-spreads\n",
    "diff = data_scenarios_EUR6M[columns_maturity_EUR3M_EUR6M] - (data_scenarios_EUR3M[columns_maturity_EUR3M_EUR6M] + spreads_scenarios_EUR3M_EUR6M[columns_maturity_EUR3M_EUR6M])\n",
    "display(diff.describe())\n",
    "del diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Scenarios\n",
    "To get a sense of how different the generated scenarios are from the current data, we plot all of them and highlight the ones that are (in a certain sense) the 'most distant'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     11
    ]
   },
   "outputs": [],
   "source": [
    "# define a function that plots the given scenarios and highlights given rows\n",
    "def plotScenarios(\n",
    "    data_current,\n",
    "    data_scenarios,\n",
    "    maturities_yf,\n",
    "    title_figure,\n",
    "    label_curves_bulk,\n",
    "    label_curves_highlighted = 'extreme scenarios',\n",
    "    highlighted_indeces = [],\n",
    "    integer_index = True,\n",
    "    use_matplotlib = False\n",
    "):\n",
    "    if use_matplotlib:\n",
    "        fig = plt.figure(figsize=(16,8))\n",
    "        ax = fig.gca()\n",
    "\n",
    "        color_current = 'w'\n",
    "        color_bulk = 'k'\n",
    "        color_maxdist = 'tab:blue'\n",
    "        ax.plot(maturities_yf, data_current, '.-', label = 'current curve', color = color_current, zorder = 20)\n",
    "        \n",
    "        alpha = 20/len(data_scenarios.index)#0.05\n",
    "        if integer_index:\n",
    "            ax.plot(maturities_yf, data_scenarios.iloc[[x for x in range(len(data_scenarios.index)) if x not in highlighted_indeces], :].transpose(), '.-', label = 'other scenarios', color = color_bulk, zorder = 15, alpha=alpha)\n",
    "            if highlighted_indeces != []:\n",
    "                ax.plot(maturities_yf, data_scenarios.iloc[highlighted_indeces, :].transpose(), '.-', label = 'extreme scenarios', color = color_maxdist, zorder = 15, alpha=1)\n",
    "        else:\n",
    "            ax.plot(maturities_yf, data_scenarios.loc[~data_scenarios.index.isin(highlighted_indeces), :].transpose(), '.-', label = 'other scenarios', color = color_bulk, zorder = 15, alpha=0.05)\n",
    "            ax.plot(maturities_yf, data_scenarios.loc[data_scenarios.index.isin(highlighted_indeces), :].transpose(), '.-', label = 'extreme scenarios', color = color_maxdist, zorder = 15, alpha=1)\n",
    "        \n",
    "        \n",
    "        plt.xlabel('Expiry (in years)')\n",
    "        plt.ylabel('Interest rate (in percent)')\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "        legend_elements = [\n",
    "            Patch(facecolor=color_current, edgecolor='gainsboro', label='current curve'),\n",
    "            Patch(facecolor=color_bulk, label=label_curves_bulk)\n",
    "        ]\n",
    "        if highlighted_indeces != []:\n",
    "            legend_elements += [Patch(facecolor=color_maxdist, label='extreme scenarios')]\n",
    "        plt.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    # use plotly\n",
    "    else:\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        if integer_index:\n",
    "            forin = [i for i in range(len(data_scenarios.index)) if i not in highlighted_indeces]\n",
    "        else:\n",
    "            forin = [i for i in data_scenarios.index if i not in highlighted_indeces]\n",
    "\n",
    "        opacity = max(0.001, min(1, 25/len(data_scenarios.index)))\n",
    "        showlegend = True\n",
    "        for i in forin:\n",
    "            if integer_index:\n",
    "                row = data_scenarios.iloc[i]\n",
    "            else:\n",
    "                row = data_scenarios.loc[i]\n",
    "\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x = maturities_yf,\n",
    "                y = row,\n",
    "                name = label_curves_bulk,\n",
    "                legendgroup = label_curves_bulk,\n",
    "                mode = default_plotly_scatter_mode,\n",
    "                showlegend = showlegend,\n",
    "                line=dict(color=\"Black\"),\n",
    "                opacity = opacity#0.08\n",
    "            ))\n",
    "            showlegend = False\n",
    "\n",
    "        showlegend = True\n",
    "        for i in highlighted_indeces:\n",
    "            if integer_index:\n",
    "                row = data_scenarios.iloc[i]\n",
    "            else:\n",
    "                row = data_scenarios.loc[i]\n",
    "                \n",
    "            fig.add_trace(go.Scatter(\n",
    "                x = maturities_yf,\n",
    "                y = row,\n",
    "                name = 'extreme scenarios',\n",
    "                legendgroup = 'extreme scenarios',\n",
    "                mode = default_plotly_scatter_mode,\n",
    "                showlegend = showlegend\n",
    "                ,line=dict(color=color_graphblue)\n",
    "            ))\n",
    "            showlegend = False\n",
    "\n",
    "        fig.add_trace(go.Scatter(x = maturities_yf, y = data_current, name = 'current curve', mode = default_plotly_scatter_mode, line=dict(color=\"LightCyan\"),))\n",
    "\n",
    "\n",
    "        fig.update_layout(\n",
    "            showlegend=True,\n",
    "            xaxis = dict(title_text = \"Expiry (in years)\"),\n",
    "            yaxis = dict(title_text = \"Interest rate (in percent)\"),\n",
    "            legend = dict(traceorder='reversed'),\n",
    "            title=get_default_title_dict(title_figure)\n",
    "        )\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# define a function that computes the 'distance' of the interest rate scenarios to given interest rate data and\n",
    "# sorts the scenarios by that distance.\n",
    "def computeDistancesAndSortDataFrame(\n",
    "    data_current,\n",
    "    data_scenarios,\n",
    "    considered_columns,\n",
    "    ascending = False,\n",
    "    output_with_distances = False,\n",
    "    distance_column_name = 'Distance'\n",
    "):\n",
    "    # compute the distances\n",
    "    diffs = data_scenarios[considered_columns] - data_current[considered_columns]\n",
    "    distances = [ np.linalg.norm(row, ord = 2) for index, row in diffs.iterrows() ]\n",
    "    \n",
    "    # add them as a column to a copy of the given scenario data\n",
    "    data_scenarios_with_dist = data_scenarios.copy()\n",
    "    data_scenarios_with_dist[distance_column_name] = distances\n",
    "    \n",
    "    # sort by distance\n",
    "    data_scenarios_with_dist.sort_values(by = distance_column_name, ascending = ascending, inplace=True)\n",
    "#     data_scenarios_with_dist = data_scenarios_with_dist.reset_index(drop=True) # use running number as index\n",
    "\n",
    "    if output_with_distances:\n",
    "        return data_scenarios_with_dist\n",
    "    else:\n",
    "        return data_scenarios_with_dist.drop(distance_column_name, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define a function that plots a histogram showing the interest rates of a given maturity\n",
    "def displayHistogramForGivenTenor(\n",
    "    data_scenarios,\n",
    "    maturity_string\n",
    "):\n",
    "    if maturity_string in data_scenarios.columns:\n",
    "        marker=dict(\n",
    "            color=color_histmarker,\n",
    "            line = dict(color = color_histmarkerborder, width = 1)\n",
    "        )\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Histogram(x = data_scenarios[maturity_string], marker = marker))\n",
    "\n",
    "        fig.update_layout(\n",
    "            showlegend=False,\n",
    "            title = get_default_title_dict(\"Histogram for maturity \\\"{0}\\\"\".format(maturity_string)),\n",
    "            xaxis = dict(title_text = \"Interest rate [%]\"),\n",
    "            yaxis = dict(title_text = \"Number of occurences\")\n",
    "        )\n",
    "\n",
    "        fig.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Use widgets to make the plot interactive\n",
    "def makeWidgetedHistogram(data_scenarios):\n",
    "    combobox_maturity = widgets.Combobox(\n",
    "        placeholder='Choose Maturity',\n",
    "        options=[col for col in data_scenarios.columns],\n",
    "        description='Maturity:',\n",
    "        ensure_option=True,\n",
    "        disabled=False\n",
    "    )\n",
    "\n",
    "    widgets.interact(\n",
    "        lambda maturity: displayHistogramForGivenTenor(data_scenarios, maturity),\n",
    "        maturity = combobox_maturity\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EONIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the scenarios and highlight 'most distant' ones\n",
    "disregarded_columns = ['1M', '2M', '4M', '5M', '7M', '8M', '10M', '11M']\n",
    "considered_columns = [c for c in columns_maturity_EONIA if c not in disregarded_columns]\n",
    "\n",
    "data_scenarios_EONIA_with_dist = computeDistancesAndSortDataFrame(\n",
    "    data_EONIA_current,\n",
    "    data_scenarios_EONIA,\n",
    "    considered_columns,\n",
    "    ascending = False,\n",
    "    output_with_distances = True,\n",
    "    distance_column_name = 'dist'\n",
    ")\n",
    "\n",
    "data_scenarios_EONIA_without_dist = data_scenarios_EONIA_with_dist.drop('dist', axis=1)\n",
    "\n",
    "# We'll use this later\n",
    "maxdist_hist = max(data_scenarios_EONIA_with_dist['dist'])\n",
    "\n",
    "plotScenarios(\n",
    "    data_EONIA_current,\n",
    "    data_scenarios_EONIA_without_dist,\n",
    "    maturities_EONIA_yf,\n",
    "    title_figure = \"Historical scenarios (EONIA)\",\n",
    "    label_curves_bulk = \"other scenarios\",\n",
    "    highlighted_indeces = [0,1,2,3],\n",
    "    integer_index = True,\n",
    "    use_matplotlib = False\n",
    ")\n",
    "\n",
    "del data_scenarios_EONIA_without_dist\n",
    "del disregarded_columns\n",
    "del considered_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at histograms showing the scenario data of specific tenors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeWidgetedHistogram(data_scenarios_EONIA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EUR3M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Plot the scenarios and highlight some extreme ones\n",
    "data_scenarios_EUR3M_with_dist = computeDistancesAndSortDataFrame(\n",
    "    data_EUR3M_current,\n",
    "    data_scenarios_EUR3M,\n",
    "    columns_maturity_EUR3M,\n",
    "    ascending = False,\n",
    "    output_with_distances = True,\n",
    "    distance_column_name = 'dist'\n",
    ")\n",
    "\n",
    "data_scenarios_EUR3M_without_dist = data_scenarios_EUR3M_with_dist.drop('dist', axis=1)\n",
    "\n",
    "plotScenarios(\n",
    "    data_EUR3M_current,\n",
    "    data_scenarios_EUR3M_without_dist,\n",
    "    maturities_EUR3M_yf,\n",
    "    title_figure = \"Historical scenarios (EUR3M)\",\n",
    "    label_curves_bulk = \"other scenarios\",\n",
    "    highlighted_indeces = [0,5,7,8],\n",
    "    integer_index = True,\n",
    "    use_matplotlib = False\n",
    ")\n",
    "\n",
    "del data_scenarios_EUR3M_with_dist\n",
    "del data_scenarios_EUR3M_without_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeWidgetedHistogram(data_scenarios_EUR3M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EUR6M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Plot the scenarios and highlight some extreme ones\n",
    "# data_scenarios_EUR6M_with_dist = computeDistancesAndSortDataFrame(\n",
    "#     data_EUR6M_current,\n",
    "#     data_scenarios_EUR6M,\n",
    "#     columns_maturity_EUR6M,\n",
    "#     ascending = False,\n",
    "#     output_with_distances = True,\n",
    "#     distance_column_name = 'dist'\n",
    "# )\n",
    "\n",
    "# data_scenarios_EUR6M_without_dist = data_scenarios_EUR6M_with_dist.drop('dist', axis=1)\n",
    "\n",
    "plotScenarios(\n",
    "    data_EUR6M_current,\n",
    "    data_scenarios_EUR6M,\n",
    "    maturities_EUR6M_yf,\n",
    "    title_figure = \"Historical scenarios (EUR6M)\",\n",
    "    label_curves_bulk = \"other scenarios\"\n",
    "#     ,highlighted_indeces = [0,1,2,3],\n",
    "#     integer_index = True,\n",
    "#     use_matplotlib = False\n",
    ")\n",
    "\n",
    "# del data_scenarios_EUR6M_with_dist\n",
    "# del data_scenarios_EUR6M_without_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeWidgetedHistogram(data_scenarios_EUR6M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case that the scenarios were constructed using relative historical changes, you'll probably find that some of them are rather extreme. To get a better understanding of why they are, we take a closer look at the scenarios containing the highest and the lowest interest rates found in any scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if scenario_construction_type == 'relative':\n",
    "    # print(data_scenarios_EONIA.min())\n",
    "    # print(data_scenarios_EONIA.idxmin())\n",
    "    # print(data_scenarios_EONIA.min().min())\n",
    "    # print(data_scenarios_EONIA.min().idxmin())\n",
    "    # print(imin)\n",
    "\n",
    "    imin = data_scenarios_EONIA.idxmin()[data_scenarios_EONIA.min().idxmin()]\n",
    "    imax = data_scenarios_EONIA.idxmax()[data_scenarios_EONIA.max().idxmax()]\n",
    "\n",
    "    display(\n",
    "        pd.DataFrame({\n",
    "            'Current': data_EONIA_current,\n",
    "            'imin': data_EONIA_rates_only.loc[imin,:],\n",
    "            'imin - n': data_EONIA_rates_only.loc[imin - timehorizon,:],\n",
    "            'Scenario (imin)': data_scenarios_EONIA.loc[imin,:],\n",
    "            'imax': data_EONIA_rates_only.loc[imax,:],\n",
    "            'imax - n': data_EONIA_rates_only.loc[imax - timehorizon,:],\n",
    "            'Scenario (imax)': data_scenarios_EONIA.loc[imax,:]\n",
    "        }).head(len(data_EONIA_current))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del data_EONIA\n",
    "# del data_EONIA_bootstrapped\n",
    "# del data_EONIA_current\n",
    "# del data_EONIA_raw\n",
    "# del data_scenarios_EONIA\n",
    "# del data_scenarios_EONIA_diff\n",
    "# del data_scenarios_EONIA_with_dist\n",
    "# del maturities_EONIA_dates\n",
    "# del maturities_EONIA_yf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Determine which curve data we are going to use in our MC simulation\n",
    "data_scenarios_MC = data_scenarios_EUR6M\n",
    "data_scenarios_MC_diff = data_scenarios_EUR6M_diff\n",
    "data_MC_current = data_EUR6M_current\n",
    "maturities_MC_yf = maturities_EUR6M_yf\n",
    "maturities_MC_dates = maturities_EUR6M_dates\n",
    "columns_maturity_MC = columns_maturity_EUR6M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random parallel shift\n",
    "We apply a random parallel shift to the current interest rate curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Generate scenarios\n",
    "n_sims_simpleMC = default_sample_size_MC\n",
    "np.random.seed(7001)\n",
    "random_shifts = np.random.normal(0, 0.02, n_sims_simpleMC)\n",
    "data_scenarios_random_shift = pd.DataFrame().reindex_like(data_scenarios_MC)\n",
    "data_scenarios_random_shift = data_scenarios_random_shift.iloc[0:0]\n",
    "\n",
    "for x in random_shifts:\n",
    "    data_scenarios_random_shift = data_scenarios_random_shift.append(data_MC_current + x, ignore_index=True)\n",
    "\n",
    "# data_scenarios_random_shift.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Plot scenarios\n",
    "plotScenarios(\n",
    "    data_MC_current,\n",
    "    data_scenarios_random_shift,\n",
    "    maturities_MC_yf,\n",
    "    title_figure = 'Monte Carlo scenarios',\n",
    "    label_curves_bulk = 'Monte Carlo scenarios',\n",
    "    use_matplotlib = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeWidgetedHistogram(data_scenarios_random_shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly picking historical data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using random indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Generate scenarios\n",
    "n_picks_indeces = default_sample_size_MC\n",
    "np.random.seed(7003)\n",
    "num_hist_scenarios = len(data_scenarios_MC.index)\n",
    "random_indeces= []\n",
    "\n",
    "while len(random_indeces) < n_picks_indeces:\n",
    "    sample = math.floor(abs(np.random.normal(0, num_hist_scenarios/1)))\n",
    "    if sample >= 0 and sample < num_hist_scenarios:\n",
    "        random_indeces.append(sample)\n",
    "\n",
    "data_scenarios_random_pick_indeces = pd.DataFrame().reindex_like(data_scenarios_MC)\n",
    "data_scenarios_random_pick_indeces = data_scenarios_random_pick_indeces.iloc[0:0]\n",
    "\n",
    "for i in random_indeces:\n",
    "    data_scenarios_random_pick_indeces = data_scenarios_random_pick_indeces.append(\n",
    "        data_scenarios_MC.iloc[num_hist_scenarios-i-1],\n",
    "        ignore_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Plot scenarios\n",
    "plotScenarios(\n",
    "    data_MC_current,\n",
    "    data_scenarios_random_pick_indeces,\n",
    "    maturities_MC_yf,\n",
    "    title_figure = 'Monte Carlo scenarios',\n",
    "    label_curves_bulk = 'Monte Carlo scenarios',\n",
    "    use_matplotlib = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeWidgetedHistogram(data_scenarios_random_pick_indeces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using multivariate normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Determine the covariance matrix and mean vector\n",
    "cov = data_scenarios_MC_diff.cov()\n",
    "mean = data_scenarios_MC_diff.mean(axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Generate scenarios\n",
    "np.random.seed(7007)\n",
    "data_scenarios_random_multivariate_normal_diff = np.random.multivariate_normal(\n",
    "    mean,\n",
    "    cov,\n",
    "    size = default_sample_size_MC,\n",
    "    check_valid = 'raise'\n",
    ")\n",
    "data_scenarios_random_multivariate_normal_diff = pd.DataFrame(\n",
    "    data = data_scenarios_random_multivariate_normal_diff,\n",
    "    columns = data_scenarios_MC_diff.columns\n",
    ")\n",
    "\n",
    "data_scenarios_random_multivariate_normal = applyScenarios(\n",
    "    data_MC_current,\n",
    "    data_scenarios_random_multivariate_normal_diff,\n",
    "    absolute = (scenario_construction_type == 'absolute')\n",
    ")\n",
    "\n",
    "# # Check quality\n",
    "# display(((cov - data_scenarios_random_multivariate_normal_diff.cov())/cov).abs().describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Plot scenarios\n",
    "plotScenarios(\n",
    "    data_MC_current,\n",
    "    data_scenarios_random_multivariate_normal,\n",
    "    maturities_MC_yf,\n",
    "    title_figure = 'Monte Carlo scenarios',\n",
    "    label_curves_bulk = 'Monte Carlo scenarios',\n",
    "    use_matplotlib = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeWidgetedHistogram(data_scenarios_random_multivariate_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation via short rate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create discount curve defined by the current interest rate data\n",
    "discountcurve_HW = getDiscountCurve(\n",
    "    'discountcurve_HW',\n",
    "    refdate,\n",
    "    maturities_MC_dates,\n",
    "    data_MC_current,\n",
    "    UnitInterestRate.PERCENT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hw_model = analytics.HullWhiteModel('HW_Model', refdate, 0.4, 0.002, discountcurve_HW) # EONIA\n",
    "hw_model = analytics.HullWhiteModel('HW_Model', refdate, 0.6, 0.0008, discountcurve_HW)\n",
    "hw_dc = model_tools.compute_yieldcurve(hw_model, refdate, maturities_MC_dates)\n",
    "\n",
    "mkt_plot.curve(discountcurve_HW, maturities_MC_dates, refdate, True)\n",
    "mkt_plot.curve(hw_dc, maturities_MC_dates, refdate, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Plot interest rate curve\n",
    "# fig = go.Figure()\n",
    "# year_fractions = []\n",
    "# for i in range(len(maturities_MC_dates)):\n",
    "#     year_fractions.append(default_daycounter.yf(refdate, maturities_MC_dates[i]))\n",
    "    \n",
    "# values = [x for x in data_MC_current]\n",
    "\n",
    "# fig.add_trace(go.Scatter(x = year_fractions, y = values, mode = 'lines+markers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_plot.curve(discountcurve_HW, maturities_MC_dates, refdate, False)\n",
    "mkt_plot.curve(hw_dc, maturities_MC_dates, refdate, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Plot discount curve based on current interest rate data and the yield curve produced by the hw_model\n",
    "# fig = go.Figure()\n",
    "\n",
    "# values = analytics.vectorDouble()\n",
    "# discountcurve_HW.value(values, refdate, maturities_MC_dates)\n",
    "# # convert to normal list\n",
    "# values = [x for x in values]\n",
    "\n",
    "# fig.add_trace(go.Scatter(x = year_fractions, y = values, mode = 'lines+markers'))\n",
    "     \n",
    "    \n",
    "# values = analytics.vectorDouble()\n",
    "# hw_dc.value(values, refdate, maturities_MC_dates)\n",
    "# # convert to normal list\n",
    "# values = [x for x in values]\n",
    "\n",
    "# fig.add_trace(go.Scatter(x = year_fractions, y = values, mode = 'lines+markers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Generate scenarios using hw_model\n",
    "sim_dates = converter.createPTimeList(refdate, maturities_MC_dates)\n",
    "refdate_LTime = converter.getLTime(refdate)\n",
    "n_sims = default_sample_size_MC\n",
    "n_steps_per_year = 250\n",
    "max_num_threads = 2\n",
    "\n",
    "hw_lab = analytics.ModelLab(hw_model, refdate_LTime)\n",
    "hw_lab.simulate(sim_dates, n_sims, n_steps_per_year, max_num_threads)\n",
    "\n",
    "# sampling_points_EONIA_datediffdays = [math.ceil(365*yf) for yf in maturities_EONIA_yf]\n",
    "# sim_dates = converter.createPTimeList(refdate, sampling_points_EONIA_datediffdays)\n",
    "\n",
    "\n",
    "# for i in range(n_sims):\n",
    "#     cir_lab.setFromSimulatedValues(cir, 1, i)  \n",
    "#     dc = model_tools.compute_yieldcurve(cir, sim_dates[0], sampling_points_EONIA_datediffdays)    \n",
    "#     mkt_plot.curve(dc, sim_dates, sim_dates[0], True, '', False)\n",
    "\n",
    "\n",
    "# data_array_df = []\n",
    "data_array_ir = []\n",
    "\n",
    "for i in range(n_sims):\n",
    "    hw_lab.setFromSimulatedValues(hw_model, 1, i)  \n",
    "    dc = model_tools.compute_yieldcurve(hw_model, sim_dates[0], sim_dates)  \n",
    "    daycounter = analytics.DayCounter(dc.getDayCounterType())\n",
    "#     data_array_df.append(\n",
    "#         [ dc.value(sim_dates[j], refdate) for j in range(len(sim_dates))]\n",
    "#     )\n",
    "    data_array_ir.append(\n",
    "        getInterestRatesFromDiscountCurve(\n",
    "            dc,\n",
    "            refdate,\n",
    "            sim_dates,\n",
    "            UnitInterestRate.PERCENT\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # mkt_plot.curve(dc, sim_dates, sim_dates[0], True, '', False)\n",
    "\n",
    "    \n",
    "data_scenarios_hull_white = pd.DataFrame(\n",
    "    data = data_array_ir,\n",
    "    columns = columns_maturity_MC\n",
    ")\n",
    "\n",
    "# del data_array_df\n",
    "del data_array_ir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Plot scenarios\n",
    "plotScenarios(\n",
    "    data_MC_current,\n",
    "    data_scenarios_hull_white,\n",
    "    maturities_MC_yf,\n",
    "    title_figure = 'Monte Carlo scenarios',\n",
    "    label_curves_bulk = 'Monte Carlo scenarios',\n",
    "    use_matplotlib = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeWidgetedHistogram(data_scenarios_hull_white)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VaR computation\n",
    "\n",
    "## Simple portfolio\n",
    "To keep things simple, we start off with a portfolio containing only one fixed coupon bond with the following specifications:\n",
    " - It was issued on 2019/12/30\n",
    " - It has a maturity of 10 years\n",
    " - Its principal is 100\n",
    " - It pays a 5 coupon every year\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Define the bond\n",
    "maturity = 10\n",
    "principal = 100.0\n",
    "coupon_rate = 0.05\n",
    "maturity_date = dt.datetime(year = refdate.year + maturity, month = refdate.month, day = refdate.day)\n",
    "#print(refdate)\n",
    "#print(maturity_date)\n",
    "\n",
    "# Generate the coupon payment schedule as a vector of datetimes\n",
    "coupon_dates = []\n",
    "for i in range(maturity):\n",
    "#     coupon_dates.append(dt.datetime(year = refdate.year + i, month = refdate.month, day = refdate.day) + dt.timedelta(days = 90))\n",
    "#     coupon_dates.append(dt.datetime(year = refdate.year + i, month = refdate.month, day = refdate.day) + dt.timedelta(days = 180))\n",
    "#     coupon_dates.append(dt.datetime(year = refdate.year + i, month = refdate.month, day = refdate.day) + dt.timedelta(days = 270))\n",
    "    coupon_dates.append(dt.datetime(year = refdate.year + i + 1, month = refdate.month, day = refdate.day))\n",
    "# print(coupon_dates)\n",
    "coupon_rates = [coupon_rate]*len(coupon_dates)\n",
    "# coupon_payments = [coupon_rate*principal]*len(coupon_dates)\n",
    "# print(coupon_payments)\n",
    "\n",
    "# We now use these specifications to define a fixed coupon bond\n",
    "fixed_coupon_bond = pyvacon.instruments.BondSpecification('Fixed_Coupon', 'DBK', enums.SecuritizationLevel.NONE, 'EUR',\n",
    "    maturity_date, refdate, principal, default_daycounter_type, coupon_dates, coupon_rates, '',[], [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current value of this portfolio can be computed by simply summing over all discounted future cash flows. Therefore, the only market variables affecting this value are the interest rates we use to determine the discount factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Credit Spread and Portfolio Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define the pricer, we're going to use to price our bond\n",
    "pricing_data_bond = pyvacon.pricing.BondPricingData()\n",
    "pricing_data_bond.param = pyvacon.pricing.BondPricingParameter()\n",
    "pricing_data_bond.param.useJLT = False\n",
    "pricing_data_bond.pricingRequest = pyvacon.pricing.PricingRequest()\n",
    "pricing_data_bond.pricingRequest.setCleanPrice(True)\n",
    "pricing_data_bond.pricer = 'BondPricer'\n",
    "pricing_data_bond.spec = fixed_coupon_bond\n",
    "\n",
    "valdate = refdate # + dt.timedelta(days = timehorizon)\n",
    "pricing_data_bond.valDate = valdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     10,
     39
    ]
   },
   "outputs": [],
   "source": [
    "# Define functions that compute the values of the simple portfolio\n",
    "def ComputeValueOfSimplePortfolio(\n",
    "    refdate,\n",
    "    maturities_dates,\n",
    "    scenario,\n",
    "    creditspread,\n",
    "    unit_interest_rates,\n",
    "    daycounter_type = default_daycounter_type,\n",
    "    interpolation_type = default_interpolation_type,\n",
    "    extrapolation_type = default_extrapolation_type\n",
    "):\n",
    "    # Add the credit spread we computed for our bond\n",
    "    scenario = scenario + creditspread\n",
    "\n",
    "    # Compute the price of the fixed coupon bond at the given valuation date\n",
    "    pricing_data_bond.discountCurve = getDiscountCurve(\n",
    "        'Discount Curve',\n",
    "        refdate,\n",
    "        maturities_dates,\n",
    "        scenario,\n",
    "        UnitInterestRate.PERCENT,\n",
    "        daycounter_type,\n",
    "        interpolation_type,\n",
    "        extrapolation_type\n",
    "    )\n",
    "    results = pyvacon.pricing.price(pricing_data_bond)\n",
    "    \n",
    "    return [results.getPrice(), results.getCleanPrice()]\n",
    "\n",
    "\n",
    "def ComputeValuesOfSimplePortfolio(\n",
    "    refdate,\n",
    "    maturities_dates,\n",
    "    data_scenarios,\n",
    "    creditspread,\n",
    "    unit_interest_rates,\n",
    "    daycounter_type = default_daycounter_type,\n",
    "    interpolation_type = default_interpolation_type,\n",
    "    extrapolation_type = default_extrapolation_type\n",
    "):\n",
    "    if not isinstance(data_scenarios, pd.DataFrame):\n",
    "        raise Exception(\"Scenario data needs to be provided via a pandas.DataFrame.\")\n",
    "        \n",
    "    # Compute the price of the fixed coupon bond at the given valuation date\n",
    "    # Repeat for every scenario\n",
    "    results_dirty = []\n",
    "    results_clean = []\n",
    "    \n",
    "    for unused, scenario in data_scenarios.iterrows():\n",
    "        results = ComputeValueOfSimplePortfolio(\n",
    "            refdate,\n",
    "            maturities_dates,\n",
    "            scenario,\n",
    "            creditspread,\n",
    "            unit_interest_rates,\n",
    "            daycounter_type,\n",
    "            interpolation_type,\n",
    "            extrapolation_type\n",
    "        )\n",
    "        \n",
    "        results_dirty.append(results[0])\n",
    "        results_clean.append(results[1])\n",
    "        \n",
    "    return [results_dirty, results_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Choose the scenarios that we're going to use\n",
    "data_scenarios = data_scenarios_EUR6M\n",
    "maturities_dates = maturities_EUR6M_dates\n",
    "data_current = data_EUR6M_current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We are currently not taking portfolio aging into account. In the computations below, we are using the reference date as valuation date. That is, we look at the effects our shift scenarios would have on the value of our portfolio, if they were to happen instantaneously (instead of over the next $n$ days)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Credit Spread\n",
    "**Credit spread** is the difference in yield between two investments of similar maturities, but different credit qualities. It can be interpreted as the risk premium for one investment over the other.\n",
    "\n",
    "Given the low EONIA rates, the bond we defined above currently has a much higher yield than a hypothetical bond paying EONIA rates on the same principal. Pricing the bond using discount factors based on EONIA rates would grossly overestimate its value. That is, the price we compute would be a lot higher than the bond's actual market value. To avoid this, we determine the constant shift we need to apply to the interest rates used for discounting in order for our price to equal the market value of the bond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Use the current interest rates + a constant rate to compute the price of the fixed coupon bond\n",
    "# Vary the constant rate and repeat until the value of the bond is right about the same as its principal\n",
    "creditspread = coupon_rate * 100 # in percent\n",
    "stepsize = coupon_rate * 100 # the initial step size used to vary the interest rate\n",
    "spreads = []\n",
    "values = []\n",
    "for k in range(20):\n",
    "    results = ComputeValueOfSimplePortfolio(\n",
    "        refdate,\n",
    "        maturities_dates,\n",
    "        data_current,\n",
    "        creditspread,\n",
    "        UnitInterestRate.PERCENT\n",
    "    )\n",
    "    \n",
    "    values.append(results[0])\n",
    "    spreads.append(creditspread)\n",
    "    \n",
    "    if values[k] > principal:\n",
    "        creditspread += stepsize\n",
    "    else:\n",
    "        creditspread -= stepsize\n",
    "    stepsize /= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The credit spread is {{round(creditspread, 3)}}%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Compute the current value without taking the credit spread into account\n",
    "results = ComputeValueOfSimplePortfolio(\n",
    "    refdate,\n",
    "    maturities_dates,\n",
    "    data_current,\n",
    "    0,\n",
    "    UnitInterestRate.PERCENT\n",
    ")\n",
    "\n",
    "current_value_without_credit_spread = results[0]\n",
    "\n",
    "del results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Compute the current value\n",
    "results = ComputeValueOfSimplePortfolio(\n",
    "    refdate,\n",
    "    maturities_dates,\n",
    "    data_current,\n",
    "    creditspread,\n",
    "    UnitInterestRate.PERCENT\n",
    ")\n",
    "\n",
    "current_value = results[0]\n",
    "\n",
    "del results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking into account the credit spread we computed above, we arrive at a current value of {{round(current_value, 6)}}, which closely matches the actual market value of 100. If we didn't take the credit spread into account, we would arrive at a value of {{round(current_value_without_credit_spread, 2)}}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='computePortfolioValues'></a>\n",
    "#### Compute Portfolio Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Compute the values of our simple portfolio in the generated scenarios\n",
    "results, unused = ComputeValuesOfSimplePortfolio(\n",
    "    refdate,\n",
    "    maturities_dates,\n",
    "    data_scenarios,\n",
    "    creditspread,\n",
    "    UnitInterestRate.PERCENT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot pricing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define a function that plots the histrograms\n",
    "def plotHistogram(\n",
    "    data,\n",
    "    binsstart,\n",
    "    binsend,\n",
    "    nbins,\n",
    "    title_xaxis,\n",
    "    markercolor = color_histmarker,\n",
    "    bordercolor = color_histmarkerborder,\n",
    "    title_figure = None\n",
    "):\n",
    "    xbins = dict(start = binsstart, end = binsend, size = (binsend-binsstart)/nbins)\n",
    "    marker=dict(\n",
    "        color=markercolor,\n",
    "        line = dict(color = bordercolor, width = 1)\n",
    "    )\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Histogram(x = data, xbins = xbins, marker = marker))\n",
    "\n",
    "    fig.update_layout(\n",
    "        showlegend=False,\n",
    "        xaxis = dict(title_text = title_xaxis, range = [binsstart, binsend]),\n",
    "        yaxis = dict(title_text = \"Number of occurences\")\n",
    "    )\n",
    "    \n",
    "    if title_figure != None and title_figure != \"\":\n",
    "        fig.update_layout(\n",
    "            title = get_default_title_dict(title_figure)\n",
    "        )\n",
    "\n",
    "    fig.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Histogramm of the pricing results\n",
    "# plotHistogram(\n",
    "#     data = results_dirty,\n",
    "#     binsstart = xmin,\n",
    "#     binsend = xmax,\n",
    "#     nbins = 60,\n",
    "#     title_xaxis = \"Portfolio value\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Histogramm of the changes/differences in value\n",
    "diffs_value = np.asarray([res - current_value for res in results])\n",
    "binsstart_simple = min(diffs_value)\n",
    "binsend_simple = max(diffs_value)\n",
    "\n",
    "plotHistogram(\n",
    "    data = diffs_value,\n",
    "    binsstart = binsstart_simple,\n",
    "    binsend = binsend_simple,\n",
    "    nbins = 60,\n",
    "    title_xaxis = \"Change in value\",\n",
    "    title_figure = \"Changes in portfolio value\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='computeVaR'></a>\n",
    "### Compute value at risk\n",
    "Let's recapitulate the definition of value at risk.\n",
    "\n",
    "> Given a time horizon of $n$ days and a confidence level $\\alpha$, the VaR is the loss of value that has the probability $\\alpha$ not to be exceeded within the next $n$ days. In other words, the VaR is the $\\alpha$-quantile of the distribution of loss in the value of a portfolio over the next $n$ days.\n",
    "\n",
    "We already took the time horizon into account when we generated our scenarios. Let's assume we want $\\alpha$ to be 99%. How do we determine the value at risk?\n",
    "\n",
    "**Simple example:** If we have exactly 100 scenarios, then the 99%-quantile of the resulting cumulative relative frequency distribution is simply the second biggest loss occurring in our simulation. In other words, if we sort the losses in ascending order, the value at risk is the 99th one.\n",
    "\n",
    "<a id='VaR_formula'></a>\n",
    "**General case:**\n",
    "Let $0 < \\alpha < 1$ be the chosen confidence level, $n_s$ be the number of scenarios we generated and let the losses $l_1, \\dots, l_{n_s}$ be sorted in ascending order, i.e. $l_i < l_j$ for all $i < j$. Then we determine the value at risk as follows.\n",
    "$$\\text{VaR} = \\left( 1 - \\lambda \\right) \\cdot l_{\\lfloor n_s \\cdot \\alpha \\rfloor} + \\lambda \\cdot l_{\\lfloor n_s \\cdot \\alpha \\rfloor + 1}$$\n",
    "where $\\lambda = \\left( n_s \\cdot \\alpha - \\lfloor n_s \\cdot \\alpha \\rfloor \\right)$.\n",
    "\n",
    "*Note: In the case that $n_s \\cdot \\alpha$ is an integer, this formula reads $\\text{VaR} = l_{n_s \\cdot \\alpha}$. It is, therefore, consistent with the simple example above.*\n",
    "\n",
    "We already computed the changes in portfolio value in all scenarios we generated (see [above](#computePortfolioValues)). We will now use that information to determine and sort the losses in value. Afterwards, we can compute the value at risk using the formula above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     8,
     113
    ]
   },
   "outputs": [],
   "source": [
    "# Define a function which displays info about scenarios with highest losses together with their cumulative relative frequencies\n",
    "def getHTMLTableLossesFrequencies(\n",
    "    losses_input,\n",
    "    confidence_level,\n",
    "    basic_table_only,\n",
    "    compute_VaR,\n",
    "    compute_ES_weights,\n",
    "    highlight_ES_relevant\n",
    "):\n",
    "    if(confidence_level < 0 or confidence_level > 1):\n",
    "        raise Exception(\"The confidence level needs to be a value between 0 and 1.\")\n",
    "        \n",
    "    losses = np.sort(losses_input)\n",
    "    _num_scenarios = len(losses)\n",
    "    \n",
    "    _index_ceil = np.ceil(_num_scenarios*confidence_level).astype(int) - 1 # first index where the cumul. rel. frequ. is above the threshold set by the chosen confidence level\n",
    "    num_additional_rows = 2\n",
    "    index_first_displayed = _index_ceil - num_additional_rows\n",
    "    \n",
    "    rangestart = index_first_displayed\n",
    "    \n",
    "    # Collect info about the scenarios with highest losses together with their cumulative relative frequency\n",
    "    _data_scenario_names = [i for i in range(rangestart, _num_scenarios+1)]\n",
    "    _data_crf = [100*i/_num_scenarios for i in range(rangestart, _num_scenarios+1)]\n",
    "    _data_losses = ['{:.4f}'.format(losses[i]) for i in range(rangestart-1, _num_scenarios)]\n",
    "    if not basic_table_only:\n",
    "        _data_weights = ['-' if x <= 100*confidence_level else '1' for x in _data_crf]\n",
    "    \n",
    "    if not basic_table_only:\n",
    "        _data_scenario_names += ['VaR']\n",
    "        _data_crf += [confidence_level*100]\n",
    "        if compute_VaR:\n",
    "            _data_losses += ['{:.4f}'.format(computeVaR(confidence_level, losses_input))]\n",
    "        else:\n",
    "            _data_losses += ['?']\n",
    "            \n",
    "    _data = [\n",
    "        _data_scenario_names,\n",
    "        _data_crf,\n",
    "        _data_losses\n",
    "    ]\n",
    "    _index = [\n",
    "        'Scenario',\n",
    "        'Cumulative relative frequency [%]',\n",
    "        'Loss'\n",
    "    ]\n",
    "            \n",
    "        \n",
    "    if compute_ES_weights and not basic_table_only:\n",
    "        unused, _weight_VaR = computeES(confidence_level, losses_input, output_weight_VaR=True)\n",
    "        _data_weights += ['{:.4f}'.format(_weight_VaR)]\n",
    "        _data += [_data_weights]\n",
    "        _index += ['Weights in ES formula']\n",
    "    \n",
    "    \n",
    "    # Put the info into a DataFrame\n",
    "    df_table = pd.DataFrame(\n",
    "        data = _data,\n",
    "        index = _index\n",
    "    ).transpose()\n",
    "    \n",
    "    # Sort by cumulative relative frequency and format floats afterwards\n",
    "    df_table.sort_values(by = 'Cumulative relative frequency [%]', ascending = True, inplace = True)\n",
    "    df_table['Cumulative relative frequency [%]'] = df_table['Cumulative relative frequency [%]'].map('{:7.3f}'.format)\n",
    "\n",
    "    # Display and apply css to output\n",
    "    tablestyles = [\n",
    "        dict(selector='tr *', props=[('text-align', 'center')]),\n",
    "        dict(selector='', props=[\n",
    "            ('margin-left', 'auto'),\n",
    "            ('margin-right', 'auto')\n",
    "        ])\n",
    "    ]\n",
    "    \n",
    "    if highlight_ES_relevant and not basic_table_only:\n",
    "        # the parameter we're going to use with the nth-child CSS selector\n",
    "        startat = num_additional_rows + 1 + 1\n",
    "        \n",
    "        # Add highlight to table styles\n",
    "        tablestyles_highlight = [\n",
    "            dict(\n",
    "                selector='tbody tr:nth-child(n+{0})'.format(startat),\n",
    "                props=[\n",
    "                    ('border-left', '2px solid %s' % color_graphblue),\n",
    "                    ('border-right', '2px solid %s' % color_graphblue)\n",
    "                ]\n",
    "            ),\n",
    "            dict(\n",
    "                selector='tbody tr:nth-child({0})'.format(startat),\n",
    "                props=[('border-top', '2px solid %s' % color_graphblue)]\n",
    "            ),\n",
    "            dict(\n",
    "                selector='tbody tr:nth-last-child(1)',\n",
    "                props=[('border-bottom', '2px solid %s' % color_graphblue)]\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        tablestyles += tablestyles_highlight\n",
    "    \n",
    "\n",
    "    \n",
    "    styler = df_table.style.set_table_styles(tablestyles).hide_index()\n",
    "    \n",
    "    return styler._repr_html_()\n",
    "    \n",
    "    \n",
    "def displayTableLossesFrequencies(\n",
    "    losses_input,\n",
    "    confidence_level,\n",
    "    basic_table_only = False,\n",
    "    compute_VaR = False,\n",
    "    compute_ES_weights = False,\n",
    "    highlight_ES_relevant = False\n",
    "):    \n",
    "    html = getHTMLTableLossesFrequencies(\n",
    "        losses_input = losses_input,\n",
    "        confidence_level = confidence_level,\n",
    "        basic_table_only = basic_table_only,\n",
    "        compute_VaR = compute_VaR,\n",
    "        compute_ES_weights = compute_ES_weights,\n",
    "        highlight_ES_relevant = highlight_ES_relevant\n",
    "    )\n",
    "    display_html(html, raw = True)\n",
    "    \n",
    "    \n",
    "# TODO\n",
    "# def displayMultipleTablesLossesFrequencies(\n",
    "#     losses_inputs,\n",
    "#     indeces_first_displayed,\n",
    "#     additional_tablestyles = None,\n",
    "#     side_by_side = False\n",
    "# ):    \n",
    "#     if len(losses_inputs) != len(indeces_first_displayed):\n",
    "#         raise Exception(\"The list losses_inputs must have the same length as indeces_first_displayed.\")\n",
    "    \n",
    "#     if additional_tablestyles != None and len(additional_tablestyles) != len(losses_inputs):\n",
    "#         raise Exception(\"The list additional_tablestyles, if supplied, must have the same length as losses_inputs and indeces_first_displayed.\")\n",
    "        \n",
    "#     joint_html = \"\"\n",
    "#     for i in range(len(losses_inputs)):\n",
    "#         html = getHTMLTableLossesFrequencies(\n",
    "#             losses_inputs[i],\n",
    "#             indeces_first_displayed[i],\n",
    "#             additional_tablestyles = additional_tablestyles[i] if additional_tablestyles != None else None\n",
    "#         )\n",
    "        \n",
    "#         joint_html += html\n",
    "        \n",
    "#         if not side_by_side:\n",
    "#             display_html(html, raw = True)\n",
    "    \n",
    "#     if side_by_side:\n",
    "#         display_html(joint_html, raw = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define a function which computes the value at risk\n",
    "def computeVaR(\n",
    "    confidence_level,\n",
    "    losses_input\n",
    "):\n",
    "    if(confidence_level < 0 or confidence_level > 1):\n",
    "        raise Exception(\"The confidence level needs to be a value between 0 and 1.\")\n",
    "        \n",
    "    loss_scenarios = np.sort(losses_input)\n",
    "    num_scenarios = len(loss_scenarios)\n",
    "    \n",
    "    # Compute the number of the entry corresponding to the confidence_level; round down if it's not an integer\n",
    "    floor = np.floor(num_scenarios*confidence_level).astype(int)  \n",
    "    \n",
    "    # To get the index of this entry, we have to subtract 1\n",
    "    floor_index = floor - 1\n",
    "    \n",
    "    # Compute VaR\n",
    "    if(floor == num_scenarios):\n",
    "        VaR = loss_scenarios[floor_index]\n",
    "    elif(floor == 0):\n",
    "        VaR = loss_scenarios[0]\n",
    "    else:\n",
    "        lam = num_scenarios * confidence_level - floor\n",
    "        VaR = (1-lam) * loss_scenarios[floor_index] + lam * loss_scenarios[floor_index + 1]\n",
    "    \n",
    "    return VaR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Determine losses\n",
    "alpha = 0.99\n",
    "\n",
    "# Determine losses and sort in ascending order\n",
    "losses = (-1)*diffs_value\n",
    "losses = np.sort(losses)\n",
    "\n",
    "# We will use the following variables in a markdown cell below\n",
    "percentage = 100*alpha\n",
    "num_scenarios = len(losses)\n",
    "index_ceil = np.ceil(num_scenarios*alpha).astype(int) - 1 # first index where the cumul. rel. frequ. is above the threshold set by the chosen confidence level\n",
    "probability_ceil = (index_ceil + 1)/num_scenarios*100\n",
    "probability_floor = (index_ceil)/num_scenarios*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first take a look at the ordered list of losses and their respective cumulative relative frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Display scenarios with highest losses together with their cumulative relative frequency\n",
    "displayTableLossesFrequencies(\n",
    "    losses,\n",
    "    confidence_level = alpha\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working with an empirical probability distribution that is based on {{num_scenarios}} scenarios, we aren't able to match the given confidence level exactly. Looking at the cumulative relative frequencies of the highest losses we simulated (see table above), we can see that scenario {{index_ceil+1}} with a loss of {{round(losses\\[index_ceil\\], 4)}} is the first scenario to cross the threshold of {{percentage}}%.\n",
    "\n",
    "Depending on the chosen confidence level and the number of scenarios, the cumulative relative frequencies immediately below and above the confidence level (in our case {{round(probability_floor, 4)}}% and {{round(probability_ceil, 4)}}%) can be close to the confidence level or quite far away. In order not to systematically underestimate or overestimate the value at risk, we linearly interpolate between these two scenarios (see <a href = '#VaR_formula'>formula above</a>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Compute VaR\n",
    "VaR_simple = computeVaR(alpha, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value at risk is {{round(VaR_simple, 4)}}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot cumulative relative frequencies of loss of portfolio value\n",
    "marker=dict(\n",
    "    color=color_histmarker\n",
    ")\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x = losses, nbinsx = len(losses)*2, marker = marker, cumulative_enabled = True, histnorm = 'probability', name = 'cumulative relative frequency distribution'))\n",
    "fig.add_trace(go.Scatter(x = [VaR_simple, VaR_simple], y = [0, alpha], mode = 'lines', line = dict(color='Orange', width = 1), hoverinfo='skip'))\n",
    "fig.add_trace(go.Scatter(x = [losses[0], VaR_simple], y = [alpha, alpha], mode = 'lines', line = dict(color='Orange', width = 1), hoverinfo='skip'))\n",
    "\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    xaxis = dict(title_text = \"Loss of portfolio value\"),\n",
    "    yaxis = dict(title_text = \"Cumulative relative frequency\")\n",
    ")\n",
    "\n",
    "fig.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del percentage\n",
    "del probability_ceil\n",
    "del probability_floor\n",
    "del diffs_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute expected shortfall\n",
    "The expected shortfall is the loss we expect to take given that the loss is bigger than or equal to the value at risk.\n",
    "\n",
    "<b>Simple example:</b>\n",
    "Let's again assume that we are working with 100 scenarios and a confidence level of 99%. As explained <a href = '#computeVaR'>above</a> the value at risk is then equal to the second biggest loss occurring in our scenarios. The expected shortfall is the average over the two biggest losses.\n",
    "\n",
    "<b>General case:</b>\n",
    "Let $0 < \\alpha < 1$ be the chosen confidence level, $n_s$ be the number of scenarios we generated and let the losses $l_1, \\dots, l_{n_s}$ be sorted in ascending order, i.e. $l_i < l_j$ for all $i < j$. Then we define the expected shortfall to be the following weighted average.\n",
    "\\begin{align}\n",
    "\\text{ES} &= \\frac{1}{\\left( \\lfloor n_s \\cdot \\alpha \\rfloor + 1 - n_s \\cdot \\alpha \\right) + \\left(n_s - \\lfloor n_s \\cdot \\alpha \\rfloor \\right)}\n",
    "\\cdot \\left(\n",
    "\\left( \\lfloor n_s \\cdot \\alpha \\rfloor + 1 - n_s \\cdot \\alpha \\right) \\cdot \\text{VaR} \\enspace\n",
    "+\n",
    "\\sum_{i = \\lfloor n_s \\cdot \\alpha \\rfloor+1}^{n_s} l_i\n",
    "\\right) \\\\\n",
    "&= \\frac{1}{n_s - n_s \\cdot \\alpha + 1}\n",
    "\\cdot \\left(\n",
    "\\left( \\lfloor n_s \\cdot \\alpha \\rfloor + 1 - n_s \\cdot \\alpha \\right) \\cdot \\text{VaR} \\enspace\n",
    "+\n",
    "\\sum_{i = \\lfloor n_s \\cdot \\alpha \\rfloor+1}^{n_s} l_i\n",
    "\\right)\n",
    "\\end{align}\n",
    "\n",
    "If $n_s \\cdot \\alpha$ is an integer, we have $\\lfloor n_s \\cdot \\alpha \\rfloor = n_s \\cdot \\alpha$ and $\\text{VaR} = l_{n_s \\cdot \\alpha}$. In that case, the expected shortfall is simply the average over all losses bigger than or equal to $l_{n_s \\cdot \\alpha}$.\n",
    "\\begin{align}\n",
    "\\text{ES} \n",
    "&= \\frac{1}{n_s - n_s \\cdot \\alpha + 1}\n",
    "\\cdot \\left(\n",
    "\\left( \\lfloor n_s \\cdot \\alpha \\rfloor + 1 - n_s \\cdot \\alpha \\right) \\cdot \\text{VaR} \\enspace\n",
    "+\n",
    "\\sum_{i = \\lfloor n_s \\cdot \\alpha \\rfloor+1}^{n_s} l_i\n",
    "\\right)\\\\\n",
    "&= \\frac{1}{n_s - n_s \\cdot \\alpha + 1}\n",
    "\\cdot \\left(\n",
    "\\left( n_s \\cdot \\alpha + 1 - n_s \\cdot \\alpha \\right) \\cdot l_{n_s \\cdot \\alpha} \\enspace\n",
    "+\n",
    "\\sum_{i = n_s \\cdot \\alpha + 1}^{n_s} l_i\n",
    "\\right)\\\\\n",
    "&= \\frac{1}{n_s - n_s \\cdot \\alpha + 1}\n",
    "\\cdot \\left(\n",
    "1 \\cdot l_{n_s \\cdot \\alpha} \\enspace\n",
    "+\n",
    "\\sum_{i = n_s \\cdot \\alpha + 1}^{n_s} l_i\n",
    "\\right)\\\\\n",
    "&= \\frac{1}{n_s - n_s \\cdot \\alpha + 1} \\cdot\n",
    "\\sum_{i = n_s \\cdot \\alpha}^{n_s} l_i\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define a function which computes the expected shortfall\n",
    "def computeES(\n",
    "    confidence_level,\n",
    "    losses_input,\n",
    "    output_weight_VaR = False\n",
    "):\n",
    "    if(confidence_level < 0 or confidence_level > 1):\n",
    "        raise Exception(\"The confidence level needs to be a value between 0 and 1.\")\n",
    "        \n",
    "    loss_scenarios = np.sort(losses_input)\n",
    "    num_scenarios = len(loss_scenarios)\n",
    "    \n",
    "    # Compute the number of the entry corresponding to the confidence_level; round down if it's not an integer\n",
    "    floor = np.floor(num_scenarios*confidence_level).astype(int)  \n",
    "    \n",
    "    # To get the index of this entry, we have to subtract 1\n",
    "    floor_index = floor - 1\n",
    "    \n",
    "    # Compute VaR\n",
    "    if(floor == num_scenarios):\n",
    "        ES = loss_scenarios[floor_index]\n",
    "    elif(floor == 0):\n",
    "        ES = np.average(loss_scenarios)\n",
    "    else:\n",
    "        w = floor + 1 - num_scenarios * confidence_level\n",
    "        weights = [w] + ([1] * (num_scenarios - floor))\n",
    "        summands = [computeVaR(confidence_level, losses_input)] + [loss_scenarios[i] for i in range(floor_index+1, len(loss_scenarios))]\n",
    "        ES = np.average(summands, weights = weights)\n",
    "    \n",
    "    if output_weight_VaR:\n",
    "        return ES, w\n",
    "    else:\n",
    "        return ES\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Display the same table as above, but highlight the scenarios relevant for computing the expected shortfall\n",
    "displayTableLossesFrequencies(\n",
    "    losses,\n",
    "    confidence_level = alpha,\n",
    "    compute_VaR = True,\n",
    "    compute_ES_weights = True,\n",
    "    highlight_ES_relevant = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Compute the expected shortfall\n",
    "expected_shortfall_simple = computeES(alpha, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected shortfall is {{round(expected_shortfall_simple, 4)}}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del alpha\n",
    "del num_scenarios\n",
    "del index_ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended portfolio (add a swap)\n",
    "We swap the fixed coupon payments for interest payments based on EONIA rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define the swap scpecification\n",
    "startdates = [refdate]\n",
    "startdates.extend(coupon_dates[0:len(coupon_dates)-1])\n",
    "#startdates = converter.createPTimeList(refdate, startdates)\n",
    "\n",
    "enddates = coupon_dates\n",
    "#enddates = converter.createPTimeList(enddates, startdates)\n",
    "\n",
    "#print(startdates)\n",
    "#print(enddates)\n",
    "\n",
    "paydates = enddates\n",
    "resetdates = startdates\n",
    "\n",
    "notionals = analytics.vectorDouble()\n",
    "notionals.append(principal)\n",
    "\n",
    "fixedleg = analytics.IrFixedLegSpecification(coupon_rate, notionals, startdates, enddates, paydates,'EUR', default_daycounter_type)\n",
    "\n",
    "floatleg = analytics.IrFloatLegSpecification(notionals, resetdates, startdates, enddates,\n",
    "                                    paydates,'EUR', 'test_udl', default_daycounter_type, \n",
    "                                    0)\n",
    "                                    #creditspread/100) # spread is given in percent\n",
    "\n",
    "ir_swap = analytics.InterestRateSwapSpecification('TEST_SWAP', 'DBK', enums.SecuritizationLevel.COLLATERALIZED, 'EUR',\n",
    "                                           converter.getLTime(paydates[-1]), fixedleg, floatleg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recompute the value of our portfolio in all scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Specify all data we need to price the swap\n",
    "pricing_data_swap = analytics.InterestRateSwapPricingData()\n",
    "\n",
    "pricing_data_pay_leg = analytics.InterestRateSwapLegPricingData()\n",
    "pricing_data_pay_leg.spec = ir_swap.getPayLeg()\n",
    "pricing_data_pay_leg.fxRate = 1.0\n",
    "pricing_data_pay_leg.weight = -1.0\n",
    "\n",
    "pricing_data_rec_leg = analytics.InterestRateSwapFloatLegPricingData()\n",
    "pricing_data_rec_leg.spec = ir_swap.getReceiveLeg()\n",
    "pricing_data_rec_leg.fxRate = 1.0\n",
    "pricing_data_rec_leg.weight = 1.0\n",
    "\n",
    "pricing_data_swap.pricer = 'InterestRateSwapPricer'\n",
    "pricing_data_swap.pricingRequest = analytics.PricingRequest()\n",
    "pricing_data_swap.valDate = converter.getLTime(refdate)\n",
    "pricing_data_swap.setCurr('EUR')\n",
    "pricing_data_swap.addLegData(pricing_data_pay_leg)\n",
    "pricing_data_swap.addLegData(pricing_data_rec_leg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Compute the price of our portfolio in every scenario\n",
    "results_dirty = []\n",
    "results_clean = []\n",
    "\n",
    "# results_dirty_bond = []\n",
    "# results_clean_bond = []\n",
    "\n",
    "# results_dirty_swap = []\n",
    "# results_clean_swap = []\n",
    "\n",
    "for index, scenario in data_scenarios.iterrows():\n",
    "    dc_scenario = getDiscountCurve(\n",
    "        'dcScenario',\n",
    "        refdate,\n",
    "        maturities_dates,\n",
    "        scenario,\n",
    "        UnitInterestRate.PERCENT\n",
    "    )\n",
    "    \n",
    "    dc_withspread = getDiscountCurve(\n",
    "        'dcWithSpread',\n",
    "        refdate,\n",
    "        maturities_dates,\n",
    "        scenario + creditspread,\n",
    "        UnitInterestRate.PERCENT\n",
    "    )\n",
    "    \n",
    "    pricing_data_bond.discountCurve = dc_scenario # dc_withspread\n",
    "    pricing_data_pay_leg.discountCurve = dc_scenario # dc_withspread\n",
    "    pricing_data_rec_leg.discountCurve = dc_scenario\n",
    "    pricing_data_rec_leg.fixingCurve = dc_scenario\n",
    "    \n",
    "    price_bond = pyvacon.pricing.price(pricing_data_bond)\n",
    "    price_swap = analytics.price(pricing_data_swap)\n",
    "    results_dirty.append(price_bond.getPrice() + price_swap.getPrice())\n",
    "    results_clean.append(price_bond.getCleanPrice() + price_swap.getCleanPrice())\n",
    "    \n",
    "#     results_dirty_bond.append(price_bond.getPrice())\n",
    "#     results_dirty_swap.append(price_swap.getPrice())\n",
    "    #print(pricing_data_bond.spec.getObjectId() + ', dirty price: ' + str(results.getPrice()) + \",  clean price: \" + str(results.getCleanPrice()))\n",
    "#print(results_dirty)\n",
    "del dc_scenario\n",
    "del dc_withspread\n",
    "del price_bond\n",
    "del price_swap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the current value as a reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define a discount curve based on the current rates\n",
    "dc_current = getDiscountCurve(\n",
    "    'dcCurrent',\n",
    "    refdate,\n",
    "    maturities_dates,\n",
    "    data_current,\n",
    "    UnitInterestRate.PERCENT\n",
    ")\n",
    "\n",
    "dc_withspread = getDiscountCurve(\n",
    "    'dcWithSpread',\n",
    "    refdate,\n",
    "    maturities_dates,\n",
    "    data_current + creditspread,\n",
    "    UnitInterestRate.PERCENT\n",
    ")\n",
    "\n",
    "pricing_data_bond.discountCurve = dc_current # dc_withspread\n",
    "pricing_data_pay_leg.discountCurve = dc_current # dc_withspread\n",
    "pricing_data_rec_leg.discountCurve = dc_current \n",
    "pricing_data_rec_leg.fixingCurve = dc_current\n",
    "\n",
    "# Compute portfolio value\n",
    "price_bond = pyvacon.pricing.price(pricing_data_bond)\n",
    "price_swap = analytics.price(pricing_data_swap)\n",
    "#print(prSwap.getPrice())\n",
    "#print(price_bond.getPrice())\n",
    "value_bond_current = price_bond.getPrice()\n",
    "value_swap_current = price_swap.getPrice()\n",
    "value_portfolio_current = price_bond.getPrice() + price_swap.getPrice()\n",
    "#print(value_swap_current)\n",
    "#print(value_bond_current)\n",
    "#print(value_portfolio_current)\n",
    "\n",
    "del dc_current\n",
    "del dc_withspread\n",
    "del price_bond\n",
    "del price_swap\n",
    "# del value_bond_current # used later\n",
    "# del value_swap_current # used later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the pricing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Histogramm of the changes/differences in value\n",
    "diffs_value_dirty = np.asarray([res - value_portfolio_current for res in results_dirty])\n",
    "plotHistogram(\n",
    "    data = diffs_value_dirty,\n",
    "    binsstart = binsstart_simple,\n",
    "    binsend = binsend_simple,\n",
    "    nbins = 60,\n",
    "    title_xaxis = 'Change in portfolio value'\n",
    ")\n",
    "\n",
    "# # Compare changes in Bond and Swap value\n",
    "# diffs_value_dirty = np.asarray([res - value_bond_current for res in results_dirty_bond])\n",
    "# plotHistogram(\n",
    "#     data = diffs_value_dirty,\n",
    "#     binsstart = min(diffs_value_dirty),\n",
    "#     binsend = max(diffs_value_dirty),\n",
    "#     nbins = 60,\n",
    "#     title_xaxis = 'Change in portfolio value (bond only)'\n",
    "# )\n",
    "\n",
    "# diffs_value_dirty = np.asarray([res - value_swap_current for res in results_dirty_swap])\n",
    "# plotHistogram(\n",
    "#     data = diffs_value_dirty,\n",
    "#     binsstart = min(diffs_value_dirty),\n",
    "#     binsend = max(diffs_value_dirty),\n",
    "#     nbins = 60,\n",
    "#     title_xaxis = 'Change in portfolio value (swap only)'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the swap we added to our portfolio cancels out any market risk, setting the value at risk to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del diffs_value_dirty\n",
    "# del value_portfolio_current\n",
    "del results_clean\n",
    "del results_dirty\n",
    "\n",
    "# del diffs_value_dirty\n",
    "# del results_clean_bond\n",
    "# del results_dirty_bond\n",
    "# del results_clean_swap\n",
    "# del results_dirty_swap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the swap we added to our portfolio cancels out any market risk, setting the value at risk to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interest Rate Shock Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters for the shock scenarios\n",
    "\n",
    "shockParams = pd.DataFrame({'Currency': [], 'Parallel': [], 'Short': [], 'Long': []})\n",
    "shockParams = shockParams.append({'Currency': 'EUR', 'Parallel': 200, 'Short': 250, 'Long': 100}, ignore_index = True)\n",
    "shockParams = shockParams.append({'Currency': 'GBP', 'Parallel': 250, 'Short': 300, 'Long': 150}, ignore_index = True)\n",
    "shockParams = shockParams.append({'Currency': 'USD', 'Parallel': 200, 'Short': 300, 'Long': 150}, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the change in value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Compute the price of our portfolio\n",
    "# Repeat for every scenario\n",
    "results_dirty = []\n",
    "results_clean = []\n",
    "results_dirty_bondonly = []\n",
    "results_clean_bondonly = []\n",
    "\n",
    "currency = 'EUR'\n",
    "parallel = shockParams.loc[shockParams['Currency'] == currency].loc[0]['Parallel']\n",
    "short = shockParams.loc[shockParams['Currency'] == currency].loc[0]['Short']\n",
    "long = shockParams.loc[shockParams['Currency'] == currency].loc[0]['Long']\n",
    "    \n",
    "shockScenarios = ['ParallelUp', 'ParallelDown', 'ShortUp', 'ShortDown', 'LongUp', 'LongDown', 'Flatten', 'Steepen']\n",
    "\n",
    "for shockScenario in shockScenarios:\n",
    "    dc_current = getShockedDiscountCurve(\n",
    "        'dc_current',\n",
    "        refdate,\n",
    "        maturities_dates,\n",
    "        data_current*100,\n",
    "        default_daycounter_type,\n",
    "        default_interpolation_type,\n",
    "        default_extrapolation_type,\n",
    "        shockScenario,\n",
    "        parallel,\n",
    "        short,\n",
    "        long\n",
    "    )\n",
    "    \n",
    "#     dc_withspread = getShockedDiscountCurve(\n",
    "#          'dc_withspread',\n",
    "#          refdate,\n",
    "#          maturities_dates,\n",
    "#          data_current + creditspread,\n",
    "#          default_daycounter_type,\n",
    "#          default_interpolation_type,\n",
    "#          default_extrapolation_type,\n",
    "#          shockScenario,\n",
    "#          parallel/100,\n",
    "#          short/100,\n",
    "#          long/100\n",
    "#      )\n",
    "    \n",
    "    pricing_data_bond.discountCurve = dc_current # dc_withspread\n",
    "    pricing_data_pay_leg.discountCurve = dc_current\n",
    "    pricing_data_rec_leg.discountCurve = dc_current\n",
    "    pricing_data_rec_leg.fixingCurve = dc_current\n",
    "    \n",
    "    price_bond = pyvacon.pricing.price(pricing_data_bond)\n",
    "    price_swap = analytics.price(pricing_data_swap)\n",
    "    dirty = price_bond.getPrice() + price_swap.getPrice()\n",
    "    clean = price_bond.getCleanPrice() + price_swap.getCleanPrice()\n",
    "    results_dirty.append(dirty)\n",
    "    results_clean.append(clean)\n",
    "    results_dirty_bondonly.append(price_bond.getPrice())\n",
    "    results_clean_bondonly.append(price_bond.getCleanPrice())\n",
    "    #print(pricing_data_bond.spec.getObjectId() + ', dirty price: ' + str(results.getPrice()) + \",  clean price: \" + str(results.getCleanPrice()))\n",
    "#print(results_dirty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the change in value (comparison between our entire portfolio and the bond on its own)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Bar plot of the changes/differences in value\n",
    "diffs_values_dirty = np.asarray([res - value_portfolio_current for res in results_dirty])\n",
    "diffs_values_dirty_bondonly = np.asarray([res - value_bond_current for res in results_dirty_bondonly])\n",
    "\n",
    "y_shockscenarios_simple = diffs_values_dirty_bondonly/value_bond_current*100\n",
    "ymin = min(y_shockscenarios_simple)\n",
    "ymax = max(y_shockscenarios_simple)\n",
    "ydiff = abs(ymax-ymin)\n",
    "rangemin = ymin - ydiff/10\n",
    "rangemax = ymax + ydiff/10\n",
    "\n",
    "marker=dict(\n",
    "    color=color_histmarker,\n",
    "    line = dict(color = color_histmarker, width = 1)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Plot results for the simple portfolio\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=shockScenarios, y=y_shockscenarios_simple, marker = marker))\n",
    "\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    xaxis = dict(title_text = \"Shock Scenario\"),\n",
    "    yaxis = dict(title_text = \"Change in portfolio value [%]\", range = [rangemin, rangemax])\n",
    "    ,title=get_default_title_dict(\"Simple Portfolio\")\n",
    ")\n",
    "\n",
    "fig.show()  \n",
    "\n",
    "\n",
    "\n",
    "# Plot results for the extended portfolio\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=shockScenarios, y=diffs_values_dirty/value_portfolio_current*100, marker = marker))\n",
    "\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    xaxis = dict(title_text = \"Shock Scenario\"),\n",
    "    yaxis = dict(title_text = \"Change in portfolio value [%]\", range = [rangemin, rangemax])\n",
    "    ,title=get_default_title_dict(\"Extended Portfolio\")\n",
    ")\n",
    "\n",
    "fig.show()  \n",
    "\n",
    "\n",
    "del y_shockscenarios_simple, ymin, ymax, ydiff, rangemin, rangemax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del results\n",
    "del results_clean\n",
    "del results_clean_bondonly\n",
    "del results_dirty\n",
    "del results_dirty_bondonly\n",
    "del current_value\n",
    "del current_value_without_credit_spread\n",
    "del binsend_simple\n",
    "del binsstart_simple\n",
    "del losses\n",
    "del data_current\n",
    "del data_scenarios\n",
    "del maturities_dates\n",
    "del expected_shortfall_simple\n",
    "del VaR_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del diffs_values_dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del diffs_values_dirty_bondonly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Choose the scenarios that we're going to use\n",
    "\n",
    "# Main scenarios\n",
    "data_scenarios = data_scenarios_MC\n",
    "maturities_dates = maturities_MC_dates\n",
    "data_current = data_MC_current\n",
    "\n",
    "# Comparative scenarios\n",
    "data_scenarios_compare = data_scenarios_random_multivariate_normal\n",
    "# data_scenarios_compare = data_scenarios_random_pick_indeces\n",
    "# data_scenarios_compare = data_scenarios_random_pick_dist\n",
    "# data_scenarios_compare = data_scenarios_random_shift\n",
    "# data_scenarios_compare = data_scenarios_random_buckets\n",
    "# data_scenarios_compare = data_scenarios_buckets\n",
    "# data_scenarios_compare = data_scenarios_hull_white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Set confidence level alpha\n",
    "alpha = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes in portfolio value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Compute the values of our simple portfolio in the chosen scenarios\n",
    "results_main, unused = ComputeValuesOfSimplePortfolio(\n",
    "    refdate,\n",
    "    maturities_dates,\n",
    "    data_scenarios,\n",
    "    creditspread,\n",
    "    UnitInterestRate.PERCENT\n",
    ")\n",
    "\n",
    "results_compare, unused = ComputeValuesOfSimplePortfolio(\n",
    "    refdate,\n",
    "    maturities_dates,\n",
    "    data_scenarios_compare,\n",
    "    creditspread,\n",
    "    UnitInterestRate.PERCENT\n",
    ")\n",
    "\n",
    "current_value, unused = ComputeValueOfSimplePortfolio(\n",
    "    refdate,\n",
    "    maturities_dates,\n",
    "    data_current,\n",
    "    creditspread,\n",
    "    UnitInterestRate.PERCENT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Since we want to compare scenarios, we determine the minimal and maximal x-values to display in upcoming plots\n",
    "xmin = min(min(results_main), min(results_compare))\n",
    "xmax = max(max(results_main), max(results_compare))\n",
    "binsstart_simple = xmin - current_value\n",
    "binsend_simple = xmax - current_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Histogramm of the changes/differences in value\n",
    "diffs_value_main = np.asarray([res - current_value for res in results_main])\n",
    "\n",
    "plotHistogram(\n",
    "    data = diffs_value_main,\n",
    "    binsstart = binsstart_simple,\n",
    "    binsend = binsend_simple,\n",
    "    nbins = 60,\n",
    "    title_xaxis = \"Change in portfolio value\",\n",
    "    title_figure = \"Main scenarios\"\n",
    ")\n",
    "\n",
    "\n",
    "diffs_value_compare = np.asarray([res - current_value for res in results_compare])\n",
    "\n",
    "plotHistogram(\n",
    "    data = diffs_value_compare,\n",
    "    binsstart = binsstart_simple,\n",
    "    binsend = binsend_simple,\n",
    "    nbins = 60,\n",
    "    title_xaxis = \"Change in portfolio value\",\n",
    "    title_figure = \"Comparative scenarios\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Determine losses\n",
    "losses_main = (-1)*diffs_value_main\n",
    "losses_compare = (-1)*diffs_value_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Display main scenarios with highest losses together with their cumulative relative frequency\n",
    "displayTableLossesFrequencies(\n",
    "    losses_main,\n",
    "    alpha,\n",
    "    basic_table_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Same for comparative scenarios\n",
    "displayTableLossesFrequencies(\n",
    "    losses_compare,\n",
    "    alpha,\n",
    "    basic_table_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value at risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Compute VaR\n",
    "VaR_main = computeVaR(alpha, losses_main)\n",
    "VaR_compare = computeVaR(alpha, losses_compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value at risk of the main scenarios is {{round(VaR_main, 4)}}.\n",
    "<br>\n",
    "The value at risk of the comparative scenarios is {{round(VaR_compare, 4)}}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del diffs_value_main\n",
    "del VaR_main\n",
    "\n",
    "del diffs_value_compare\n",
    "del VaR_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected shortfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Compute ES\n",
    "expected_shortfall_main = computeES(alpha, losses_main)\n",
    "expected_shortfall_compare = computeES(alpha, losses_compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected shortfall of the main scenarios is {{round(expected_shortfall_main, 4)}}.\n",
    "<br>\n",
    "The value at risk of the comparative scenarios is {{round(expected_shortfall_compare, 4)}}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# clean up\n",
    "del alpha\n",
    "\n",
    "del losses_main\n",
    "del expected_shortfall_main\n",
    "\n",
    "del losses_compare\n",
    "del expected_shortfall_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distinguish different risk factors\n",
    "In this section we want to consider the EUR6M curve as being composed of the EUR3M plus a basis spread curve.\n",
    "We will compute the VaR of our simple portfolio for both of these factors and look at how they relate to each other and the VaR we computed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hinweis an mich selbst: Wir verwenden hier die Schnittmenge der Maturities, fuer die Daten fuer beide Kurven zur Verfuegung stehen. Bei den aktuellen Daten sind dies genau die Stuetzstellen der EUR6M, da diese komplett in denen fuer die EUR3M enthalten sind. In unserem Fall hat die 'Einschraekung' auf die Schnittmenge also keinen Effekt, koennte in einer anderen Situation aber durchaus einen Unterschied machen.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EUR3M scenario + current basis spread\n",
    "data_scenarios_ir = data_scenarios_EUR3M[columns_maturity_EUR3M_EUR6M].copy()\n",
    "for i in range(len(data_scenarios_ir.index)):\n",
    "    data_scenarios_ir.iloc[i] = data_scenarios_ir.iloc[i] + spreads_EUR3M_EUR6M_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current EUR3M + basis spread scenario\n",
    "data_scenarios_bs = spreads_scenarios_EUR3M_EUR6M.copy()\n",
    "for i in range(len(data_scenarios_bs)):\n",
    "    data_scenarios_bs.iloc[i] = data_scenarios_bs.iloc[i] + data_EUR3M_current[columns_maturity_EUR3M_EUR6M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# EUR3M scenario + basis spread scenario (i.e. both factors together)\n",
    "data_scenarios_total = data_scenarios_EUR3M[columns_maturity_EUR3M_EUR6M].copy() + spreads_scenarios_EUR3M_EUR6M.copy()\n",
    "# data_scenarios_total = data_EUR6M_current[columns_maturity_EUR3M_EUR6M]\n",
    "# display((data_scenarios_total - data_scenarios_EUR6M[columns_maturity_EUR3M_EUR6M]).abs().describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Compute the portfolio value in each if these scenarios\n",
    "results_ir, unused = ComputeValuesOfSimplePortfolio(\n",
    "    refdate,\n",
    "    maturities_EUR3M_EUR6M_dates,\n",
    "    data_scenarios_ir,\n",
    "    creditspread,\n",
    "    UnitInterestRate.PERCENT\n",
    ")\n",
    "\n",
    "results_bs, unused = ComputeValuesOfSimplePortfolio(\n",
    "    refdate,\n",
    "    maturities_EUR3M_EUR6M_dates,\n",
    "    data_scenarios_bs,\n",
    "    creditspread,\n",
    "    UnitInterestRate.PERCENT\n",
    ")\n",
    "\n",
    "results_total, unused = ComputeValuesOfSimplePortfolio(\n",
    "    refdate,\n",
    "    maturities_EUR3M_EUR6M_dates,\n",
    "    data_scenarios_total,\n",
    "    creditspread,\n",
    "    UnitInterestRate.PERCENT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Compute the current portfolio value\n",
    "value_current, unused = ComputeValueOfSimplePortfolio(\n",
    "    refdate,\n",
    "    maturities_EUR3M_EUR6M_dates,\n",
    "    data_EUR6M_current[columns_maturity_EUR3M_EUR6M],\n",
    "    creditspread,\n",
    "    UnitInterestRate.PERCENT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Histogramm of the changes/differences in value\n",
    "diffs_value_ir = np.asarray([res - value_current for res in results_ir])\n",
    "binsstart_simple_ir = min(results_ir) - value_current\n",
    "binsend_simple_ir = max(results_ir) - value_current\n",
    "\n",
    "plotHistogram(\n",
    "    data = diffs_value_ir,\n",
    "    binsstart = binsstart_simple_ir,\n",
    "    binsend = binsend_simple_ir,\n",
    "    nbins = 60,\n",
    "    title_xaxis = \"Change in portfolio value (IR)\"\n",
    ")\n",
    "\n",
    "\n",
    "diffs_value_bs = np.asarray([res - value_current for res in results_bs])\n",
    "binsstart_simple_bs = min(results_bs) - value_current\n",
    "binsend_simple_bs = max(results_bs) - value_current\n",
    "\n",
    "plotHistogram(\n",
    "    data = diffs_value_bs,\n",
    "    binsstart = binsstart_simple_bs,\n",
    "    binsend = binsend_simple_bs,\n",
    "    nbins = 60,\n",
    "    title_xaxis = \"Change in portfolio value (BS)\"\n",
    ")\n",
    "\n",
    "\n",
    "diffs_value_total = np.asarray([res - value_current for res in results_total])\n",
    "binsstart_simple_total = min(results_total) - value_current\n",
    "binsend_simple_total = max(results_total) - value_current\n",
    "\n",
    "# plotHistogram(\n",
    "#     data = diffs_value_total,\n",
    "#     binsstart = binsstart_simple_total,\n",
    "#     binsend = binsend_simple_total,\n",
    "#     nbins = 60,\n",
    "#     title_xaxis = \"Change in portfolio value (total)\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Compute value at risk and expected shortfall\n",
    "alpha = 0.99\n",
    "percentage = 100*alpha\n",
    "\n",
    "# Determine losses and sort in ascending order\n",
    "diffs_value_ir = (-1)*np.sort((-1)*diffs_value_ir)\n",
    "losses_ir = -diffs_value_ir\n",
    "num_scenarios_ir = len(losses_ir)\n",
    "index_ceil_ir = np.ceil(num_scenarios_ir*alpha).astype(int) - 1\n",
    "\n",
    "VaR_ir = computeVaR(alpha, losses_ir)\n",
    "expected_shortfall_ir = computeES(alpha, losses_ir)\n",
    "\n",
    "\n",
    "# Determine losses and sort in ascending order\n",
    "diffs_value_bs = (-1)*np.sort((-1)*diffs_value_bs)\n",
    "losses_bs = -diffs_value_bs\n",
    "num_scenarios_bs = len(losses_bs)\n",
    "index_ceil_bs = np.ceil(num_scenarios_bs*alpha).astype(int) - 1\n",
    "\n",
    "VaR_bs = computeVaR(alpha, losses_bs)\n",
    "expected_shortfall_bs = computeES(alpha, losses_bs)\n",
    "\n",
    "\n",
    "# Determine losses and sort in ascending order\n",
    "diffs_value_total = (-1)*np.sort((-1)*diffs_value_total)\n",
    "losses_total = -diffs_value_total\n",
    "num_scenarios_total = len(losses_total)\n",
    "index_ceil_total = np.ceil(num_scenarios_total*alpha).astype(int) - 1\n",
    "\n",
    "VaR_total = computeVaR(alpha, losses_total)\n",
    "expected_shortfall_total = computeES(alpha, losses_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The {{timehorizon}}-day {{percentage}}%-VaR regarding changes in the reference index is {{round(VaR_ir, 4)}}. The expected shortfall is {{round(expected_shortfall_ir, 4)}}.\n",
    "\n",
    "The {{timehorizon}}-day {{percentage}}%-VaR regarding changes in basis spreads is {{round(VaR_bs, 4)}}. The expected shortfall is {{round(expected_shortfall_bs, 4)}}.\n",
    "\n",
    "Taking both changes into account simultaneously, the {{timehorizon}}-day {{percentage}}%-VaR is {{round(VaR_total, 4)}} and the expected shortfall is {{round(expected_shortfall_total, 4)}}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Display scenarios with highest losses together with their cumulative relative frequency (regarding changes in reference index)\n",
    "displayTableLossesFrequencies(\n",
    "    losses_ir,\n",
    "    alpha,\n",
    "    basic_table_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Do the same for the changes in basis spread\n",
    "displayTableLossesFrequencies(\n",
    "    losses_bs,\n",
    "    alpha,\n",
    "    basic_table_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# And again, taking both factors into account\n",
    "displayTableLossesFrequencies(\n",
    "    losses_total,\n",
    "    alpha,\n",
    "    basic_table_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del VaR_ir, VaR_bs, VaR_total\n",
    "del alpha, index_ceil_ir, index_ceil_bs, index_ceil_total, percentage\n",
    "del binsstart_simple_ir, binsstart_simple_bs, binsend_simple_ir, binsend_simple_bs, binsend_simple_total, binsstart_simple_total\n",
    "del results_ir, results_bs, results_total\n",
    "del diffs_value_ir, diffs_value_bs, diffs_value_total\n",
    "del losses_ir, losses_bs, losses_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reread the whole notebook and alter text according to recent changes\n",
    "- Differenz OIS <-> 6M als Summe der beiden anderen Differenzen?\n",
    "- Entwicklung von VaR und ES darstellen (sich bewegendes Zeitfenster)\n",
    "- check use of iloc vs loc\n",
    "- 'Einheit' der Zinssaetze mit an die Funktionen uebergeben (dezimal, percent, basispoint) // in Arbeit\n",
    "- Das Wort 'current' als Beschreibung fr die jngste in den Marktdaten vorhandene EONIA-Kurve ueberdenken\n",
    "- Actually order the historical data by date ascending (instead of descending) to avoid confusion\n",
    "- Illustrate the definition of VaR\n",
    "- Portfolio mit zwei Bonds unterschiedlicher Laufzeiten untersuchen\n",
    "- highlight extreme scenarios for EUR6M\n",
    "- Business day / holiday calendars bercksichtigen\n",
    "- maturities als yf mittels dateutils.relativedelta und daycounter ausrechnen\n",
    "- auch bei Bond Spec sinnvoll einsetzbar\n",
    "- getBoostrappedData not isinstance(discount_curves, pd.DataFrame) darf auch pd.Series sein\n",
    "- RefDates bei losses mitziehen\n",
    "- ~Histrogramme fr einzelne Sttzstellen bei HistSim und MC~\n",
    "- ~startat = 5 berprfen~\n",
    "- Hintergrundinfo zu den Shock Scenarios\n",
    "- ~PlotMonteCarloScenarios und plotHistoricalScenarios zusammenfhren~\n",
    "- Bei unterschiedlichen Laufzeiten in den Quotes Interpolation mglich machen, um Spreads ausrechnen zu knnen\n",
    "- Fixing bei Swap\n",
    "- ~MC mit Varianz Kovarianz, multivariat, kovarianzmatrix fr nderungen zu allen Sttzstellen, Cholesky-Zerlegung~\n",
    "- ~Vergleich der Ergebnisse unterschiedlicher Verfahren der Szenarioerzeugung in separaten Abschnitt auslagern~\n",
    "- Interest Rate Shock Scenarios wieder in eigenen Abschnitt auslagern (?)\n",
    "- Hull-White kalibrieren\n",
    "- Wie gehen wir mit Lcken in den Daten um?\n",
    "- ~Berechnung von VaR und ES berall anpassen (hinterher mit Stefan vergleichen)~\n",
    "- ~Tabelle mit Szenarioinformationen zur Erklaerung von ES erweitern?~\n",
    "- Moeglichkeit schaffen Strings wie '1Y' und '6M' in eine Zeitspanne umzuwandeln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ell_n = ln(v_{n+1}) - \\ln(v_n) = ln\\left(\\frac{v_{n+1}}{v_n}\\right)$\n",
    "\n",
    "$v_k = v \\exp(\\ell_k)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "183px",
    "width": "166px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "268.239px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
