{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/rivacon_frontmark_combined_header.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "from enum import Enum, unique\n",
    "\n",
    "import pyvacon\n",
    "import pyvacon.marketdata.testdata as mkt_testdata\n",
    "import pyvacon.tools.enums as enums\n",
    "import pyvacon.marketdata.bootstrapping as bootstr\n",
    "import pyvacon.marketdata.plot as mkt_plot\n",
    "import pyvacon.models.plot as model_plot\n",
    "import pyvacon.models.tools as model_tools\n",
    "import pyvacon.analytics as analytics\n",
    "import pyvacon.tools.converter as converter\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch, Rectangle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.transforms as mtransforms\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime as dt\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# use ipynb to import function definitions from another notebook\n",
    "import ipynb\n",
    "from ipynb.fs.defs.ir_shock_scenarios import getShockedDiscountCurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# We define some constants which we'll use repeatedly throughout this notebook.\n",
    "notebook_is_draft = True\n",
    "color_main = 'tab:blue'\n",
    "color_highlight = 'tab:orange'\n",
    "color_graphblue = \"#4e79a7\"\n",
    "color_histmarker = \"#4e79a7\"\n",
    "color_histmarkerborder = \"White\"\n",
    "grid_alpha = 0.4\n",
    "default_daycounter_type = enums.DayCounter.ACT365_FIXED\n",
    "default_daycounter = analytics.DayCounter(default_daycounter_type)\n",
    "default_interpolation_type = enums.InterpolationType.LINEAR\n",
    "default_extrapolation_type = enums.ExtrapolationType.CONSTANT\n",
    "default_plotly_scatter_mode = 'lines'\n",
    "if notebook_is_draft:\n",
    "    default_sample_size_MC = 200\n",
    "else:\n",
    "    default_sample_size_MC = 6000\n",
    "refdate = dt.datetime(year = 2019, month = 12, day = 30)\n",
    "\n",
    "@unique\n",
    "class UnitInterestRate(Enum):\n",
    "    DECIMAL = 1\n",
    "    PERCENT = 2\n",
    "    BASISPOINTS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     16,
     45,
     76,
     102
    ]
   },
   "outputs": [],
   "source": [
    "# Same for functions\n",
    "def get_default_title_dict(title_text):\n",
    "    return dict(\n",
    "        text = title_text,\n",
    "        y = 0.9,\n",
    "        x = 0.475,\n",
    "        xanchor = 'center',\n",
    "        yanchor = 'top')\n",
    "\n",
    "def getDates(refdate, year_fractions):\n",
    "    return [refdate + dt.timedelta(int(round(year_fractions[i]*365.25))) for i in range(len(year_fractions))]\n",
    "\n",
    "def getDiscountFactors(\n",
    "    interest_rates,\n",
    "    year_fractions,\n",
    "    unit_interest_rates\n",
    "):\n",
    "    if len(interest_rates) != len(year_fractions):\n",
    "        raise Exception('You must supply an equal number of interest rates and year fractions!')\n",
    "    \n",
    "    if unit_interest_rates == UnitInterestRate.DECIMAL:\n",
    "        factor_unit = 1\n",
    "    elif unit_interest_rates == UnitInterestRate.PERCENT:\n",
    "        factor_unit = 1/100\n",
    "    elif unit_interest_rates == UnitInterestRate.BASISPOINTS:\n",
    "        factor_unit = 1/(100*100)\n",
    "    else:\n",
    "        raise Exception('Value for parameter \\'unit_interest_rates\\' needs to be one of the values supplied by the Enum class \\'UnitInterestRate\\'!')\n",
    "        \n",
    "    dsc_fac = analytics.vectorDouble()\n",
    "    for i in range(len(interest_rates)):\n",
    "        dsc_fac.append(math.exp(-interest_rates[i]*factor_unit*year_fractions[i]))  \n",
    "        \n",
    "    return dsc_fac\n",
    "  \n",
    "\n",
    "def getDiscountCurve(\n",
    "    object_name,\n",
    "    refdate,\n",
    "    dates,\n",
    "    interest_rates,\n",
    "    unit_interest_rates,\n",
    "    daycounter_type = default_daycounter_type,\n",
    "    interpolation_type = default_interpolation_type,\n",
    "    extrapolation_type = default_extrapolation_type\n",
    "):\n",
    "    daycounter = analytics.DayCounter(daycounter_type)\n",
    "    \n",
    "    year_fractions = []\n",
    "    for i in range(len(dates)):\n",
    "        year_fractions.append(daycounter.yf(refdate, dates[i]))\n",
    "    \n",
    "    dsc_fac = getDiscountFactors(\n",
    "        interest_rates,\n",
    "        year_fractions,\n",
    "        unit_interest_rates\n",
    "    )\n",
    "    \n",
    "    discountCurve = analytics.DiscountCurve(\n",
    "        object_name,\n",
    "        refdate,\n",
    "        dates,\n",
    "        dsc_fac,\n",
    "        daycounter_type,\n",
    "        interpolation_type,\n",
    "        extrapolation_type\n",
    "    )\n",
    "    \n",
    "    return discountCurve\n",
    "\n",
    "\n",
    "def getInterestRatesFromDiscountCurve(\n",
    "    discount_curve,\n",
    "    refdate,\n",
    "    dates,\n",
    "    unit_interest_rates\n",
    "):\n",
    "    if unit_interest_rates == UnitInterestRate.DECIMAL:\n",
    "        factor_unit = 1\n",
    "    elif unit_interest_rates == UnitInterestRate.PERCENT:\n",
    "        factor_unit = 100\n",
    "    elif unit_interest_rates == UnitInterestRate.BASISPOINTS:\n",
    "        factor_unit = 100*100\n",
    "    else:\n",
    "        raise Exception('Value for parameter \\'unit_interest_rates\\' needs to be one of the values supplied by the Enum class \\'UnitInterestRate\\'!')\n",
    "    \n",
    "    daycounter = analytics.DayCounter(discount_curve.getDayCounterType())\n",
    "    interest_rates = [ (-1) * math.log(discount_curve.value(dates[i], refdate)) / daycounter.yf(dates[i], refdate) * factor_unit for i in range(len(dates))]\n",
    "    return interest_rates\n",
    "\n",
    "\n",
    "def getBootstrappedData(\n",
    "    raw_data,\n",
    "    column_refdate,\n",
    "    columns_maturity,\n",
    "    maturities_yf,\n",
    "    quotes_template,\n",
    "    curve_name,\n",
    "    daycounter_type,\n",
    "    unit_interest_rates,\n",
    "    discount_curves = None #,\n",
    "#     basis_curves = None # TODO\n",
    "):\n",
    "    if unit_interest_rates == UnitInterestRate.DECIMAL:\n",
    "        factor_unit = 1\n",
    "    elif unit_interest_rates == UnitInterestRate.PERCENT:\n",
    "        factor_unit = 100\n",
    "    elif unit_interest_rates == UnitInterestRate.BASISPOINTS:\n",
    "        factor_unit = 100*100\n",
    "    else:\n",
    "        raise Exception('Value for parameter \\'unit_interest_rates\\' needs to be one of the values supplied by the Enum class \\'UnitInterestRate\\'!')\n",
    "    \n",
    "    series_bootstrapped = []\n",
    "        \n",
    "    for i in range(0, len(raw_data.index)):\n",
    "        row = raw_data[columns_maturity].iloc[i]\n",
    "\n",
    "        # Copy the template and insert actual quotes\n",
    "        quotes = quotes_template.copy(deep = True)\n",
    "        quotes['Quote'] = row\n",
    "        quotes['Quote'] = quotes['Quote'].apply(lambda x: x/factor_unit) # raw data is given in unit_interest_rates\n",
    "\n",
    "        # set up curve parameters for bootstrapping algorithm\n",
    "        refdate_curve = raw_data.iloc[i][column_refdate]\n",
    "        \n",
    "        if isinstance(refdate_curve, pd.Timestamp):\n",
    "            refdate_curve_dt = refdate_curve.to_pydatetime()\n",
    "        else:\n",
    "            refdate_curve_dt = refdate_curve\n",
    "            \n",
    "        if not ( isinstance(refdate_curve_dt, dt.datetime) or isinstance(refdate_curve_dt, dt.date) ):\n",
    "            raise Exception('The given reference dates need to be of type datetime.date, datetime.datetime or pandas.Timestamp')\n",
    "            \n",
    "        dates_curve = getDates(refdate_curve_dt, maturities_yf)\n",
    "        \n",
    "        if not isinstance(discount_curves, pd.DataFrame):\n",
    "            discount_curve = None\n",
    "        else:\n",
    "#             if column_refdate not in discount_curves.columns:\n",
    "#                 raise Exception('The DataFrame containing discount curves needs to supply the ref')\n",
    "#             query = discount_curves[discount_curves['Analytics.DiscountCurve'].getRefDate() == refdate_curve]\n",
    "            query = discount_curves[discount_curves[column_refdate] == refdate_curve]\n",
    "            if query.empty:\n",
    "                discount_curve = None\n",
    "                # Throw error? Show warning?\n",
    "            elif len(query.index) > 1:\n",
    "                raise Exception('More than one discount curve was provided for reference date ' + refdate_curve_dt.strftime(\"%d-%m-%Y\") + '. I don\\'t know which one to use.')\n",
    "            else:\n",
    "                discount_curve = query['Analytics.DiscountCurve'].iloc[0]\n",
    "        \n",
    "        basis_curve = None # TODO\n",
    "        \n",
    "        curve_spec =  {\n",
    "            'refDate': refdate_curve_dt, \n",
    "            'curveName': curve_name + refdate_curve_dt.strftime(\"%d-%m-%Y\"),\n",
    "            'dayCount': daycounter_type,\n",
    "            'calendar': analytics.SimpleHolidayCalendar('GER_HOL'),\n",
    "            'discountCurve': discount_curve,\n",
    "            'basisCurve': basis_curve\n",
    "        }\n",
    "\n",
    "        # bootstrap the curve     \n",
    "        curve_bootstrapped = bootstr.bootstrap_curve(quotes, curve_spec)\n",
    "\n",
    "        # the resulting curve is given as a discount curve\n",
    "        # -> compute the zero rates for the given maturities\n",
    "        df = analytics.vectorDouble()\n",
    "        dc = analytics.DayCounter(daycounter_type)\n",
    "        curve_bootstrapped.value(df, refdate_curve_dt, dates_curve)\n",
    "        data_bootstrapped = [-math.log(df[i])/dc.yf(refdate_curve_dt, dates_curve[i]) for i in range(len(df))]\n",
    "        data_bootstrapped = [factor_unit*x for x in data_bootstrapped] # convert back to unit_interest_rates\n",
    "\n",
    "        # put the data into a series and store it in a list\n",
    "        series = pd.Series(data = [refdate_curve, curve_bootstrapped] + data_bootstrapped, index = [column_refdate, 'Analytics.DiscountCurve'] + columns_maturity)\n",
    "        series_bootstrapped.append(series)\n",
    "                \n",
    "    return pd.DataFrame(data = series_bootstrapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## Value at risk\n",
    "Value at risk (VaR) is a measure for the risk in a portfolio of financial assets. Given a time horizon of $n$ days and a confidence level $\\alpha$, the VaR is the loss of value, which has the probability $\\alpha$ not to be exceeded within the next $n$ days. In other words, the VaR is the $\\alpha$-quantile of the distribution of loss in the value of a portfolio other the next $n$ days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different methods for estimating the value at risk can be put into two major categories: Those using analytical models and those using simulations.\n",
    "\n",
    "The goal of **analytical** methods is to define a probability distribution, which approximates the actual probability distribution of the portfolio value. One can then write down a closed formula for the value at risk.\n",
    "\n",
    "**Simulation**-based methods simulate the change in value over the next $n$ days and use the resulting relative frequency distribution to 'read off' the value at risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical simulation\n",
    "A very popular way of simulating changes in value uses past market data to estimate what will happen in the future. To do so, we first have to identify all market variables affecting the portfolio value. Then we collect data on how these variables moved over the past $k+n$ days. This allows us to calculate $k$ historical scenarios of what can happen in $n$ days. Assuming that the market will behave in the future as it did in the past, we can compute the portfolio value in each of these scenarios. This provides us with a relative frequency distribution, which we then use to determine the value at risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo simulation\n",
    "Monte Carlo simulation is similar to historical simulation in the sense that we also\n",
    "- generate a set of market scenarios,\n",
    "- compute the value of our portfolio in each of these scenarios and\n",
    "- use the resulting relative frequency distribution to determine the value at risk.\n",
    "\n",
    "They differ in the method for generating market scenarios: Instead of historical data, Monte Carlo simulation uses randomly generated movements of all relevant market variables. This requires more work (for example, you first have to develop a model for the market movements), but also comes with more flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook\n",
    "We are going to look at two rather simple portfolios, one containing only a single bond and one containing a swap in addition to that bond. Their values can be computed by summing over all discounted future cash flows of said bond (and swap). Therefore, the only market variables affecting the portfolio values are the interest rates we use to determine the discount factors.\n",
    "We will use both historical and Monte Carlo simulation to obtain interest rate scenarios. Based on these scenarios, we are going to determine the value at risk for both portfolios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historical simulation\n",
    "## Historical data\n",
    "We choose to discount future cash flows using EONIA interest rate curves. We have historical data from every business day of 2018 and 2019 available to us. The data includes the actual over-night rates plus forward rates for various maturities. We'll load the data for maturities of 1 day, 1-11 months and 1-10 years.\n",
    "\n",
    "*Note: These interest rates are not zero-coupon rates. We will determine the zero rates in [the bootstrapping section below](#bootstrapping).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# load test data from an Excel file\n",
    "xl = pd.ExcelFile('TestDaten.xlsx')\n",
    "#print(xl.sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5,
     29,
     44
    ]
   },
   "outputs": [],
   "source": [
    "# import data into pandas dataframe\n",
    "data_EONIA_raw = xl.parse('EONIA')\n",
    "data_EUR3M_raw = xl.parse('EUR3M')\n",
    "data_EUR6M_raw = xl.parse('EUR6M')\n",
    "del xl\n",
    "columns_maturity_EONIA = [\n",
    "   '1D',\n",
    "   '1M',\n",
    "   '2M',\n",
    "   '3M',\n",
    "   '4M',\n",
    "   '5M',\n",
    "   '6M',\n",
    "   '7M',\n",
    "   '8M',\n",
    "   '9M',\n",
    "   '10M',\n",
    "   '11M',\n",
    "   '1Y',\n",
    "   '2Y',\n",
    "   '3Y',\n",
    "   '4Y',\n",
    "   '5Y',\n",
    "   '6Y',\n",
    "   '7Y',\n",
    "   '8Y',\n",
    "   '9Y',\n",
    "   '10Y'\n",
    "]\n",
    "columns_maturity_EUR3M = [\n",
    "   '3M',\n",
    "   '6M',\n",
    "   '9M',\n",
    "   '1Y',\n",
    "   '2Y',\n",
    "   '3Y',\n",
    "   '4Y',\n",
    "   '5Y',\n",
    "   '6Y',\n",
    "   '7Y',\n",
    "   '8Y',\n",
    "   '9Y',\n",
    "   '10Y'\n",
    "]\n",
    "columns_maturity_EUR6M = [\n",
    "   '6M',\n",
    "   '1Y',\n",
    "   '2Y',\n",
    "   '3Y',\n",
    "   '4Y',\n",
    "   '5Y',\n",
    "   '6Y',\n",
    "   '7Y',\n",
    "   '8Y',\n",
    "   '9Y',\n",
    "   '10Y'\n",
    "]\n",
    "data_EONIA_raw = pd.DataFrame(data_EONIA_raw, columns = ['RefDate'] + columns_maturity_EONIA)\n",
    "data_EUR3M_raw = pd.DataFrame(data_EUR3M_raw, columns = ['RefDate'] + columns_maturity_EUR3M)\n",
    "data_EUR6M_raw = pd.DataFrame(data_EUR6M_raw, columns = ['RefDate'] + columns_maturity_EUR6M)\n",
    "# print(['RefDate'] + columns_maturity_EONIA)\n",
    "\n",
    "# convert Excel dates to a more useful format\n",
    "data_EONIA_raw['RefDate'] = pd.TimedeltaIndex(data_EONIA_raw['RefDate'], unit='d') + dt.datetime(1899, 12, 30)\n",
    "data_EUR3M_raw['RefDate'] = pd.TimedeltaIndex(data_EUR3M_raw['RefDate'], unit='d') + dt.datetime(1899, 12, 30)\n",
    "data_EUR6M_raw['RefDate'] = pd.TimedeltaIndex(data_EUR6M_raw['RefDate'], unit='d') + dt.datetime(1899, 12, 30)\n",
    "#display(data_EONIA.head(5))\n",
    "#display(data_EONIA.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we'll need them later, we store the selected maturities in the form of year fractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# maturities in years\n",
    "maturities_EONIA_yf = [1/365] # 1 day\n",
    "maturities_EONIA_yf.extend( (np.arange(11)+1)/12 ) # 1 to 11 months\n",
    "maturities_EONIA_yf.extend(np.arange(10)+1) # 1 to 10 years\n",
    "\n",
    "maturities_EUR3M_yf = [3/12, 6/12, 9/12] # 3, 6 and 9 months\n",
    "maturities_EUR3M_yf.extend(np.arange(10)+1) # 1 to 10 years\n",
    "\n",
    "maturities_EUR6M_yf = [6/12] # 6 months\n",
    "maturities_EUR6M_yf.extend(np.arange(10)+1) # 1 to 10 years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bootstrapping'></a>\n",
    "### Bootstrapping\n",
    "#### EONIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# The pyvacon bootstrap algorithm needs the quotes of the curve we want to boostrap to be provided\n",
    "# via a DataFrame that has a particular structure. We will now build such a DataFrame with all quotes\n",
    "# set to NaN. We will insert actual quotes before bootstrapping.\n",
    "ncols = len(columns_maturity_EONIA)\n",
    "dfQuotes_EONIA_template = pd.DataFrame(data = columns_maturity_EONIA, index = columns_maturity_EONIA, columns = ['Maturity'])\n",
    "tenors = ['1D', '1M', '2M', '3M', '4M', '5M', '6M', '7M', '8M', '9M', '10M', '11M'] + 10 * ['1Y']\n",
    "dfQuotes_EONIA_template['Instrument'] = ['DEPOSIT'] + ( (ncols-1)*['OIS'] )\n",
    "dfQuotes_EONIA_template['Quote'] = ncols*['NaN']# will be replaced with actual data later\n",
    "dfQuotes_EONIA_template['Currency'] = ncols*['EUR']\n",
    "dfQuotes_EONIA_template['UnderlyingIndex'] = ncols*['EONIA']\n",
    "dfQuotes_EONIA_template['UnderlyingTenor'] = tenors\n",
    "dfQuotes_EONIA_template['UnderlyingPaymentFrequency'] = tenors\n",
    "dfQuotes_EONIA_template['BasisIndex'] = ncols*['NaN']\n",
    "dfQuotes_EONIA_template['BasisTenor'] = ncols*['NaN']\n",
    "dfQuotes_EONIA_template['BasisPaymentFrequency'] = ncols*['NaN']\n",
    "dfQuotes_EONIA_template['PaymentFrequencyFixed'] = tenors\n",
    "dfQuotes_EONIA_template['DayCountFixed'] = ncols*['Act360']\n",
    "dfQuotes_EONIA_template['DayCountFloat'] = ncols*['Act360']\n",
    "dfQuotes_EONIA_template['DayCountBasis'] = ncols*['NaN']\n",
    "dfQuotes_EONIA_template['RollConventionFixed'] = ncols*['ModifiedFollowing']\n",
    "dfQuotes_EONIA_template['RollConventionFloat'] = ncols*['ModifiedFollowing']\n",
    "dfQuotes_EONIA_template['RollConventionBasis'] = ncols*['NaN']\n",
    "dfQuotes_EONIA_template['SpotLag'] = ['0D'] + ((ncols-1)*['2D'])\n",
    "del ncols\n",
    "# dfQuotes_EONIA_template.head(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# bootstrap the data and put it into a DataFrame\n",
    "data_EONIA_bootstrapped = getBootstrappedData(\n",
    "    data_EONIA_raw,\n",
    "    'RefDate',\n",
    "    columns_maturity_EONIA,\n",
    "    maturities_EONIA_yf,\n",
    "    dfQuotes_EONIA_template,\n",
    "    'EONIA',\n",
    "    enums.DayCounter.ACT360,\n",
    "    UnitInterestRate.PERCENT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# compare raw and bootstrapped curves\n",
    "dc = analytics.DayCounter(enums.DayCounter.ACT360)\n",
    "if(notebook_is_draft):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x = maturities_EONIA_yf, y = data_EONIA_raw[columns_maturity_EONIA].loc[1], name = 'raw', mode = default_plotly_scatter_mode))\n",
    "    fig.add_trace(go.Scatter(x = maturities_EONIA_yf, y = data_EONIA_bootstrapped[columns_maturity_EONIA].loc[1], name = 'bootstrapped', mode = default_plotly_scatter_mode))\n",
    "\n",
    "    fig.add_trace(go.Scatter(x = maturities_EONIA_yf, y = data_EONIA_raw[columns_maturity_EONIA].loc[12], name = 'raw', mode = default_plotly_scatter_mode))\n",
    "    fig.add_trace(go.Scatter(x = maturities_EONIA_yf, y = data_EONIA_bootstrapped[columns_maturity_EONIA].loc[12], name = 'bootstrapped', mode = default_plotly_scatter_mode))\n",
    "\n",
    "\n",
    "    fig.update_layout(\n",
    "        showlegend=True,\n",
    "        xaxis = dict(title_text = \"Expiry (in years)\"),\n",
    "        yaxis = dict(title_text = \"Interest rate (in percent)\")\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "#del series_bootstrapped\n",
    "del dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the bootstrapped data from here on out\n",
    "data_EONIA = data_EONIA_bootstrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EURIBOR 3M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# do the same for EUR3M\n",
    "ncols = len(columns_maturity_EUR3M)\n",
    "dfQuotes_EUR3M_template = pd.DataFrame(data = columns_maturity_EUR3M, index = columns_maturity_EUR3M, columns = ['Maturity'])\n",
    "dfQuotes_EUR3M_template['Instrument'] = ['DEPOSIT'] + ( (ncols-1)*['IRS'] )\n",
    "dfQuotes_EUR3M_template['Quote'] = ncols*['NaN']# will be replaced with actual data later\n",
    "dfQuotes_EUR3M_template['Currency'] = ncols*['EUR']\n",
    "dfQuotes_EUR3M_template['UnderlyingIndex'] = ncols*['EURIBOR']\n",
    "dfQuotes_EUR3M_template['UnderlyingTenor'] = ncols*['3M']\n",
    "dfQuotes_EUR3M_template['UnderlyingPaymentFrequency'] = ncols*['3M']\n",
    "dfQuotes_EUR3M_template['BasisIndex'] = ncols*['NaN']\n",
    "dfQuotes_EUR3M_template['BasisTenor'] = ncols*['NaN']\n",
    "dfQuotes_EUR3M_template['BasisPaymentFrequency'] = ncols*['NaN']\n",
    "dfQuotes_EUR3M_template['PaymentFrequencyFixed'] = ['3M', '6M', '9M'] + ( (ncols-3)*['1Y'] )\n",
    "dfQuotes_EUR3M_template['DayCountFixed'] = ncols*['Act360']\n",
    "dfQuotes_EUR3M_template['DayCountFloat'] = ncols*['Act360']\n",
    "dfQuotes_EUR3M_template['DayCountBasis'] = ncols*['NaN']\n",
    "dfQuotes_EUR3M_template['RollConventionFixed'] = ncols*['ModifiedFollowing']\n",
    "dfQuotes_EUR3M_template['RollConventionFloat'] = ncols*['ModifiedFollowing']\n",
    "dfQuotes_EUR3M_template['RollConventionBasis'] = ncols*['NaN']\n",
    "dfQuotes_EUR3M_template['SpotLag'] = ncols*['2D']\n",
    "del ncols\n",
    "# dfQuotes_EUR3M_template.head(999)\n",
    "\n",
    "# bootstrap the data and put it into a DataFrame\n",
    "data_EUR3M_bootstrapped = getBootstrappedData(\n",
    "    data_EUR3M_raw,\n",
    "    'RefDate',\n",
    "    columns_maturity_EUR3M,\n",
    "    maturities_EUR3M_yf,\n",
    "    dfQuotes_EUR3M_template,\n",
    "    'EUR3M',\n",
    "    enums.DayCounter.ACT360,\n",
    "    UnitInterestRate.PERCENT,\n",
    "    data_EONIA_bootstrapped[['RefDate', 'Analytics.DiscountCurve']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# compare raw and bootstrapped curves\n",
    "dc = analytics.DayCounter(enums.DayCounter.ACT360)\n",
    "if(notebook_is_draft):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x = maturities_EUR3M_yf, y = data_EUR3M_raw[columns_maturity_EUR3M].iloc[1], name = 'raw', mode = default_plotly_scatter_mode))\n",
    "    fig.add_trace(go.Scatter(x = maturities_EUR3M_yf, y = data_EUR3M_bootstrapped[columns_maturity_EUR3M].iloc[1], name = 'bootstrapped', mode = default_plotly_scatter_mode))\n",
    "\n",
    "    fig.add_trace(go.Scatter(x = maturities_EUR3M_yf, y = data_EUR3M_raw[columns_maturity_EUR3M].iloc[12], name = 'raw', mode = default_plotly_scatter_mode))\n",
    "    fig.add_trace(go.Scatter(x = maturities_EUR3M_yf, y = data_EUR3M_bootstrapped[columns_maturity_EUR3M].iloc[12], name = 'bootstrapped', mode = default_plotly_scatter_mode))\n",
    "\n",
    "    fig.update_layout(\n",
    "        showlegend=True,\n",
    "        xaxis = dict(title_text = \"Expiry (in years)\"),\n",
    "        yaxis = dict(title_text = \"Interest rate (in percent)\")\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "#del series_bootstrapped\n",
    "del dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the bootstrapped data from here on out\n",
    "data_EUR3M = data_EUR3M_bootstrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EURIBOR 6M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# do the same for EUR6M\n",
    "ncols = len(columns_maturity_EUR6M)\n",
    "dfQuotes_EUR6M_template = pd.DataFrame(data = columns_maturity_EUR6M, index = columns_maturity_EUR6M, columns = ['Maturity'])\n",
    "dfQuotes_EUR6M_template['Instrument'] = 0*['DEPOSIT'] + ( (ncols-0)*['IRS'] )#ncols*['IRS']#\n",
    "dfQuotes_EUR6M_template['Quote'] = ncols*['NaN']# will be replaced with actual data later\n",
    "dfQuotes_EUR6M_template['Currency'] = ncols*['EUR']\n",
    "dfQuotes_EUR6M_template['UnderlyingIndex'] = ncols*['EURIBOR']\n",
    "dfQuotes_EUR6M_template['UnderlyingTenor'] = ncols*['6M']\n",
    "dfQuotes_EUR6M_template['UnderlyingPaymentFrequency'] = ncols*['6M']\n",
    "dfQuotes_EUR6M_template['BasisIndex'] = ncols*['NaN']\n",
    "dfQuotes_EUR6M_template['BasisTenor'] = ncols*['NaN']\n",
    "dfQuotes_EUR6M_template['BasisPaymentFrequency'] = ncols*['NaN']\n",
    "dfQuotes_EUR6M_template['PaymentFrequencyFixed'] = ['6M'] + ( (ncols-1)*['1Y'] )\n",
    "dfQuotes_EUR6M_template['DayCountFixed'] = ncols*['Act360']\n",
    "dfQuotes_EUR6M_template['DayCountFloat'] = ncols*['Act360']\n",
    "dfQuotes_EUR6M_template['DayCountBasis'] = ncols*['NaN']\n",
    "dfQuotes_EUR6M_template['RollConventionFixed'] = ncols*['ModifiedFollowing']\n",
    "dfQuotes_EUR6M_template['RollConventionFloat'] = ncols*['ModifiedFollowing']\n",
    "dfQuotes_EUR6M_template['RollConventionBasis'] = ncols*['NaN']\n",
    "dfQuotes_EUR6M_template['SpotLag'] = ncols*['2D']\n",
    "del ncols\n",
    "# dfQuotes_EUR6M_template.head(999)\n",
    "\n",
    "# bootstrap the data and put it into a DataFrame\n",
    "data_EUR6M_bootstrapped = getBootstrappedData(\n",
    "    data_EUR6M_raw,\n",
    "    'RefDate',\n",
    "    columns_maturity_EUR6M,\n",
    "    maturities_EUR6M_yf,\n",
    "    dfQuotes_EUR6M_template,\n",
    "    'EUR6M',\n",
    "    enums.DayCounter.ACT360,\n",
    "    UnitInterestRate.PERCENT,\n",
    "    data_EONIA_bootstrapped[['RefDate', 'Analytics.DiscountCurve']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# compare raw and bootstrapped curves\n",
    "dc = analytics.DayCounter(enums.DayCounter.ACT360)\n",
    "if(notebook_is_draft):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x = maturities_EUR6M_yf, y = data_EUR6M_raw[columns_maturity_EUR6M].iloc[1], name = 'raw', mode = default_plotly_scatter_mode))\n",
    "    fig.add_trace(go.Scatter(x = maturities_EUR6M_yf, y = data_EUR6M_bootstrapped[columns_maturity_EUR6M].iloc[1], name = 'bootstrapped', mode = default_plotly_scatter_mode))\n",
    "\n",
    "    fig.add_trace(go.Scatter(x = maturities_EUR6M_yf, y = data_EUR6M_raw[columns_maturity_EUR6M].iloc[12], name = 'raw', mode = default_plotly_scatter_mode))\n",
    "    fig.add_trace(go.Scatter(x = maturities_EUR6M_yf, y = data_EUR6M_bootstrapped[columns_maturity_EUR6M].iloc[12], name = 'bootstrapped', mode = default_plotly_scatter_mode))\n",
    "\n",
    "    fig.update_layout(\n",
    "        showlegend=True,\n",
    "        xaxis = dict(title_text = \"Expiry (in years)\"),\n",
    "        yaxis = dict(title_text = \"Interest rate (in percent)\")\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "#del series_bootstrapped\n",
    "del dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the bootstrapped data from here on out\n",
    "data_EUR6M = data_EUR6M_bootstrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences in basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     8
    ]
   },
   "outputs": [],
   "source": [
    "# define a function that computes the differences in interest rates for common reference dates and maturities\n",
    "def computeBasisSpreads(\n",
    "    data1,\n",
    "    data2,\n",
    "    columns_maturity1,\n",
    "    columns_maturity2,\n",
    "    column_refdate1,\n",
    "    column_refdate2\n",
    "):\n",
    "    common_dates = [d for d in data1[column_refdate1].tolist() if d in data2[column_refdate2].tolist()]\n",
    "\n",
    "    series = []\n",
    "\n",
    "    for adate in common_dates:\n",
    "        row1 = data1[data1[column_refdate1] == adate][columns_maturity1]\n",
    "        row2 = data2[data2[column_refdate2] == adate][columns_maturity2]\n",
    "        \n",
    "        if len(row1.index) != 1:\n",
    "            raise Exception('Found an unexpected number of rows in data set number 1 for reference date ' + adate.strftime(\"%d-%m-%Y\") + '. Expected 1, found ' + str(len(row1.index)) + '.')\n",
    "\n",
    "        if len(row2.index) != 1:\n",
    "            raise Exception('Found an unexpected number of rows in data set number 2 for reference date ' + adate.strftime(\"%d-%m-%Y\") + '. Expected 1, found ' + str(len(row2.index)) + '.')\n",
    "\n",
    "        common_maturities = [c for c in row1.columns if c in row2.columns]\n",
    "        diff = row1[common_maturities].iloc[0] - row2[common_maturities].iloc[0]\n",
    "\n",
    "        series.append(pd.Series(data = [adate]+diff.tolist(), index = [column_refdate1] + diff.index.tolist()))\n",
    "\n",
    "    return pd.DataFrame(data = series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EUR3M-EONIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreads_3M_EONIA = computeBasisSpreads(\n",
    "    data1 = data_EUR3M_bootstrapped,\n",
    "    data2 = data_EONIA_bootstrapped,\n",
    "    columns_maturity1 = columns_maturity_EUR3M,\n",
    "    columns_maturity2 = columns_maturity_EONIA,\n",
    "    column_refdate1 = 'RefDate',\n",
    "    column_refdate2 = 'RefDate'\n",
    ")\n",
    "columns_maturity_EUR3M_EONIA = [c for c in columns_maturity_EUR3M if c in columns_maturity_EONIA]\n",
    "maturities_EUR3M_EONIA_yf = [yf for yf in maturities_EUR3M_yf if yf in maturities_EONIA_yf] # TODO: find better way of doing this\n",
    "# print(columns_maturity_EUR3M_EONIA)\n",
    "# print(maturities_EUR3M_EONIA_yf)\n",
    "spreads_3M_EONIA.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EUR3M-EUR6M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreads_3M_6M = computeBasisSpreads(\n",
    "    data1 = data_EUR3M_bootstrapped,\n",
    "    data2 = data_EUR6M_bootstrapped,\n",
    "    columns_maturity1 = columns_maturity_EUR3M,\n",
    "    columns_maturity2 = columns_maturity_EUR6M,\n",
    "    column_refdate1 = 'RefDate',\n",
    "    column_refdate2 = 'RefDate'\n",
    ")\n",
    "columns_maturity_EUR3M_EUR6M = [c for c in columns_maturity_EUR3M if c in columns_maturity_EUR6M]\n",
    "maturities_EUR3M_EUR6M_yf = [yf for yf in maturities_EUR3M_yf if yf in maturities_EUR6M_yf] # TODO: find better way of doing this\n",
    "# print(columns_maturity_EUR3M_EUR6M)\n",
    "# print(maturities_EUR3M_EUR6M_yf)\n",
    "spreads_3M_6M.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario generation\n",
    "As mentioned in the introduction, our goal is to use historical data to simulate how much the relevant market variables might change from now to $n$ days from now.\n",
    "\n",
    "### Example 1\n",
    "Let's assume that $n=1$. In that case, we are asking how much a given market variable can change from one business day to the next.\n",
    "We assume that our historical data is ordered by date ascending and numbered consecutively, starting at 1. If $v_i$ denotes the value of the market variable on day $i$, then we can compute change scenarios in the following way.\n",
    "\n",
    "| Scenario | From | To | Absolute change | Relative change |\n",
    "| :---: | :---: | :---: | :---: | :---: |\n",
    "| 1 | Day 1 | Day 2 | $d_1 = v_2 - v_1$ | $q_1 = \\frac{v_2}{v_1}$ |\n",
    "| 2 | Day 2 | Day 3 | $d_2 = v_3 - v_2$ | $q_2 = \\frac{v_3}{v_2}$ |\n",
    "| 3 | Day 3 | Day 4 | $d_3 = v_4 - v_3$ | $q_3 = \\frac{v_4}{v_3}$ |\n",
    "| 4 | Day 4 | Day 5 | $d_4 = v_5 - v_4$ | $q_4 = \\frac{v_5}{v_4}$ |\n",
    "| ... | ||||\n",
    "\n",
    "After we compute these change scenarios (or shift scenarios), we can apply them to the current value $v$ of the market variable to obtain market scenarios: We can either add the absolute changes to the current value...\n",
    "\n",
    "| Scenario | Value of market variable |\n",
    "| :---: | :---: | \n",
    "| 1 | $v + d_1$ | \n",
    "| 2 | $v + d_2$ | \n",
    "| 3 | $v + d_3$ | \n",
    "| 4 | $v + d_4$ |\n",
    "| ... | |\n",
    "\n",
    "... or multiply the current value by the relative changes\n",
    "\n",
    "| Scenario | Value of market variable |\n",
    "| :---: | :---: | \n",
    "| 1 | $v \\cdot q_1$ |\n",
    "| 2 | $v \\cdot q_2$ | \n",
    "| 3 | $v \\cdot q_3$ | \n",
    "| 4 | $v \\cdot q_4$ |\n",
    "| ... | |\n",
    "\n",
    "Which of these approaches you choose should depend on the considered market variable. In the case of interest rates, it turns out that using absolute changes produces more realistic scenarios than using relative changes.\n",
    "\n",
    "### Example 2\n",
    "Note that, since we have one data point for every business day, the way we computed the change scenarios in Example 1 seemed very natural. If we now let $n=10$, we have to think about it more carefully. Consider the following two approaches.\n",
    "\n",
    "*Approach 1*\n",
    "\n",
    "We compute the change in value from day $i$ to day $i+10$ for **all days** where that is possible.\n",
    "\n",
    "| Scenario | From | To | Absolute Change | Relative Change |\n",
    "| :---: | :---: | :---: | :---: | :---: |\n",
    "| 1 | Day 1 | Day 11 | $v_{11} - v_1$ | $\\frac{v_{11}}{v_1}$ |\n",
    "| 2 | Day 2 | Day 12 | $v_{12} - v_2$ | $\\frac{v_{12}}{v_2}$ |\n",
    "| 3 | Day 3 | Day 13 | $v_{13} - v_3$ | $\\frac{v_{13}}{v_3}$ |\n",
    "| 4 | Day 4 | Day 14 | $v_{14} - v_4$ | $\\frac{v_{14}}{v_4}$ |\n",
    "| ... | ||||\n",
    "\n",
    "You'll find that this leads to significant **overlap in the time frames** (From -> To) behind the scenarios. The time frames of scenario 2 and scenario 4, for example, overlap in days 4 to 12. This introduces **correlation** between the scenarios.\n",
    "\n",
    "Remark: This is an example of **autocorrelation**. In the context of time series, autocorrelation is a measure of the similarity between values of one and the same variable at different points in time. In our example that variable is the change in interest rates in the last 10 days. By choosing overlapping time frames, each data point has an influence on multiple values in the time series. This leads to correlation between the values. The bigger the overlap, the stronger the correlation.\n",
    "\n",
    "\n",
    "*Approach 2*\n",
    "\n",
    "To avoid this effect, we can choose the time frames such that they have less or no overlap.\n",
    "\n",
    "| Scenario | From | To | Absolute Change | Relative Change |\n",
    "| :---: | :---: | :---: | :---: | :---: |\n",
    "| 1 | Day 1 | Day 11 | $v_{11} - v_{1}$ | $\\frac{v_{11}}{v_1}$ |\n",
    "| 2 | Day 11 | Day 21 | $v_{21} - v_{11}$ | $\\frac{v_{21}}{v_{11}}$ |\n",
    "| 3 | Day 21 | Day 31 | $v_{31} - v_{21}$ | $\\frac{v_{31}}{v_{21}}$ |\n",
    "| 4 | Day 31 | Day 41 | $v_{41} - v_{31}$ | $\\frac{v_{41}}{v_{31}}$ |\n",
    "| ... | ||||\n",
    "\n",
    "As a consequence, we end up with only about **a tenth the number of scenarios** we had in Approach 1. Of course, we can try to get more data, but that can be expensive or simply not possible (especially, if you consider time frames spanning a whole year, as is often the case in practice). Furthermore, one can argue that data becomes less relevant the further it reaches into the past. \n",
    "\n",
    "\n",
    "### What we do in this notebook\n",
    "The following code is generic in the sense that you can freely choose the time horizon $n$ and whether you want the scenarios to be computed using absolute or relative changes. However, the amount of overlap in the time frames can currently not be controlled.\n",
    "\n",
    "Assuming that our historical data is ordered by date ascending and numbered consecutively, let $n$ be the selected time horizon in days, $v$ be the current value of a market variable and $v_i$ the value it had on date $i$. Then we'll compute the value $s_i$ of the market variable in the $i$-th scenario as either\n",
    "$$s_i = v + (v_{i+n} - v_i)$$\n",
    "or\n",
    "$$s_i = v \\cdot \\frac{v_{i+n}}{v_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timehorizon = 1 # number of business days\n",
    "scenario_construction_type = 'absolute' # absolute or relative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: You can change these parameters to your liking and rerun the code to see the effects. You can use this to verify that the interest rate scenarios generated by applying relative changes can be a bit unrealistic.*\n",
    "\n",
    "We now compute scenarios using both approaches. Afterwards, we choose which set of scenarios we're actually going to use (based on the constant defined above). We assume that the latest EONIA curve available to us is the same as the current curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that computes the absolute or relative differences\n",
    "def computeScenarios(\n",
    "    data,\n",
    "    column_refdate,\n",
    "    columns_maturity,\n",
    "    ndays,\n",
    "    absolute = True\n",
    "):\n",
    "    # Sort by refdate descending\n",
    "    data_sorted = data.sort_values(column_refdate, ascending = False)\n",
    "    \n",
    "    # Restrict to the columns containing interest rates\n",
    "    data_rates_only = pd.DataFrame(data = data_sorted, columns = columns_maturity)\n",
    "    \n",
    "    # Copy the data frame structure\n",
    "    data_scenarios = pd.DataFrame().reindex_like(data_rates_only)\n",
    "    \n",
    "    # Compute the absolute or relative changes over n days\n",
    "    for i in range(len(data_scenarios.index)-ndays):\n",
    "        if absolute:\n",
    "            data_scenarios.iloc[i+ndays, :] = data_rates_only.iloc[i, :] - data_rates_only.iloc[i+ndays, :]\n",
    "        else:\n",
    "            if data_rates_only.iloc[i+ndays, :] != 0:\n",
    "                data_scenarios.iloc[i+ndays, :] = data_rates_onlyly.iloc[i, :] / data_rates_only.iloc[i+ndays, :]\n",
    "    \n",
    "    # Remove the rows containing NaN (i.e. the first n rows and those where we divided by 0)\n",
    "    return data_scenarios.dropna()\n",
    "\n",
    "\n",
    "def applyScenarios(\n",
    "    current,\n",
    "    scenarios\n",
    "):\n",
    "    if not isinstance(current, pd.Series):\n",
    "        raise Exception('The current data needs to be provided in a pandas.Series.')\n",
    "    if not isinstance(scenarios, pd.DataFrame):\n",
    "        raise Exception('Scenarios need to be provided in a pandas.DataFrame.')\n",
    "    if not set(current.index) == set(scenarios.columns):\n",
    "        raise Exception('The names of the columns of the current data and the scenario data need to match.')\n",
    "    \n",
    "    applied = pd.DataFrame().reindex_like(scenarios)\n",
    "    for i in applied.index:\n",
    "        applied.loc[i] = current + scenarios.loc[i]\n",
    "    return applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# save the current market data in a pandas.series\n",
    "data_EONIA_current = data_EONIA.iloc[0,:][columns_maturity_EONIA]\n",
    "\n",
    "# compute and apply scenarios\n",
    "data_scenarios_diff = computeScenarios(\n",
    "    data_EONIA,\n",
    "    'RefDate',\n",
    "    columns_maturity_EONIA,\n",
    "    timehorizon,\n",
    "    absolute = (scenario_construction_type == 'absolute')\n",
    ")\n",
    "data_scenarios = applyScenarios(data_EONIA_current, data_scenarios_diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ((also compute scenarios for EUR3M, EUR6M and spreads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_EUR3M.sort_values('RefDate', ascending = False, inplace = True)\n",
    "data_EUR6M.sort_values('RefDate', ascending = False, inplace = True)\n",
    "spreads_3M_EONIA.sort_values('RefDate', ascending = False, inplace = True)\n",
    "spreads_3M_6M.sort_values('RefDate', ascending = False, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Scenarios\n",
    "To get a sense of how different the generated scenarios are from the current data, we plot all of them and highlight the ones that are (in a certain sense) the 'most distant'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     2
    ]
   },
   "outputs": [],
   "source": [
    "# Compute the 'distances' of all scenarios to the current EONIA curve and sort them by that distance\n",
    "\n",
    "considered_columns = [\n",
    "    '1D',\n",
    "#    '1M',\n",
    "#    '2M',\n",
    "   '3M',\n",
    "#    '4M',\n",
    "#    '5M',\n",
    "   '6M',\n",
    "#    '7M',\n",
    "#    '8M',\n",
    "#    '9M',\n",
    "#    '10M',\n",
    "#    '11M',\n",
    "   '1Y',\n",
    "   '2Y',\n",
    "   '3Y',\n",
    "   '4Y',\n",
    "   '5Y',\n",
    "   '6Y',\n",
    "   '7Y',\n",
    "   '8Y',\n",
    "   '9Y',\n",
    "   '10Y'\n",
    "]\n",
    "diffs = data_scenarios[considered_columns] - data_EONIA_current[considered_columns]\n",
    "distances = [ np.linalg.norm(row, ord = 2) for index, row in diffs.iterrows() ]\n",
    "data_scenarios_with_dist = data_scenarios.copy()\n",
    "#print(data_scenarios_with_dist)\n",
    "data_scenarios_with_dist['dist'] = distances\n",
    "#print(distances)\n",
    "data_scenarios_with_dist.sort_values(by = 'dist', ascending = False, inplace=True)\n",
    "data_scenarios_with_dist = data_scenarios_with_dist.reset_index(drop=True)\n",
    "data_scenarios_without_dist = data_scenarios_with_dist.drop('dist', axis=1)\n",
    "#print(data_scenarios_with_dist)\n",
    "#print(data_scenarios_with_dist.iloc[0:10])\n",
    "\n",
    "# We'll highlight the 'most distant' scenarios in a different color in the plot below\n",
    "indeces_most_distant = data_scenarios_with_dist.index.isin([0,1,2,3])\n",
    "\n",
    "# We'll use this later\n",
    "maxdist_hist = max(distances)\n",
    "\n",
    "# clean up\n",
    "# diffs.describe()\n",
    "del diffs\n",
    "del distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     3,
     28
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the scenarios\n",
    "\n",
    "# use matplotlib\n",
    "if False:\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    ax = fig.gca()\n",
    "\n",
    "    color_current = 'w'\n",
    "    color_bulk = 'k'\n",
    "    color_maxdist = 'tab:blue'\n",
    "    ax.plot(maturities_EONIA_yf, data_EONIA_current, '.-', label = 'current EONIA curve', color = color_current, zorder = 20)\n",
    "    ax.plot(maturities_EONIA_yf, data_scenarios_without_dist[~indeces_most_distant].transpose(), '.-', label = 'other scenarios', color = color_bulk, zorder = 15, alpha=0.05)\n",
    "    ax.plot(maturities_EONIA_yf, data_scenarios_without_dist[indeces_most_distant].transpose(), '.-', label = 'extreme scenarios', color = color_maxdist, zorder = 15, alpha=1)\n",
    "\n",
    "    plt.xlabel('Expiry (in years)')\n",
    "    plt.ylabel('Interest rate (in percent)')\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    legend_elements = [\n",
    "        Patch(facecolor=color_current, edgecolor='gainsboro', label='current EONIA curve'),\n",
    "        Patch(facecolor=color_maxdist, label='extreme scenarios'),\n",
    "        Patch(facecolor=color_bulk, label='other scenarios')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# use plotly\n",
    "else:\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # We'll highlight the 'most distant' scenarios in a different color in the plot below\n",
    "    highlighted_indeces = [0,1,2,3]\n",
    "\n",
    "    showlegend = True\n",
    "    for i in data_scenarios_without_dist.index:\n",
    "        if i not in highlighted_indeces:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x = maturities_EONIA_yf,\n",
    "                y = data_scenarios_without_dist.iloc[i],\n",
    "                name = 'other scenarios',\n",
    "                legendgroup = 'other scenarios',\n",
    "                mode = default_plotly_scatter_mode,\n",
    "                showlegend = showlegend,\n",
    "                line=dict(color=\"Black\"),\n",
    "                opacity = 0.08\n",
    "            ))\n",
    "            showlegend = False\n",
    "\n",
    "    showlegend = True\n",
    "    for i in highlighted_indeces:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x = maturities_EONIA_yf,\n",
    "            y = data_scenarios_without_dist.iloc[i],\n",
    "            name = 'extreme scenarios',\n",
    "            legendgroup = 'extreme scenarios',\n",
    "            mode = default_plotly_scatter_mode,\n",
    "            showlegend = showlegend,\n",
    "            line=dict(color=color_graphblue)\n",
    "        ))\n",
    "        showlegend = False\n",
    "\n",
    "    fig.add_trace(go.Scatter(x = maturities_EONIA_yf, y = data_EONIA_current, name = 'current EONIA curve', mode = default_plotly_scatter_mode, line=dict(color=\"LightCyan\"),))\n",
    "\n",
    "\n",
    "    fig.update_layout(\n",
    "        showlegend=True,\n",
    "        xaxis = dict(title_text = \"Expiry (in years)\"),\n",
    "        yaxis = dict(title_text = \"Interest rate (in percent)\"),\n",
    "        legend = dict(traceorder='reversed'),\n",
    "        title=get_default_title_dict(\"Historical scenarios\")\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case that the scenarios were constructed using relative historical changes, you'll probably find that some of them are rather extreme. To get a better understanding of why they are, we take a closer look at the scenarios containing the highest and the lowest interest rates found in any scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if scenario_construction_type == 'relative':\n",
    "    # print(data_scenarios.min())\n",
    "    # print(data_scenarios.idxmin())\n",
    "    # print(data_scenarios.min().min())\n",
    "    # print(data_scenarios.min().idxmin())\n",
    "    # print(imin)\n",
    "\n",
    "    imin = data_scenarios.idxmin()[data_scenarios.min().idxmin()]\n",
    "    imax = data_scenarios.idxmax()[data_scenarios.max().idxmax()]\n",
    "\n",
    "    display(\n",
    "        pd.DataFrame({\n",
    "            'Current': data_EONIA_current,\n",
    "            'imin': data_EONIA_rates_only.loc[imin,:],\n",
    "            'imin - n': data_EONIA_rates_only.loc[imin - timehorizon,:],\n",
    "            'Scenario (imin)': data_scenarios.loc[imin,:],\n",
    "            'imax': data_EONIA_rates_only.loc[imax,:],\n",
    "            'imax - n': data_EONIA_rates_only.loc[imax - timehorizon,:],\n",
    "            'Scenario (imax)': data_scenarios.loc[imax,:]\n",
    "        }).head(len(data_EONIA_current))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define a function that handles plotting the scenarios\n",
    "def PlotMonteCarloScenarios(monteCarloScenarios, useMatplotlib = True):\n",
    "    \n",
    "    if useMatplotlib:\n",
    "        fig = plt.figure(figsize=(16,8))\n",
    "        ax = fig.gca()\n",
    "        color_current = 'w'\n",
    "        color_bulk = 'k'\n",
    "        ax.plot(maturities_EONIA_yf, data_EONIA_current, '.-', color = color_current, zorder = 20)\n",
    "        ax.plot(maturities_EONIA_yf, monteCarloScenarios.transpose(), '.-', color = color_bulk, zorder = 15, alpha=20/len(monteCarloScenarios.index))\n",
    "\n",
    "        plt.xlabel('Expiry (in years)')\n",
    "        plt.ylabel('Interest rate (in percent)')\n",
    "\n",
    "        legend_elements = [\n",
    "            Patch(facecolor=color_current, edgecolor='gainsboro', label='current EONIA curve'),\n",
    "            Patch(facecolor=color_bulk, label='Monte Carlo scenarios')\n",
    "        ]\n",
    "        plt.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "        plt.show()\n",
    "    else:\n",
    "        fig = go.Figure()\n",
    "\n",
    "        showlegend = True\n",
    "        opacity = max(0.001, min(1, 25/len(monteCarloScenarios.index)))\n",
    "        for i in monteCarloScenarios.index:\n",
    "            if i not in highlighted_indeces:\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x = maturities_EONIA_yf,\n",
    "                    y = monteCarloScenarios.iloc[i],\n",
    "                    name = 'other scenarios',\n",
    "                    legendgroup = 'other scenarios',\n",
    "                    mode = default_plotly_scatter_mode,\n",
    "                    showlegend = showlegend,\n",
    "                    line=dict(color=\"Black\"),\n",
    "                    opacity = opacity\n",
    "                ))\n",
    "                showlegend = False\n",
    "\n",
    "        fig.add_trace(go.Scatter(x = maturities_EONIA_yf, y = data_EONIA_current, name = 'current EONIA curve', mode = default_plotly_scatter_mode, line=dict(color=\"LightCyan\"),))\n",
    "\n",
    "\n",
    "        fig.update_layout(\n",
    "            showlegend=True,\n",
    "            xaxis = dict(title_text = \"Expiry (in years)\"),\n",
    "            yaxis = dict(title_text = \"Interest rate (in percent)\"),\n",
    "            legend = dict(traceorder='reversed'),\n",
    "            title={\n",
    "                'text': \"Monte Carlo scenarios\",\n",
    "                'y':0.9,\n",
    "                'x':0.475,\n",
    "                'xanchor': 'center',\n",
    "                'yanchor': 'top'}\n",
    "        )\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple simulation\n",
    "We apply a randomized parallel shift to the current interest rate curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Generate scenarios\n",
    "n_sims_simpleMC = default_sample_size_MC\n",
    "np.random.seed(7001)\n",
    "random_shifts = np.random.normal(0, 0.02, n_sims_simpleMC)\n",
    "data_scenarios_random_shift = pd.DataFrame().reindex_like(data_scenarios)\n",
    "data_scenarios_random_shift = data_scenarios_random_shift.iloc[0:0]\n",
    "\n",
    "for x in random_shifts:\n",
    "    data_scenarios_random_shift = data_scenarios_random_shift.append(data_EONIA_current + x, ignore_index=True)\n",
    "\n",
    "# data_scenarios_random_shift.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotMonteCarloScenarios(data_scenarios_random_shift)\n",
    "# PlotMonteCarloScenarios(data_scenarios_random_shift, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly picking historical data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using random distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Generate scenarios\n",
    "n_picks_dist = default_sample_size_MC\n",
    "np.random.seed(7002)\n",
    "random_dists = np.random.normal(0, maxdist_hist/3, n_picks_dist)\n",
    "random_dists = [abs(d) for d in random_dists]\n",
    "\n",
    "data_scenarios_random_pick_dist = pd.DataFrame().reindex_like(data_scenarios_with_dist)\n",
    "data_scenarios_random_pick_dist = data_scenarios_random_pick_dist.iloc[0:0]\n",
    "\n",
    "for d in random_dists:\n",
    "    data_scenarios_random_pick_dist = data_scenarios_random_pick_dist.append(\n",
    "        data_scenarios_with_dist.iloc[(data_scenarios_with_dist['dist']-d).abs().argsort()[:1]],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    \n",
    "#print(maxdist_hist)\n",
    "#data_scenarios_random_pick_dist.describe()\n",
    "\n",
    "data_scenarios_random_pick_dist = data_scenarios_random_pick_dist.drop('dist', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# PlotMonteCarloScenarios(data_scenarios_random_pick_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using random indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Generate scenarios\n",
    "n_picks_indeces = default_sample_size_MC\n",
    "np.random.seed(7003)\n",
    "num_hist_scenarios = len(data_scenarios_without_dist.index)\n",
    "random_indeces= []\n",
    "\n",
    "while len(random_indeces) < n_picks_indeces:\n",
    "    sample = math.floor(abs(np.random.normal(0, num_hist_scenarios/1)))\n",
    "    if sample >= 0 and sample < num_hist_scenarios:\n",
    "        random_indeces.append(sample)\n",
    "\n",
    "data_scenarios_random_pick_indeces = pd.DataFrame().reindex_like(data_scenarios_without_dist)\n",
    "data_scenarios_random_pick_indeces = data_scenarios_random_pick_indeces.iloc[0:0]\n",
    "\n",
    "for i in random_indeces:\n",
    "    data_scenarios_random_pick_indeces = data_scenarios_random_pick_indeces.append(\n",
    "        data_scenarios_without_dist.iloc[num_hist_scenarios-i-1],\n",
    "        ignore_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PlotMonteCarloScenarios(data_scenarios_random_pick_indeces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using buckets\n",
    "Instead of sampling from all historical interest rate curves available to us, we now divide them into buckets and choose one representative curve for each of them. Then, we take random samples from the set of buckets, instead of the entire set of historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Create buckets\n",
    "data_scenarios_buckets = pd.DataFrame().reindex_like(data_scenarios_without_dist)\n",
    "data_scenarios_buckets = data_scenarios_buckets.iloc[0:0]\n",
    "\n",
    "# The first bucket contains the current curve itself, i.e. the curve with distance 0\n",
    "data_scenarios_buckets = data_scenarios_buckets.append(\n",
    "    data_EONIA_current,\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "n_hist_buckets_max = 60\n",
    "# Then we increase the distance in steps of maxdist/n\n",
    "k = len(data_scenarios_with_dist.index)-1 # data_scenarios_with_dist is ordered by dist descending\n",
    "k_old = np.NaN\n",
    "for i in range(1, n_hist_buckets_max-1):\n",
    "    while data_scenarios_with_dist[\"dist\"][k] < i * maxdist_hist/n_hist_buckets_max:\n",
    "        k -= 1;\n",
    "#     print(k+1)\n",
    "    if k_old != k:\n",
    "        data_scenarios_buckets = data_scenarios_buckets.append(\n",
    "            data_scenarios_without_dist.iloc[k+1],\n",
    "            ignore_index=True\n",
    "        )\n",
    "    k_old = k\n",
    "\n",
    "# The last bucket is represented by the curve, which is the 'most distant' to the current curve\n",
    "data_scenarios_buckets = data_scenarios_buckets.append(\n",
    "    data_scenarios_without_dist.iloc[0],\n",
    "    ignore_index=True\n",
    ")\n",
    "# print(len(data_scenarios_buckets.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Generate scenarios\n",
    "n_picks_buckets = default_sample_size_MC\n",
    "np.random.seed(7004)\n",
    "\n",
    "data_scenarios_random_buckets = pd.DataFrame().reindex_like(data_scenarios_without_dist)\n",
    "data_scenarios_random_buckets = data_scenarios_random_buckets.iloc[0:0]\n",
    "\n",
    "while len(data_scenarios_random_buckets.index) < n_picks_buckets:\n",
    "    sample = math.floor(abs(np.random.normal(0, len(data_scenarios_buckets.index)/2)))\n",
    "    if sample >= 0 and sample < len(data_scenarios_buckets.index):\n",
    "        data_scenarios_random_buckets = data_scenarios_random_buckets.append(\n",
    "            data_scenarios_buckets.iloc[sample],\n",
    "            ignore_index=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PlotMonteCarloScenarios(data_scenarios_random_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation via short rate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DC defined by the current EONIA data\n",
    "maturities_EONIA_dates = getDates(refdate, maturities_EONIA_yf)\n",
    "dc_riskfree = getDiscountCurve(\n",
    "    'dc_riskfree',\n",
    "    refdate,\n",
    "    maturities_EONIA_dates,\n",
    "    data_EONIA_current,\n",
    "    UnitInterestRate.PERCENT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_model = analytics.HullWhiteModel('HW_Model', refdate, 0.4, 0.002, dc_riskfree)\n",
    "hw_dc = model_tools.compute_yieldcurve(hw_model, refdate,maturities_EONIA_dates)\n",
    "\n",
    "mkt_plot.curve(dc_riskfree, maturities_EONIA_dates, refdate, True)\n",
    "mkt_plot.curve(hw_dc, maturities_EONIA_dates, refdate, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Plot EONIA interest rate curve\n",
    "# fig = go.Figure()\n",
    "# year_fractions = []\n",
    "# for i in range(len(maturities_EONIA_dates)):\n",
    "#     year_fractions.append(default_daycounter.yf(refdate, maturities_EONIA_dates[i]))\n",
    "    \n",
    "# values = [x for x in data_EONIA_current]\n",
    "\n",
    "# fig.add_trace(go.Scatter(x = year_fractions, y = values, mode = 'lines+markers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_plot.curve(dc_riskfree, maturities_EONIA_dates, refdate, False)\n",
    "mkt_plot.curve(hw_dc, maturities_EONIA_dates, refdate, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Plot discount curve based on EONIA rates and the yield curve produced by the hw_model\n",
    "# fig = go.Figure()\n",
    "\n",
    "# values = analytics.vectorDouble()\n",
    "# dc_riskfree.value(values, refdate, maturities_EONIA_dates)\n",
    "# # convert to normal list\n",
    "# values = [x for x in values]\n",
    "\n",
    "# fig.add_trace(go.Scatter(x = year_fractions, y = values, mode = 'lines+markers'))\n",
    "     \n",
    "    \n",
    "# values = analytics.vectorDouble()\n",
    "# hw_dc.value(values, refdate, maturities_EONIA_dates)\n",
    "# # convert to normal list\n",
    "# values = [x for x in values]\n",
    "\n",
    "# fig.add_trace(go.Scatter(x = year_fractions, y = values, mode = 'lines+markers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# produce scenarios using hw_model\n",
    "sim_dates = converter.createPTimeList(refdate, maturities_EONIA_dates)\n",
    "refdate_LTime = converter.getLTime(refdate)\n",
    "n_sims = default_sample_size_MC\n",
    "n_steps_per_year = 200\n",
    "max_num_threads = 2\n",
    "\n",
    "hw_lab = analytics.ModelLab(hw_model, refdate_LTime)\n",
    "hw_lab.simulate(sim_dates, n_sims, n_steps_per_year, max_num_threads)\n",
    "\n",
    "# sampling_points_EONIA_datediffdays = [math.ceil(365*yf) for yf in maturities_EONIA_yf]\n",
    "# sim_dates = converter.createPTimeList(refdate, sampling_points_EONIA_datediffdays)\n",
    "\n",
    "\n",
    "# for i in range(n_sims):\n",
    "#     cir_lab.setFromSimulatedValues(cir, 1, i)  \n",
    "#     dc = model_tools.compute_yieldcurve(cir, sim_dates[0], sampling_points_EONIA_datediffdays)    \n",
    "#     mkt_plot.curve(dc, sim_dates, sim_dates[0], True, '', False)\n",
    "\n",
    "\n",
    "# data_array_df = []\n",
    "data_array_ir = []\n",
    "\n",
    "for i in range(n_sims):\n",
    "    hw_lab.setFromSimulatedValues(hw_model, 1, i)  \n",
    "    dc = model_tools.compute_yieldcurve(hw_model, sim_dates[0], sim_dates)  \n",
    "    daycounter = analytics.DayCounter(dc.getDayCounterType())\n",
    "#     data_array_df.append(\n",
    "#         [ dc.value(sim_dates[j], refdate) for j in range(len(sim_dates))]\n",
    "#     )\n",
    "    data_array_ir.append(\n",
    "        getInterestRatesFromDiscountCurve(\n",
    "            dc,\n",
    "            refdate,\n",
    "            sim_dates,\n",
    "            UnitInterestRate.PERCENT\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # mkt_plot.curve(dc, sim_dates, sim_dates[0], True, '', False)\n",
    "\n",
    "    \n",
    "data_scenarios_hull_white = pd.DataFrame(\n",
    "    data = data_array_ir,\n",
    "    columns = columns_maturity_EONIA\n",
    ")\n",
    "\n",
    "# del data_array_df\n",
    "del data_array_ir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotMonteCarloScenarios(data_scenarios_hull_white)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple portfolio\n",
    "To keep things simple, we start off with a portfolio containing only one fixed coupon bond with the following specifications:\n",
    " - It was issued on 2019/12/30\n",
    " - It has a maturity of 10 years\n",
    " - Its principal is 100\n",
    " - It pays a 5 coupon every year\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#define_bond\n",
    "maturity = 10\n",
    "principal = 100.0\n",
    "coupon_rate = 0.05\n",
    "maturity_date = dt.datetime(year = refdate.year + maturity, month = refdate.month, day = refdate.day)\n",
    "#print(refdate)\n",
    "#print(maturity_date)\n",
    "\n",
    "# Generate the coupon payment schedule as a vector of datetimes\n",
    "coupon_dates = []\n",
    "for i in range(maturity):\n",
    "    coupon_dates.append(dt.datetime(year = refdate.year + i + 1, month = refdate.month, day = refdate.day))\n",
    "#print(coupon_dates)\n",
    "coupon_rates = [coupon_rate]*len(coupon_dates)\n",
    "coupon_payments = [coupon_rate*principal]*len(coupon_dates)\n",
    "\n",
    "# We now use these specifications to define a fixed coupon bond\n",
    "fixed_coupon_bond = pyvacon.instruments.BondSpecification('Fixed_Coupon', 'DBK', enums.SecuritizationLevel.NONE, 'EUR',\n",
    "    maturity_date, refdate, principal, default_daycounter_type, coupon_dates, coupon_rates, '', [], [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current value of this portfolio can be computed by simply summing over all discounted future cash flows. Therefore, the only market variables affecting this value are the interest rates we use to determine the discount factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Credit Spread and Portfolio Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define the pricer, we're going to use to price our bond\n",
    "pricing_data_simple = pyvacon.pricing.BondPricingData()\n",
    "pricing_data_simple.param = pyvacon.pricing.BondPricingParameter()\n",
    "pricing_data_simple.param.useJLT = False\n",
    "pricing_data_simple.pricingRequest = pyvacon.pricing.PricingRequest()\n",
    "pricing_data_simple.pricingRequest.setCleanPrice(True)\n",
    "pricing_data_simple.pricer = 'BondPricer'\n",
    "pricing_data_simple.spec = fixed_coupon_bond\n",
    "\n",
    "valdate = refdate # + dt.timedelta(days = timehorizon)\n",
    "pricing_data_simple.valDate = valdate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We are currently not taking portfolio aging into account: In the computations below, we are using the reference date as valuation date. That is, we look at the effects our shift scenarios would have on the value of our portfolio, if they were to happen instantaneously (instead of over the next $n$ days)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Credit Spread\n",
    "**Credit spread** is the difference in yield between two investments of similar maturities, but different credit qualities. It can be interpreted as the risk premium for one investment over the other.\n",
    "\n",
    "Given the low EONIA rates, the bond we defined above currently has a much higher yield than a hypothetical bond paying EONIA rates on the same principal. Pricing the bond using discount factors based on EONIA rates would grossly overestimate its value. That is, the price we compute would be a lot higher than the bond's actual market value. To avoid this, we determine the constant shift we need to apply to the interest rates used for discounting in order for our price to equal the market value of the bond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Use the current EONIA rates + a constant rate to compute the price of the fixed coupon bond\n",
    "# Vary the constant rate and repeat until the value of the bond is right about the same as its principal\n",
    "creditspread = coupon_rate * 100 # in percent\n",
    "stepsize = coupon_rate * 100 # the initial step size used to vary the interest rate\n",
    "spreads = []\n",
    "values = []\n",
    "for k in range(20):\n",
    "    # create DC defined by the scenario\n",
    "    spreadScenario = data_EONIA_current + creditspread;\n",
    "    pricing_data_simple.discountCurve = getDiscountCurve(\n",
    "        'Discount Curve',\n",
    "        refdate,\n",
    "        maturities_EONIA_dates,\n",
    "        spreadScenario,\n",
    "        UnitInterestRate.PERCENT\n",
    "    )\n",
    "    \n",
    "    results = pyvacon.pricing.price(pricing_data_simple)\n",
    "    \n",
    "    values.append(results.getPrice())\n",
    "    spreads.append(creditspread)\n",
    "    \n",
    "    if values[k] > principal:\n",
    "        creditspread += stepsize\n",
    "    else:\n",
    "        creditspread -= stepsize\n",
    "    stepsize /= 2\n",
    "\n",
    "#print(spreads)\n",
    "#print(values)\n",
    "#print(creditspread)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "round(creditspread, 3)": "4.888"
    }
   },
   "source": [
    "The credit spread is {{round(creditspread, 3)}}%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Compute the current value without taking the credit spread into account\n",
    "pricing_data_simple.discountCurve = getDiscountCurve(\n",
    "    'Discount Curve',\n",
    "    refdate,\n",
    "    maturities_EONIA_dates,\n",
    "    data_EONIA_current,\n",
    "    UnitInterestRate.PERCENT\n",
    ")\n",
    "results = pyvacon.pricing.price(pricing_data_simple)\n",
    "current_value_without_credit_spread = results.getPrice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Compute the current value\n",
    "pricing_data_simple.discountCurve = getDiscountCurve(\n",
    "    'Discount Curve',\n",
    "    refdate,\n",
    "    maturities_EONIA_dates,\n",
    "    data_EONIA_current + creditspread,\n",
    "    UnitInterestRate.PERCENT\n",
    ")\n",
    "results = pyvacon.pricing.price(pricing_data_simple)\n",
    "currentPriceDirty = results.getPrice()\n",
    "currentPriceClean = results.getCleanPrice()\n",
    "#print(currentPriceDirty)\n",
    "#print(currentPriceClean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "round(currentPriceDirty, 6)": "99.999946",
     "round(current_value_without_credit_spread, 2)": "150.02"
    }
   },
   "source": [
    "By taking into account the credit spread we computed above, we arrive at a current value of {{round(currentPriceDirty, 6)}}, which closely matches the actual market value of 100. If we didn't take the credit spread into account, we would arrive at a value of {{round(current_value_without_credit_spread, 2)}}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Portfolio Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Choose the scenarios that we're going to use\n",
    "data_scenarios = data_scenarios\n",
    "#data_scenarios = data_scenarios_random_buckets\n",
    "compare_scenarios = True\n",
    "#data_scenarios_compare = data_scenarios_random_pick_indeces\n",
    "#data_scenarios_compare = data_scenarios_random_pick_dist\n",
    "#data_scenarios_compare = data_scenarios_random_shift\n",
    "#data_scenarios_compare = data_scenarios_random_buckets\n",
    "#data_scenarios_compare = data_scenarios_buckets\n",
    "data_scenarios_compare = data_scenarios_hull_white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     2
    ]
   },
   "outputs": [],
   "source": [
    "# Compute the price of the fixed coupon bond at the valuation date defined above\n",
    "# Repeat for every scenario\n",
    "def ComputeValuesOfSimplePortfolio(refdate, sampling_points_dates, data_scenarios, creditspread, daycounter_type, interpolation_type, extrapolation_type):\n",
    "    results_dirty = []\n",
    "    results_clean = []\n",
    "    for index, scenario in data_scenarios.iterrows():\n",
    "        # add the credit spread we computed for our bond\n",
    "        scenario = scenario + creditspread\n",
    "        \n",
    "        pricing_data_simple.discountCurve = getDiscountCurve(\n",
    "            'Discount Curve',\n",
    "            refdate,\n",
    "            sampling_points_dates,\n",
    "            scenario,\n",
    "            UnitInterestRate.PERCENT\n",
    "        )\n",
    "\n",
    "        results = pyvacon.pricing.price(pricing_data_simple)\n",
    "        results_dirty.append(results.getPrice())\n",
    "        results_clean.append(results.getCleanPrice())\n",
    "        #print(pricing_data_simple.spec.getObjectId() + ', dirty price: ' + str(results.getPrice()) + \",  clean price: \" + str(results.getCleanPrice()))\n",
    "    return [results_dirty, results_clean]\n",
    "    \n",
    "\n",
    "results_dirty, results_clean = ComputeValuesOfSimplePortfolio(refdate, maturities_EONIA_dates, data_scenarios, creditspread, default_daycounter_type, default_interpolation_type, default_extrapolation_type)\n",
    "    \n",
    "if compare_scenarios:\n",
    "    results_dirty_compare, results_clean_compare = ComputeValuesOfSimplePortfolio(refdate, maturities_EONIA_dates, data_scenarios_compare, creditspread, default_daycounter_type, default_interpolation_type, default_extrapolation_type)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# In case we want to compare scenarios, we determine the minimal and maximal x-values to display in upcoming plots\n",
    "if compare_scenarios:\n",
    "    xmin = min(min(results_dirty), min(results_dirty_compare))\n",
    "    xmax = max(max(results_dirty), max(results_dirty_compare))\n",
    "    \n",
    "    #minIndex = results_dirty.index(min(results_dirty))\n",
    "    #print(minIndex)\n",
    "    #print(results_dirty[minIndex])\n",
    "    #print(data_scenarios.iloc[minIndex])\n",
    "\n",
    "    #maxIndex = results_dirty.index(max(results_dirty))\n",
    "    #print(maxIndex)\n",
    "    #print(results_dirty[maxIndex])\n",
    "    #print(data_scenarios.iloc[maxIndex])\n",
    "\n",
    "    #results_series = pd.Series(results_dirty)\n",
    "    #display(results_series.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot pricing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define a function that plots the histrograms\n",
    "def plotHistogram(\n",
    "    data,\n",
    "    binsstart,\n",
    "    binsend,\n",
    "    nbins,\n",
    "    title_xaxis,\n",
    "    markercolor = color_histmarker,\n",
    "    bordercolor = color_histmarkerborder\n",
    "):\n",
    "    xbins = dict(start = binsstart, end = binsend, size = (binsend-binsstart)/nbins)\n",
    "    marker=dict(\n",
    "        color=markercolor,\n",
    "        line = dict(color = bordercolor, width = 1)\n",
    "    )\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Histogram(x = data, xbins = xbins, marker = marker))\n",
    "\n",
    "    fig.update_layout(\n",
    "        showlegend=False,\n",
    "        xaxis = dict(title_text = title_xaxis, range = [binsstart, binsend]),\n",
    "        yaxis = dict(title_text = \"Number of occurences\")\n",
    "    #     ,title={\n",
    "    #         'text': \"Historical scenarios\",\n",
    "    #         'y':0.9,\n",
    "    #         'x':0.475,\n",
    "    #         'xanchor': 'center',\n",
    "    #         'yanchor': 'top'}\n",
    "    )\n",
    "\n",
    "    fig.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Histogramm of the pricing results\n",
    "plotHistogram(\n",
    "    data = results_dirty,\n",
    "    binsstart = xmin,\n",
    "    binsend = xmax,\n",
    "    nbins = 60,\n",
    "    title_xaxis = \"Portfolio value\"\n",
    ")\n",
    "\n",
    "if compare_scenarios:\n",
    "    plotHistogram(\n",
    "        data = results_dirty_compare,\n",
    "        binsstart = xmin,\n",
    "        binsend = xmax,\n",
    "        nbins = 60,\n",
    "        title_xaxis = \"Portfolio value\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Histogramm of the changes/differences in value\n",
    "valDiffsDirty = np.asarray([res - currentPriceDirty for res in results_dirty])\n",
    "binsstart_simple = xmin - currentPriceDirty\n",
    "binsend_simple = xmax - currentPriceDirty\n",
    "plotHistogram(\n",
    "    data = valDiffsDirty,\n",
    "    binsstart = binsstart_simple,\n",
    "    binsend = binsend_simple,\n",
    "    nbins = 60,\n",
    "    title_xaxis = \"Change in portfolio value\"\n",
    ")\n",
    "\n",
    "\n",
    "if compare_scenarios:\n",
    "    valDiffsDirty_compare = np.asarray([res - currentPriceDirty for res in results_dirty_compare])\n",
    "    plotHistogram(\n",
    "        data = valDiffsDirty_compare,\n",
    "        binsstart = binsstart_simple,\n",
    "        binsend = binsend_simple,\n",
    "        nbins = 60,\n",
    "        title_xaxis = \"Change in portfolio value\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Value at Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# determine quantile\n",
    "valDiffsDirty = (-1)*np.sort((-1)*valDiffsDirty)\n",
    "quantile = 0.99\n",
    "#print(np.quantile(valDiffsDirty, 1-quantile, interpolation='higher')) # apparently always uses ascending order\n",
    "\n",
    "# Compute the number of the entry corresponding to the quantile defined above\n",
    "quantileIndex = np.ceil(len(valDiffsDirty)*quantile).astype(int)\n",
    "#print(quantileIndex)\n",
    "#print(quantile * len(valDiffsDirty))\n",
    "\n",
    "# To get the index of this entry, we have to subtract 1\n",
    "quantileIndex -= 1\n",
    "\n",
    "# Check correctness\n",
    "#print('--------------')\n",
    "#print(valDiffsDirty[quantileIndex-1])\n",
    "#print((quantileIndex)/len(valDiffsDirty))\n",
    "#print('--------------')\n",
    "#print(valDiffsDirty[quantileIndex])\n",
    "#print((quantileIndex + 1)/len(valDiffsDirty))\n",
    "#print('--------------')\n",
    "#print(valDiffsDirty[quantileIndex+1])\n",
    "#print((quantileIndex + 2)/len(valDiffsDirty))\n",
    "#print('--------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot cummulative relative frequencies of loss of portfolio value\n",
    "losses = -valDiffsDirty\n",
    "\n",
    "marker=dict(\n",
    "    color=color_histmarker\n",
    ")\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x = losses, nbinsx = len(losses)*2, marker = marker, cumulative_enabled = True, histnorm = 'probability'))\n",
    "\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    xaxis = dict(title_text = \"Loss of portfolio value\"),\n",
    "    yaxis = dict(title_text = \"Cumulative relative frequency\")\n",
    "#     ,title={\n",
    "#         'text': \"Historical scenarios\",\n",
    "#         'y':0.9,\n",
    "#         'x':0.475,\n",
    "#         'xanchor': 'center',\n",
    "#         'yanchor': 'top'}\n",
    ")\n",
    "\n",
    "fig.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "round((quantileIndex + 1)/len(valDiffsDirty)*100, 3)": "99.197",
     "round(-1 * valDiffsDirty[quantileIndex], 4)": "0.5208",
     "timehorizon": "1"
    }
   },
   "source": [
    "With a probability of {{round((quantileIndex + 1)/len(valDiffsDirty)*100, 3)}}% the value of our portfolio is not going to shrink by more than {{round(-1 * valDiffsDirty[quantileIndex], 4)}} in the next {{timehorizon}} day(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended portfolio\n",
    "## Add a swap\n",
    "We swap the fixed coupon payments for interest payments based on EONIA rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define the swap scpecification\n",
    "startdates = [refdate]\n",
    "startdates.extend(coupon_dates[0:len(coupon_dates)-1])\n",
    "#startdates = converter.createPTimeList(refdate, startdates)\n",
    "\n",
    "enddates = coupon_dates\n",
    "#enddates = converter.createPTimeList(enddates, startdates)\n",
    "\n",
    "#print(startdates)\n",
    "#print(enddates)\n",
    "\n",
    "paydates = enddates\n",
    "resetdates = startdates\n",
    "\n",
    "notionals = analytics.vectorDouble()\n",
    "notionals.append(principal)\n",
    "\n",
    "fixedleg = analytics.IrFixedLegSpecification(coupon_rate, notionals, startdates, enddates, paydates,'EUR', default_daycounter_type)\n",
    "\n",
    "floatleg = analytics.IrFloatLegSpecification(notionals, resetdates, startdates, enddates,\n",
    "                                    paydates,'EUR', 'test_udl', default_daycounter_type, \n",
    "                                    0)\n",
    "                                    #creditspread/100) # spread is given in percent\n",
    "\n",
    "ir_swap = analytics.InterestRateSwapSpecification('TEST_SWAP', 'DBK', enums.SecuritizationLevel.COLLATERALIZED, 'EUR',\n",
    "                                           converter.getLTime(paydates[-1]), fixedleg, floatleg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recompute the value of our portfolio in all scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Specify all data we need to price the swap\n",
    "ir_swap_pricing_data = analytics.InterestRateSwapPricingData()\n",
    "\n",
    "pay_leg_pricing_data = analytics.InterestRateSwapLegPricingData()\n",
    "pay_leg_pricing_data.spec = ir_swap.getPayLeg()\n",
    "pay_leg_pricing_data.fxRate = 1.0\n",
    "pay_leg_pricing_data.weight = -1.0\n",
    "\n",
    "rec_leg_pricing_data = analytics.InterestRateSwapFloatLegPricingData()\n",
    "rec_leg_pricing_data.spec = ir_swap.getReceiveLeg()\n",
    "rec_leg_pricing_data.fxRate = 1.0\n",
    "rec_leg_pricing_data.weight = 1.0\n",
    "\n",
    "ir_swap_pricing_data.pricer = 'InterestRateSwapPricer'\n",
    "ir_swap_pricing_data.pricingRequest = analytics.PricingRequest()\n",
    "ir_swap_pricing_data.valDate = converter.getLTime(refdate)\n",
    "ir_swap_pricing_data.setCurr('EUR')\n",
    "ir_swap_pricing_data.addLegData(pay_leg_pricing_data)\n",
    "ir_swap_pricing_data.addLegData(rec_leg_pricing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Compute the price of our portfolio\n",
    "# Repeat for every scenario\n",
    "results_dirty = []\n",
    "results_clean = []\n",
    "for index, scenario in data_scenarios.iterrows():\n",
    "    dcEONIA = getDiscountCurve(\n",
    "        'dcEONIA',\n",
    "        refdate,\n",
    "        maturities_EONIA_dates,\n",
    "        scenario,\n",
    "        UnitInterestRate.PERCENT\n",
    "    )\n",
    "    \n",
    "    dcWithSpread = getDiscountCurve(\n",
    "        'dcWithSpread',\n",
    "        refdate,\n",
    "        maturities_EONIA_dates,\n",
    "        scenario + creditspread,\n",
    "        UnitInterestRate.PERCENT\n",
    "    )\n",
    "    \n",
    "    pricing_data_simple.discountCurve = dcEONIA # dcWithSpread\n",
    "    pay_leg_pricing_data.discountCurve = dcEONIA\n",
    "    rec_leg_pricing_data.discountCurve = dcEONIA\n",
    "    rec_leg_pricing_data.fixingCurve = dcEONIA\n",
    "    \n",
    "    prBond = pyvacon.pricing.price(pricing_data_simple)\n",
    "    prSwap = analytics.price(ir_swap_pricing_data)\n",
    "    dirty = prBond.getPrice() + prSwap.getPrice()\n",
    "    clean = prBond.getCleanPrice() + prSwap.getCleanPrice()\n",
    "    results_dirty.append(dirty)\n",
    "    results_clean.append(clean)\n",
    "    #print(pricing_data_simple.spec.getObjectId() + ', dirty price: ' + str(results.getPrice()) + \",  clean price: \" + str(results.getCleanPrice()))\n",
    "#print(results_dirty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the current value as a reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define a discount curve based on the current EONIA rates\n",
    "dcEONIA = getDiscountCurve(\n",
    "    'dcEONIA',\n",
    "    refdate,\n",
    "    maturities_EONIA_dates,\n",
    "    data_EONIA_current,\n",
    "    UnitInterestRate.PERCENT\n",
    ")\n",
    "\n",
    "dcWithSpread = getDiscountCurve(\n",
    "    'dcWithSpread',\n",
    "    refdate,\n",
    "    maturities_EONIA_dates,\n",
    "    data_EONIA_current + creditspread,\n",
    "    UnitInterestRate.PERCENT\n",
    ")\n",
    "\n",
    "pricing_data_simple.discountCurve = dcEONIA # dcWithSpread\n",
    "pay_leg_pricing_data.discountCurve = dcEONIA\n",
    "rec_leg_pricing_data.discountCurve = dcEONIA \n",
    "rec_leg_pricing_data.fixingCurve = dcEONIA\n",
    "\n",
    "# compute portfolio value\n",
    "prBond = pyvacon.pricing.price(pricing_data_simple)\n",
    "prSwap = analytics.price(ir_swap_pricing_data)\n",
    "#print(prSwap.getPrice())\n",
    "#print(prBond.getPrice())\n",
    "currentValueBond = prBond.getPrice()\n",
    "currentValueSwap = prSwap.getPrice()\n",
    "currentValue = prBond.getPrice() + prSwap.getPrice()\n",
    "#print(currentValueSwap)\n",
    "#print(currentValueBond)\n",
    "#print(currentValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the pricing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Histogramm of the changes/differences in value\n",
    "valDiffsDirty = np.asarray([res - currentValue for res in results_dirty])\n",
    "plotHistogram(\n",
    "    data = valDiffsDirty,\n",
    "    binsstart = binsstart_simple,\n",
    "    binsend = binsend_simple,\n",
    "    nbins = 60,\n",
    "    title_xaxis = 'Change in portfolio value'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the swap we added to our portfolio cancels out any market risk, setting the value at risk to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interest Rate Shock Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters for the shock scenarios\n",
    "\n",
    "shockParams = pd.DataFrame({'Currency': [], 'Parallel': [], 'Short': [], 'Long': []})\n",
    "shockParams = shockParams.append({'Currency': 'EUR', 'Parallel': 200, 'Short': 250, 'Long': 100}, ignore_index = True)\n",
    "shockParams = shockParams.append({'Currency': 'GBP', 'Parallel': 250, 'Short': 300, 'Long': 150}, ignore_index = True)\n",
    "shockParams = shockParams.append({'Currency': 'USD', 'Parallel': 200, 'Short': 300, 'Long': 150}, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the change in value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Compute the price of our portfolio\n",
    "# Repeat for every scenario\n",
    "results_dirty = []\n",
    "results_clean = []\n",
    "results_dirty_bondonly = []\n",
    "results_clean_bondonly = []\n",
    "\n",
    "currency = 'EUR'\n",
    "parallel = shockParams.loc[shockParams['Currency'] == currency].loc[0]['Parallel']\n",
    "short = shockParams.loc[shockParams['Currency'] == currency].loc[0]['Short']\n",
    "long = shockParams.loc[shockParams['Currency'] == currency].loc[0]['Long']\n",
    "    \n",
    "shockScenarios = ['ParallelUp', 'ParallelDown', 'ShortUp', 'ShortDown', 'LongUp', 'LongDown', 'Flatten', 'Steepen']\n",
    "\n",
    "for shockScenario in shockScenarios:\n",
    "    dcEONIA = getShockedDiscountCurve(\n",
    "        'dc_linear',\n",
    "        refdate,\n",
    "        maturities_EONIA_dates,\n",
    "        data_EONIA_current*100,\n",
    "        default_daycounter_type,\n",
    "        default_interpolation_type,\n",
    "        default_extrapolation_type,\n",
    "        shockScenario,\n",
    "        parallel,\n",
    "        short,\n",
    "        long\n",
    "    )\n",
    "    \n",
    "#     dcWithSpread = getShockedDiscountCurve(\n",
    "#          'dc_linear_spread',\n",
    "#          refdate,\n",
    "#          maturities_EONIA_dates,\n",
    "#          data_EONIA_current + creditspread,\n",
    "#          default_daycounter_type,\n",
    "#          default_interpolation_type,\n",
    "#          default_extrapolation_type,\n",
    "#          shockScenario,\n",
    "#          parallel/100,\n",
    "#          short/100,\n",
    "#          long/100\n",
    "#      )\n",
    "    \n",
    "    pricing_data_simple.discountCurve = dcEONIA # dcWithSpread\n",
    "    pay_leg_pricing_data.discountCurve = dcEONIA\n",
    "    rec_leg_pricing_data.discountCurve = dcEONIA\n",
    "    rec_leg_pricing_data.fixingCurve = dcEONIA\n",
    "    \n",
    "    prBond = pyvacon.pricing.price(pricing_data_simple)\n",
    "    prSwap = analytics.price(ir_swap_pricing_data)\n",
    "    dirty = prBond.getPrice() + prSwap.getPrice()\n",
    "    clean = prBond.getCleanPrice() + prSwap.getCleanPrice()\n",
    "    results_dirty.append(dirty)\n",
    "    results_clean.append(clean)\n",
    "    results_dirty_bondonly.append(prBond.getPrice())\n",
    "    results_clean_bondonly.append(prBond.getCleanPrice())\n",
    "    #print(pricing_data_simple.spec.getObjectId() + ', dirty price: ' + str(results.getPrice()) + \",  clean price: \" + str(results.getCleanPrice()))\n",
    "#print(results_dirty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the change in value (comparison between our entire portfolio and the bond on its own)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Bar plot of the changes/differences in value\n",
    "valDiffsDirty = np.asarray([res - currentValue for res in results_dirty])\n",
    "valDiffsDirtyBondOnly = np.asarray([res - currentValueBond for res in results_dirty_bondonly])\n",
    "\n",
    "y_shockscenarios_simple = valDiffsDirtyBondOnly/currentValueBond*100\n",
    "ymin = min(y_shockscenarios_simple)\n",
    "ymax = max(y_shockscenarios_simple)\n",
    "ydiff = abs(ymax-ymin)\n",
    "rangemin = ymin - ydiff/10\n",
    "rangemax = ymax + ydiff/10\n",
    "\n",
    "marker=dict(\n",
    "    color=color_histmarker,\n",
    "    line = dict(color = color_histmarker, width = 1)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Plot results for the simple portfolio\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=shockScenarios, y=y_shockscenarios_simple, marker = marker))\n",
    "\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    xaxis = dict(title_text = \"Shock Scenario\"),\n",
    "    yaxis = dict(title_text = \"Change in portfolio value [%]\", range = [rangemin, rangemax])\n",
    "    ,title=get_default_title_dict(\"Simple Portfolio\")\n",
    ")\n",
    "\n",
    "fig.show()  \n",
    "\n",
    "\n",
    "\n",
    "# Plot results for the extended portfolio\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=shockScenarios, y=valDiffsDirty/currentValue*100, marker = marker))\n",
    "\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    xaxis = dict(title_text = \"Shock Scenario\"),\n",
    "    yaxis = dict(title_text = \"Change in portfolio value [%]\", range = [rangemin, rangemax])\n",
    "    ,title=get_default_title_dict(\"Extended Portfolio\")\n",
    ")\n",
    "\n",
    "fig.show()  \n",
    "\n",
    "\n",
    "del y_shockscenarios_simple, ymin, ymax, ydiff, rangemin, rangemax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bootstrap zero-coupon interest rates\n",
    "- Bootstrappen der 3M und 6M\n",
    "- Differenz OIS <-> 3M\n",
    "- Differenz 3M <-> 6M\n",
    "- Differenz OIS <-> 6M als Summe der beiden anderen Differenzen?\n",
    "- check use of iloc vs loc\n",
    "- 'Einheit' der Zinssaetze mit an die Funktionen uebergeben (dezimal, percent, basispoint) // in Arbeit\n",
    "- Das Wort 'current' als Beschreibung fr die jngste in den Marktdaten vorhandene EONIA-Kurve ueberdenken\n",
    "- Actually order the historical data by date ascending (instead of descending) to avoid confusion\n",
    "- Illustrate the definition of VaR\n",
    "- Portfolio mit zwei Bonds unterschiedlicher Laufzeiten untersuchen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "268.25px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
