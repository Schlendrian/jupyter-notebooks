{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvacon\n",
    "import pyvacon.marketdata.testdata as mkt_testdata\n",
    "import pyvacon.tools.enums as enums\n",
    "import pyvacon.marketdata.plot as mkt_plot\n",
    "import pyvacon.models.plot as model_plot\n",
    "import pyvacon.models.tools as model_tools\n",
    "import pyvacon.analytics as analytics\n",
    "import pyvacon.tools.converter as converter\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch, Rectangle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.transforms as mtransforms\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime as dt\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define some constants, which we'll use repeatedly throughout this notebook.\n",
    "color_main = 'tab:blue'\n",
    "color_highlight = 'tab:orange'\n",
    "grid_alpha = 0.4\n",
    "daycounter_type_standard = enums.DayCounter.ACTACT\n",
    "interpolation_type_standard = enums.InterpolationType.LINEAR\n",
    "extrapolation_type_standard = enums.ExtrapolationType.NONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## Value at risk\n",
    "Value at risk (VaR) is a measure for the risk in a portfolio of financial assets. Given a time horizon of $n$ days and a confidence level $\\alpha$, the VaR is the loss of value, which has the probability $\\alpha$ not to be exceeded within the next $n$ days. In other words, the VaR is the $\\alpha$-quantile of the distribution of loss in the value of a portfolio other the next $n$ days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different methods for estimating the value at risk can be put into two major categories: Those using analytical models and those using simulations.\n",
    "\n",
    "The goal of **analytical** methods is to define a probability distribution, which approximates the actual probability distribution of the portfolio value. One can then write down a closed formula for the value at risk.\n",
    "\n",
    "**Simulation**-based methods simulate the change in value over the next $n$ days and use the resulting relative frequency distribution to 'read off' the value at risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical simulation\n",
    "A very popular way of simulating changes in value uses past market data to estimate what will happen in the future. To do so, we first have to identify all market variables affecting the portfolio value. Then we collect data on how these variables moved over the past $k+n$ days. This allows us to calculate $k$ historical scenarios of what can happen in $n$ days. Assuming that the market will behave in the future as it did in the past, we can compute the portfolio value in each of these scenarios. This provides us with a relative frequency distribution, which we then use to determine the value at risk.\n",
    "\n",
    "The goal of this notebook is to give a simple example of such a historical simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo simulation\n",
    "Monte Carlo simulation is similar to historical simulation in the sense that we also\n",
    "- generate a set of market scenarios,\n",
    "- compute the value of our portfolio in each of these scenarios and\n",
    "- use the resulting relative frequency distribution to determine the value at risk.\n",
    "\n",
    "They differ in the method for generating market scenarios: Instead of historical data, Monte Carlo simulation uses randomly generated movements of all relevant market variables. This requires more work (for example, you first have to develop a model for the market movements), but also comes with more flexibility.\n",
    "\n",
    "*Note: While we don't have a notebook about this topic as of yet, we do intend to create one in the future.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple portfolio\n",
    "To keep things simple, we start off with a portfolio containing only one fixed coupon bond with the following specifications:\n",
    " - It was issued on 2019/12/30\n",
    " - It has a maturity of 10 years\n",
    " - Its principal is 100€\n",
    " - It pays a 5€ coupon every year\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refdate = dt.datetime(year = 2019, month = 12, day = 30)\n",
    "maturity = 10\n",
    "principal = 100.0\n",
    "coupon_rate = 0.05\n",
    "maturity_date = dt.datetime(year = refdate.year + maturity, month = refdate.month, day = refdate.day)\n",
    "#print(refdate)\n",
    "#print(maturity_date)\n",
    "\n",
    "# Generate the coupon payment schedule as a vector of datetimes\n",
    "coupon_dates = []\n",
    "for i in range(maturity):\n",
    "    coupon_dates.append(dt.datetime(year = refdate.year + i + 1, month = refdate.month, day = refdate.day))\n",
    "#print(coupon_dates)\n",
    "coupon_rates = [coupon_rate]*len(coupon_dates)\n",
    "coupon_payments = [coupon_rate*principal]*len(coupon_dates)\n",
    "\n",
    "# We now use these specifications to define a fixed coupon bond\n",
    "fixed_coupon_bond = pyvacon.instruments.BondSpecification('Fixed_Coupon', 'DBK', enums.SecuritizationLevel.NONE, 'EUR',\n",
    "    maturity_date, refdate, principal, daycounter_type_standard, coupon_dates, coupon_rates, '', [], [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current value of this portfolio can be computed by simply summing over all discounted future cash flows. Therefore, the only market variables affecting this value are the interest rates we use to determine the discount factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical data\n",
    "We choose to discount future cash flows using EONIA interest rate curves. We have historical data from every business day of 2018 and 2019 available to us. The data includes the actual over-night rates plus forward rates for various maturities. We'll load the data for maturities of 1 day, 1-11 months and 1-10 years.\n",
    "\n",
    "*Note: These interest rates are not zero-coupon rates, but we are currently using them as is. Bootstrapping is still on the TODO list.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data from an Excel file\n",
    "xl = pd.ExcelFile('TestDaten.xlsx')\n",
    "#print(xl.sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_EONIA = xl.parse('EONIA')\n",
    "data_EONIA = pd.DataFrame(data_EONIA,\n",
    "                       columns = [\n",
    "                           'stichtag6',\n",
    "                           'prz_bs1t',\n",
    "                           'prz_bs1m',\n",
    "                           'prz_bs2m',\n",
    "                           'prz_bs3m',\n",
    "                           'prz_bs4m',\n",
    "                           'prz_bs5m',\n",
    "                           'prz_bs6m',\n",
    "                           'prz_bs7m',\n",
    "                           'prz_bs8m',\n",
    "                           'prz_bs9m',\n",
    "                           'prz_bs10m',\n",
    "                           'prz_bs11m',\n",
    "                           'prz_bs1j',\n",
    "                           'prz_bs2j',\n",
    "                           'prz_bs3j',\n",
    "                           'prz_bs4j',\n",
    "                           'prz_bs5j',\n",
    "                           'prz_bs6j',\n",
    "                           'prz_bs7j',\n",
    "                           'prz_bs8j',\n",
    "                           'prz_bs9j',\n",
    "                           'prz_bs10j'\n",
    "                       ])\n",
    "\n",
    "\n",
    "# convert Excel dates to a more useful format and add them to the data frame as a new column\n",
    "data_EONIA['datum'] = pd.TimedeltaIndex(data_EONIA['stichtag6'], unit='d') + dt.datetime(1899, 12, 30)\n",
    "#display(data_EONIA.head(5))\n",
    "#display(data_EONIA.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the reference date as the date of the first market data sample (rows are already ordered by date desc)\n",
    "#refdate = dt.datetime(year = data_EONIA.iloc[0]['datum'].year, month = data_EONIA.iloc[0]['datum'].month, day = data_EONIA.iloc[0]['datum'].day)\n",
    "#refdate = dt.datetime(data_EONIA.iloc[0]['datum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we'll need them later, we store the selected maturities in the form of year fractions and dates (relative to our reference date defined above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maturities in years\n",
    "sampling_points_EONIA_yf = [1/365] # 1 day\n",
    "sampling_points_EONIA_yf.extend( (np.arange(11)+1)/12 ) # 1 to 11 months\n",
    "sampling_points_EONIA_yf.extend(np.arange(10)+1) # 1 to 10 years\n",
    "#print(sampling_points_EONIA_yf)\n",
    "\n",
    "# compute the maturity dates (using the reference date defined above)\n",
    "sampling_points_EONIA_dates = []\n",
    "sampling_points_EONIA_dates.append(refdate + dt.timedelta(days = 1))\n",
    "for i in range(11):\n",
    "    sampling_points_EONIA_dates.append(refdate + dt.timedelta(days = (i+1)*30))\n",
    "for i in range(10):\n",
    "    year = refdate.year + i + 1\n",
    "    month = refdate.month\n",
    "    day = refdate.day\n",
    "    sampling_points_EONIA_dates.append(\n",
    "        dt.datetime(year = year, month = month , day = day)\n",
    "    )\n",
    "#print(sampling_points_EONIA_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario generation\n",
    "As mentioned in the introduction, our goal is to use historical data to simulate how much the relevant market variables might change from now to $n$ days from now.\n",
    "\n",
    "### Example 1\n",
    "Let's assume that $n=1$. In that case, we are asking how much a given market variable can change from one business day to the next.\n",
    "We assume that our historical data is ordered by date ascending and numbered consecutively, starting at 1. If $v_i$ denotes the value of the market variable on day $i$, then we can compute change scenarios in the following way.\n",
    "\n",
    "| Scenario | From | To | Absolute change | Relative change |\n",
    "| :---: | :---: | :---: | :---: | :---: |\n",
    "| 1 | Day 1 | Day 2 | $d_1 = v_2 - v_1$ | $q_1 = \\frac{v_2}{v_1}$ |\n",
    "| 2 | Day 2 | Day 3 | $d_2 = v_3 - v_2$ | $q_2 = \\frac{v_3}{v_2}$ |\n",
    "| 3 | Day 3 | Day 4 | $d_3 = v_4 - v_3$ | $q_3 = \\frac{v_4}{v_3}$ |\n",
    "| 4 | Day 4 | Day 5 | $d_4 = v_5 - v_4$ | $q_4 = \\frac{v_5}{v_4}$ |\n",
    "| ... | ||||\n",
    "\n",
    "After we compute these change scenarios (or shift scenarios), we can apply them to the current value $v$ of the market variable to obtain market scenarios: We can either add the absolute changes to the current value...\n",
    "\n",
    "| Scenario | Value of market variable |\n",
    "| :---: | :---: | \n",
    "| 1 | $v + d_1$ | \n",
    "| 2 | $v + d_2$ | \n",
    "| 3 | $v + d_3$ | \n",
    "| 4 | $v + d_4$ |\n",
    "| ... | |\n",
    "\n",
    "... or multiply the current value by the relative changes\n",
    "\n",
    "| Scenario | Value of market variable |\n",
    "| :---: | :---: | \n",
    "| 1 | $v \\cdot q_1$ |\n",
    "| 2 | $v \\cdot q_2$ | \n",
    "| 3 | $v \\cdot q_3$ | \n",
    "| 4 | $v \\cdot q_4$ |\n",
    "| ... | |\n",
    "\n",
    "Which of these approaches you choose should depend on the considered market variable. In the case of interest rates, it turns out that using absolute changes produces more realistic scenarios than using relative changes.\n",
    "\n",
    "### Example 2\n",
    "Note that, since we have one data point for every business day, the way we computed the change scenarios in Example 1 seemed very natural. If we now let $n=10$, we have to think about it more carefully. Consider the following two approaches.\n",
    "\n",
    "*Approach 1*\n",
    "\n",
    "We compute the change in value from day $i$ to day $i+10$ for **all days** where that is possible.\n",
    "\n",
    "| Scenario | From | To | Absolute Change | Relative Change |\n",
    "| :---: | :---: | :---: | :---: | :---: |\n",
    "| 1 | Day 1 | Day 11 | $v_{11} - v_1$ | $\\frac{v_{11}}{v_1}$ |\n",
    "| 2 | Day 2 | Day 12 | $v_{12} - v_2$ | $\\frac{v_{12}}{v_2}$ |\n",
    "| 3 | Day 3 | Day 13 | $v_{13} - v_3$ | $\\frac{v_{13}}{v_3}$ |\n",
    "| 4 | Day 4 | Day 14 | $v_{14} - v_4$ | $\\frac{v_{14}}{v_4}$ |\n",
    "| ... | ||||\n",
    "\n",
    "You'll find that this leads to significant **overlap in the time frames** (From -> To) behind the scenarios. The time frames of scenario 2 and scenario 4, for example, overlap in days 4 to 12. This results in an **increased correlation** between the scenarios.\n",
    "\n",
    "Remark: This is an example of **autocorrelation**. In the context of time series, autocorrelation is a measure of the similarity between values of one and the same variable at different points in time. In the case of interest rates, one naturally expects a certain level of autocorrelation: Today's interest rates tend to be similar to yesterday's, but less similar to those from a year ago. By choosing overlapping time frames as a basis for our scenarios, we're introducing another source of autocorrelation.\n",
    "\n",
    "\n",
    "*Approach 2*\n",
    "\n",
    "To avoid this effect, we can choose the time frames such that they have less or no overlap.\n",
    "\n",
    "| Scenario | From | To | Absolute Change | Relative Change |\n",
    "| :---: | :---: | :---: | :---: | :---: |\n",
    "| 1 | Day 1 | Day 11 | $v_{11} - v_{1}$ | $\\frac{v_{11}}{v_1}$ |\n",
    "| 2 | Day 11 | Day 21 | $v_{21} - v_{11}$ | $\\frac{v_{21}}{v_{11}}$ |\n",
    "| 3 | Day 21 | Day 31 | $v_{31} - v_{21}$ | $\\frac{v_{31}}{v_{21}}$ |\n",
    "| 4 | Day 31 | Day 41 | $v_{41} - v_{31}$ | $\\frac{v_{41}}{v_{31}}$ |\n",
    "| ... | ||||\n",
    "\n",
    "As a consequence, we end up with only about **a tenth the number of scenarios** we had in Approach 1. Of course, we can try to get more data, but that can be expensive or simply not possible (especially, if you consider time frames spanning a whole year, as is often the case in practice). Furthermore, one can argue that data becomes less relevant the further it reaches into the past. \n",
    "\n",
    "\n",
    "### What we do in this notebook\n",
    "The following code is generic in the sense that you can freely choose the time horizon $n$ and whether you want the scenarios to be computed using absolute or relative changes. However, the amount of overlap in the time frames can currently not be controlled.\n",
    "\n",
    "Assuming that our historical data is ordered by date ascending and numbered consecutively, let $n$ be the selected time horizon in days, $v$ be the current value of a market variable and $v_i$ the value it had on date $i$. Then we'll compute the value $s_i$ of the market variable in the $i$-th scenario as either\n",
    "$$s_i = v + (v_{i+n} - v_i)$$\n",
    "or\n",
    "$$s_i = v \\cdot \\frac{v_{i+n}}{v_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timehorizon = 1 # number of business days\n",
    "scenario_construction_type = 'absolute' # absolute or relative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: You can change these parameters to your liking and rerun the code to see the effects. You can use this to verify that the interest rate scenarios generated by applying relative changes can be a bit unrealistic.*\n",
    "\n",
    "We now compute scenarios using both approaches. Afterwards, we choose which set of scenarios we're actually going to use (based on the constant defined above). We assume that the latest EONIA curve available to us is the same as the current curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict to the columns containing interest rates\n",
    "data_EONIA_rates_only = pd.DataFrame(\n",
    "                            data_EONIA,\n",
    "                            columns = data_EONIA.columns[~data_EONIA.columns.isin(['stichtag6','datum'])]\n",
    "                        )\n",
    "\n",
    "# save the current market data in a pandas.series\n",
    "data_EONIA_current = data_EONIA_rates_only.iloc[0,:]\n",
    "#display(data_EONIA_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute absolute and relative changes\n",
    "\n",
    "# Copy the data frame structure\n",
    "data_scenarios_absolute = pd.DataFrame().reindex_like(data_EONIA_rates_only)\n",
    "data_scenarios_relative = pd.DataFrame().reindex_like(data_EONIA_rates_only)\n",
    "\n",
    "# Compute the values\n",
    "n = timehorizon\n",
    "\n",
    "for i in range(len(data_EONIA_rates_only.index) - n):\n",
    "    for col in data_EONIA_rates_only.columns:\n",
    "        data_scenarios_absolute.iloc[i + n, :][col] = data_EONIA_current[col] + data_EONIA_rates_only.iloc[i, :][col] - data_EONIA_rates_only.iloc[i + n, :][col]\n",
    "        if data_EONIA_rates_only.iloc[i + n, :][col] != 0:\n",
    "            data_scenarios_relative.iloc[i + n, :][col] = data_EONIA_current[col] * data_EONIA_rates_only.iloc[i, :][col] / data_EONIA_rates_only.iloc[i + n, :][col]\n",
    "\n",
    "            \n",
    "# Remove the rows containing NaN (i.e. the first n rows and those where we divided by 0)\n",
    "data_scenarios_absolute = data_scenarios_absolute.dropna()\n",
    "data_scenarios_relative = data_scenarios_relative.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide which scenarios to use in the rest of the notebook\n",
    "if scenario_construction_type == 'relative':\n",
    "    data_scenarios = data_scenarios_relative\n",
    "if scenario_construction_type == 'absolute':\n",
    "    data_scenarios = data_scenarios_absolute\n",
    "\n",
    "# data_scenarios.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Scenarios\n",
    "To get a sense of how different the generated scenarios are from the current data, we plot all of them and highlight the ones that are (in a certain sense) the 'most distant'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the 'distances' of all scenarios to the current EONIA curve and sort them by that distance\n",
    "\n",
    "diffs = data_scenarios - data_EONIA_current\n",
    "distances = [ np.linalg.norm(row) for index, row in diffs.iterrows() ]\n",
    "data_scenarios_with_dist = data_scenarios.copy()\n",
    "#print(data_scenarios_with_dist)\n",
    "data_scenarios_with_dist['dist'] = distances\n",
    "#print(distances)\n",
    "data_scenarios_with_dist.sort_values(by = 'dist', ascending = False, inplace=True)\n",
    "data_scenarios_with_dist = data_scenarios_with_dist.drop('dist', axis=1)\n",
    "data_scenarios_with_dist = data_scenarios_with_dist.reset_index(drop=True)\n",
    "#print(data_scenarios_with_dist)\n",
    "#print(data_scenarios_with_dist.iloc[0:10])\n",
    "\n",
    "# We'll highlight the 'most distant' scenarios in a different color in the plot below\n",
    "indeces_most_distant = data_scenarios_with_dist.index.isin([0,1,2,3])\n",
    "\n",
    "# clean up\n",
    "del diffs\n",
    "del distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the graph\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax = fig.gca()\n",
    "\n",
    "color_current = 'w'\n",
    "color_bulk = 'k'\n",
    "color_maxdist = 'tab:blue'\n",
    "ax.plot(sampling_points_EONIA_yf, data_EONIA_current, '.-', label = 'current EONIA curve', color = color_current, zorder = 20)\n",
    "ax.plot(sampling_points_EONIA_yf, data_scenarios_with_dist[~indeces_most_distant].transpose(), '.-', label = 'other scenarios', color = color_bulk, zorder = 15, alpha=0.05)\n",
    "ax.plot(sampling_points_EONIA_yf, data_scenarios_with_dist[indeces_most_distant].transpose(), '.-', label = 'extreme scenarios', color = color_maxdist, zorder = 15, alpha=1)\n",
    "\n",
    "plt.xlabel('Expiry (in years)')\n",
    "plt.ylabel('Interest rate (in base points)')\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor=color_current, edgecolor='gainsboro', label='current EONIA curve'),\n",
    "    Patch(facecolor=color_maxdist, label='extreme scenarios'),\n",
    "    Patch(facecolor=color_bulk, label='other scenarios')\n",
    "]\n",
    "plt.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case that the scenarios were constructed using relative historical changes, you'll probably find that some of them are rather extreme. To get a better understanding of why they are, we take a closer look at the scenarios containing the highest and the lowest interest rates found in any scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scenario_construction_type == 'relative':\n",
    "    # print(data_scenarios.min())\n",
    "    # print(data_scenarios.idxmin())\n",
    "    # print(data_scenarios.min().min())\n",
    "    # print(data_scenarios.min().idxmin())\n",
    "    # print(imin)\n",
    "\n",
    "    imin = data_scenarios.idxmin()[data_scenarios.min().idxmin()]\n",
    "    imax = data_scenarios.idxmax()[data_scenarios.max().idxmax()]\n",
    "\n",
    "    display(\n",
    "        pd.DataFrame({\n",
    "            'Current': data_EONIA_current,\n",
    "            'imin': data_EONIA_rates_only.loc[imin,:],\n",
    "            'imin - n': data_EONIA_rates_only.loc[imin - timehorizon,:],\n",
    "            'Scenario (imin)': data_scenarios.loc[imin,:],\n",
    "            'imax': data_EONIA_rates_only.loc[imax,:],\n",
    "            'imax - n': data_EONIA_rates_only.loc[imax - timehorizon,:],\n",
    "            'Scenario (imax)': data_scenarios.loc[imax,:]\n",
    "        }).head(len(data_EONIA_current))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Credit Spread and Portfolio Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pricer, we're going to use to price our bond\n",
    "pricing_data_simple = pyvacon.pricing.BondPricingData()\n",
    "pricing_data_simple.param = pyvacon.pricing.BondPricingParameter()\n",
    "pricing_data_simple.param.useJLT = False\n",
    "pricing_data_simple.pricingRequest = pyvacon.pricing.PricingRequest()\n",
    "pricing_data_simple.pricingRequest.setCleanPrice(True)\n",
    "pricing_data_simple.pricer = 'BondPricer'\n",
    "pricing_data_simple.spec = fixed_coupon_bond\n",
    "\n",
    "valdate = refdate # + dt.timedelta(days = timehorizon)\n",
    "pricing_data_simple.valDate = valdate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We are currently not taking portfolio aging into account: In the computations below, we are using the reference date as valuation date. That is, we look at the effects our shift scenarios would have on the value of our portfolio, if they were to happen instantaneously (instead of over the next $n$ days)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Credit Spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the current EONIA rates + a constant rate to compute the price of the fixed coupon bond\n",
    "# Vary the constant rate and repeat until the value of the bond is right about the same as its principal\n",
    "creditspread = coupon_rate * 100 # in base points\n",
    "stepsize = coupon_rate * 100 # the initial step size used to vary the interest rate\n",
    "spreads = []\n",
    "values = []\n",
    "for k in range(20):\n",
    "    # create DC defined by the scenario\n",
    "    dsc_fac = analytics.vectorDouble()\n",
    "    spreadScenario = data_EONIA_current + creditspread;\n",
    "    for i in range(len(spreadScenario)):\n",
    "        dsc_fac.append(math.exp(-spreadScenario.iloc[i]/100*i)) # t = i years  # market data is given in base points -> /100  \n",
    "            \n",
    "    discountCurve = analytics.DiscountCurve('dc_linear', refdate, sampling_points_EONIA_dates, dsc_fac, enums.DayCounter.ACTACT, enums.InterpolationType.LINEAR, enums.ExtrapolationType.NONE)\n",
    "    pricing_data_simple.discountCurve = discountCurve\n",
    "    \n",
    "    results = pyvacon.pricing.price(pricing_data_simple)\n",
    "    \n",
    "    values.append(results.getPrice())\n",
    "    spreads.append(creditspread)\n",
    "    \n",
    "    if values[k] > principal:\n",
    "        creditspread += stepsize\n",
    "    else:\n",
    "        creditspread -= stepsize\n",
    "    stepsize /= 2\n",
    "\n",
    "#print(spreads)\n",
    "#print(values)\n",
    "#print(creditspread)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The credit spread is {{round(creditspread, 3)}}%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Portfolio Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the price of the fixed coupon bond at the valuation date defined above\n",
    "# Repeat for every scenario\n",
    "results_dirty = []\n",
    "results_clean = []\n",
    "for index, scenario in data_scenarios.iterrows():\n",
    "    # add the credit spread we computed for our bond\n",
    "    scenario = scenario + creditspread\n",
    "    \n",
    "    # create DC defined by the scenario\n",
    "    dsc_fac = analytics.vectorDouble()\n",
    "    for i in range(len(scenario)):\n",
    "            dsc_fac.append(math.exp(-scenario.iloc[i]/100*i)) # t = i years  # market data is given in base points -> /100  \n",
    "            \n",
    "    discountCurve = analytics.DiscountCurve('dc_linear', refdate, sampling_points_EONIA_dates, dsc_fac, daycounter_type_standard, interpolation_type_standard, extrapolation_type_standard)\n",
    "    pricing_data_simple.discountCurve = discountCurve\n",
    "    \n",
    "    results = pyvacon.pricing.price(pricing_data_simple)\n",
    "    results_dirty.append(results.getPrice())\n",
    "    results_clean.append(results.getCleanPrice())\n",
    "    #print(pricing_data_simple.spec.getObjectId() + ', dirty price: ' + str(results.getPrice()) + \",  clean price: \" + str(results.getCleanPrice()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minIndex = results_dirty.index(min(results_dirty))\n",
    "#print(minIndex)\n",
    "#print(results_dirty[minIndex])\n",
    "#print(data_scenarios.iloc[minIndex])\n",
    "\n",
    "#maxIndex = results_dirty.index(max(results_dirty))\n",
    "#print(maxIndex)\n",
    "#print(results_dirty[maxIndex])\n",
    "#print(data_scenarios.iloc[maxIndex])\n",
    "\n",
    "#results_series = pd.Series(results_dirty)\n",
    "#display(results_series.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the current value\n",
    "\n",
    "# Create DC defined by the scenario\n",
    "dsc_fac = analytics.vectorDouble()\n",
    "for i in range(len(scenario)):\n",
    "        dsc_fac.append(math.exp(-(data_EONIA_current + creditspread).iloc[i]/100*i)) # t = i years  # market data is given in base points -> /100  \n",
    "\n",
    "discountCurve = analytics.DiscountCurve('dc_linear', refdate, sampling_points_EONIA_dates, dsc_fac, daycounter_type_standard, interpolation_type_standard, extrapolation_type_standard)\n",
    "pricing_data_simple.discountCurve = discountCurve\n",
    "results = pyvacon.pricing.price(pricing_data_simple)\n",
    "currentPriceDirty = results.getPrice()\n",
    "currentPriceClean = results.getCleanPrice()\n",
    "#print(currentPriceDirty)\n",
    "#print(currentPriceClean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot pricing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramm of the pricing results\n",
    "results_dirty = np.sort(results_dirty)\n",
    "\n",
    "fig_values = plt.figure(figsize=(16,8))\n",
    "ax = fig_values.gca()\n",
    "\n",
    "ax.hist(results_dirty, bins=60, color = color_main, zorder = 20, edgecolor='w')\n",
    "ax.axvline(x=currentPriceDirty, ymin=0, ymax=1, color=color_highlight, zorder = 30)\n",
    "\n",
    "plt.xlabel('Portfolio value')\n",
    "plt.ylabel('Number of occurences')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramm of the changes/differences in value\n",
    "valDiffsDirty = np.asarray([res - currentPriceDirty for res in results_dirty])\n",
    "fig_diff = plt.figure(figsize=(16,8))\n",
    "ax = fig_diff.gca()\n",
    "\n",
    "# Save the bin edges for later use\n",
    "bins_simple_portfolio = ax.hist(valDiffsDirty, bins=60, color=color_main, zorder = 20, edgecolor='w')[1]\n",
    "ax.axvline(x=0, ymin=0, ymax=1, color=color_highlight, zorder = 30)\n",
    "\n",
    "plt.xlabel('Change in portfolio value')\n",
    "plt.ylabel('Number of occurences')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Value at Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valDiffsDirty = (-1)*np.sort((-1)*valDiffsDirty)\n",
    "quantile = 0.99\n",
    "#print(np.quantile(valDiffsDirty, 1-quantile, interpolation='higher')) # apparently always uses ascending order\n",
    "\n",
    "# Compute the number of the entry corresponding to the quantile defined above\n",
    "quantileIndex = np.ceil(len(valDiffsDirty)*quantile).astype(int)\n",
    "#print(quantileIndex)\n",
    "#print(quantile * len(valDiffsDirty))\n",
    "\n",
    "# To get the index of this entry, we have to subtract 1\n",
    "quantileIndex -= 1\n",
    "\n",
    "# Check correctness\n",
    "#print('--------------')\n",
    "#print(valDiffsDirty[quantileIndex-1])\n",
    "#print((quantileIndex)/len(valDiffsDirty))\n",
    "#print('--------------')\n",
    "#print(valDiffsDirty[quantileIndex])\n",
    "#print((quantileIndex + 1)/len(valDiffsDirty))\n",
    "#print('--------------')\n",
    "#print(valDiffsDirty[quantileIndex+1])\n",
    "#print((quantileIndex + 2)/len(valDiffsDirty))\n",
    "#print('--------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cummulative relative frequencies of loss of portfolio value\n",
    "fig_diff = plt.figure(figsize=(16,8))\n",
    "ax = fig_diff.gca()\n",
    "\n",
    "losses = -valDiffsDirty\n",
    "losses = np.sort(losses)\n",
    "\n",
    "# choose bins in a way which maximizes the resolution of the histogramm\n",
    "bins = np.unique(losses)\n",
    "bins = np.append(bins, losses[len(losses)-1] + 0.00001 )\n",
    "\n",
    "# 'density = True' produces relative frequencies instead of absolute numbers of occurences\n",
    "ax.hist(losses, bins=bins, color = color_main, cumulative = True, density = True, zorder = 20)\n",
    "\n",
    "# draw lines to highlight the quantile\n",
    "vlineAt = -valDiffsDirty[quantileIndex];\n",
    "hline = ax.axhline(y=quantile, xmin=0, xmax=1, color=color_highlight, linewidth=1, zorder = 30)\n",
    "vline = ax.axvline(x=vlineAt, ymin=0, ymax=1, color=color_highlight, linewidth=1, zorder = 30)\n",
    "\n",
    "# clip the lines\n",
    "eps = 0.1\n",
    "xmin = ax.get_xlim()[0];\n",
    "ymin = ax.get_ylim()[0];\n",
    "hrect = Rectangle((xmin, quantile - eps), abs(xmin) + vlineAt, 2*eps, facecolor=\"none\", edgecolor=\"none\")\n",
    "vrect = Rectangle((vlineAt - eps, ymin), 2*eps, abs(ymin) + quantile, facecolor=\"none\", edgecolor=\"none\")\n",
    "\n",
    "ax.add_artist(hrect)\n",
    "ax.add_artist(vrect)\n",
    "hline.set_clip_path(hrect)\n",
    "vline.set_clip_path(vrect)\n",
    "\n",
    "# TODO: weiter dran arbeiten\n",
    "\n",
    "\n",
    "plt.xlabel('Loss of portfolio value')\n",
    "plt.ylabel('Cumulative relative frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a probability of {{round((quantileIndex + 1)/len(valDiffsDirty)*100, 2)}}% the value of our portfolio is not going to shrink by more than {{round(-1 * valDiffsDirty[quantileIndex], 4)}} in the next {{timehorizon}} day(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended portfolio\n",
    "## Add a swap\n",
    "We swap the fixed coupon payments for an interest rate of EONIA plus the credit spread we computed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the swap scpecification\n",
    "startdates = [refdate]\n",
    "startdates.extend(coupon_dates[0:len(coupon_dates)-1])\n",
    "#startdates = converter.createPTimeList(refdate, startdates)\n",
    "\n",
    "enddates = coupon_dates\n",
    "#enddates = converter.createPTimeList(enddates, startdates)\n",
    "\n",
    "#print(startdates)\n",
    "#print(enddates)\n",
    "\n",
    "paydates = enddates\n",
    "resetdates = startdates\n",
    "\n",
    "notionals = analytics.vectorDouble()\n",
    "notionals.append(principal)\n",
    "\n",
    "fixedleg = analytics.IrFixedLegSpecification(coupon_rate, notionals, startdates, enddates, paydates,'EUR', daycounter_type_standard)\n",
    "\n",
    "floatleg = analytics.IrFloatLegSpecification(notionals, resetdates, startdates, enddates,\n",
    "                                    paydates,'EUR', 'test_udl', daycounter_type_standard, \n",
    "                                    0)\n",
    "                                    #creditspread/100) # spread is given in basepoints\n",
    "\n",
    "ir_swap = analytics.InterestRateSwapSpecification('TEST_SWAP', 'DBK', enums.SecuritizationLevel.COLLATERALIZED, 'EUR',\n",
    "                                           converter.getLTime(paydates[-1]), fixedleg, floatleg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recompute the value of our portfolio in all scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify all data we need to price the swap\n",
    "ir_swap_pricing_data = analytics.InterestRateSwapPricingData()\n",
    "\n",
    "pay_leg_pricing_data = analytics.InterestRateSwapLegPricingData()\n",
    "pay_leg_pricing_data.spec = ir_swap.getPayLeg()\n",
    "pay_leg_pricing_data.fxRate = 1.0\n",
    "pay_leg_pricing_data.weight = -1.0\n",
    "\n",
    "rec_leg_pricing_data = analytics.InterestRateSwapFloatLegPricingData()\n",
    "rec_leg_pricing_data.spec = ir_swap.getReceiveLeg()\n",
    "rec_leg_pricing_data.fxRate = 1.0\n",
    "rec_leg_pricing_data.weight = 1.0\n",
    "\n",
    "ir_swap_pricing_data.pricer = 'InterestRateSwapPricer'\n",
    "ir_swap_pricing_data.pricingRequest = analytics.PricingRequest()\n",
    "ir_swap_pricing_data.valDate = converter.getLTime(refdate)\n",
    "ir_swap_pricing_data.setCurr('EUR')\n",
    "ir_swap_pricing_data.addLegData(pay_leg_pricing_data)\n",
    "ir_swap_pricing_data.addLegData(rec_leg_pricing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the price of our portfolio\n",
    "# Repeat for every scenario\n",
    "results_dirty = []\n",
    "results_clean = []\n",
    "for index, scenario in data_scenarios.iterrows():\n",
    "    # add the credit spread we computed for our bond\n",
    "    \n",
    "    # create DC defined by the scenario\n",
    "    factorsEONIA = analytics.vectorDouble()\n",
    "    factorsWithSpread = analytics.vectorDouble()\n",
    "    for i in range(len(scenario)):\n",
    "        factorsEONIA.append(math.exp(-scenario.iloc[i]/100*i)) # t = i years  # market data is given in base points -> /100  \n",
    "        factorsWithSpread.append(math.exp(-(scenario.iloc[i] + creditspread)/100*i)) # t = i years  # market data is given in base points -> /100  \n",
    "            \n",
    "    dcEONIA = analytics.DiscountCurve('dc_linear', refdate, sampling_points_EONIA_dates, factorsEONIA, daycounter_type_standard, interpolation_type_standard, extrapolation_type_standard)\n",
    "    dcWithSpread   = analytics.DiscountCurve('dc_linear_spread', refdate, sampling_points_EONIA_dates, factorsWithSpread, daycounter_type_standard, interpolation_type_standard, extrapolation_type_standard)\n",
    "    \n",
    "    pricing_data_simple.discountCurve = dcEONIA # dcWithSpread\n",
    "    pay_leg_pricing_data.discountCurve = dcEONIA\n",
    "    rec_leg_pricing_data.discountCurve = dcEONIA\n",
    "    rec_leg_pricing_data.fixingCurve = dcEONIA\n",
    "    \n",
    "    prBond = pyvacon.pricing.price(pricing_data_simple)\n",
    "    prSwap = analytics.price(ir_swap_pricing_data)\n",
    "    dirty = prBond.getPrice() + prSwap.getPrice()\n",
    "    clean = prBond.getCleanPrice() + prSwap.getCleanPrice()\n",
    "    results_dirty.append(dirty)\n",
    "    results_clean.append(clean)\n",
    "    #print(pricing_data_simple.spec.getObjectId() + ', dirty price: ' + str(results.getPrice()) + \",  clean price: \" + str(results.getCleanPrice()))\n",
    "#print(results_dirty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the current value as a reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a discount curve based on the current EONIA rates\n",
    "factorsEONIA = analytics.vectorDouble()\n",
    "factorsWithSpread = analytics.vectorDouble()\n",
    "for i in range(len(data_EONIA_current)):\n",
    "    factorsEONIA.append(math.exp(-data_EONIA_current.iloc[i]/100*i)) # t = i years  # market data is given in base points -> /100  \n",
    "    factorsWithSpread.append(math.exp(-(data_EONIA_current.iloc[i] + creditspread)/100*i)) # t = i years  # market data is given in base points -> /100  \n",
    "    \n",
    "dcEONIA = analytics.DiscountCurve('dc_linear', refdate, sampling_points_EONIA_dates, factorsEONIA, daycounter_type_standard, interpolation_type_standard, extrapolation_type_standard)\n",
    "dcWithSpread = analytics.DiscountCurve('dc_linear_spread', refdate, sampling_points_EONIA_dates, factorsWithSpread, daycounter_type_standard, interpolation_type_standard, extrapolation_type_standard)\n",
    "\n",
    "pricing_data_simple.discountCurve = dcEONIA # dcWithSpread\n",
    "pay_leg_pricing_data.discountCurve = dcEONIA\n",
    "rec_leg_pricing_data.discountCurve = dcEONIA \n",
    "rec_leg_pricing_data.fixingCurve = dcEONIA\n",
    "\n",
    "# compute portfolio value\n",
    "prBond = pyvacon.pricing.price(pricing_data_simple)\n",
    "prSwap = analytics.price(ir_swap_pricing_data)\n",
    "#print(prSwap.getPrice())\n",
    "#print(prBond.getPrice())\n",
    "currentValueBond = prBond.getPrice()\n",
    "currentValueSwap = prSwap.getPrice()\n",
    "currentValue = prBond.getPrice() + prSwap.getPrice()\n",
    "#print(currentValueSwap)\n",
    "#print(currentValueBond)\n",
    "#print(currentValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the pricing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramm of the changes/differences in value\n",
    "valDiffsDirty = np.asarray([res - currentValue for res in results_dirty])\n",
    "fig_diff = plt.figure(figsize=(16,8))\n",
    "ax = fig_diff.gca()\n",
    "\n",
    "# Use the same bins as we did in the histogram for the simple portfolio\n",
    "ax.hist(valDiffsDirty, bins=bins_simple_portfolio, color = color_main, zorder = 20, edgecolor='w')\n",
    "\n",
    "plt.xlabel('Change in portfolio value')\n",
    "plt.ylabel('Number of occurences')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the swap we added to our portfolio cancels out any market risk, setting the value at risk to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Interest Rate Shock Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# define functions to help compute the shock scenarios\n",
    "\n",
    "def shortRateShock(t, basepoints):\n",
    "    if isinstance(t, (list, np.ndarray)):\n",
    "        result = []\n",
    "        for x in t:\n",
    "            result.append(shortRateShock(x, basepoints))\n",
    "        #print('list')\n",
    "        return result\n",
    "    #print('scalar')\n",
    "    #print(type(t))\n",
    "    return np.exp(-t/4) * basepoints\n",
    "\n",
    "\n",
    "def longRateShock(t, basepoints):\n",
    "    if isinstance(t, (list, np.ndarray)):\n",
    "        result = []\n",
    "        for x in t:\n",
    "            result.append(longRateShock(x, basepoints))\n",
    "        return result\n",
    "    return (1-math.exp(-t/4)) * basepoints\n",
    "\n",
    "\n",
    "def steepener(t, basepointsShort, basepointsLong):\n",
    "    if isinstance(t, (list, np.ndarray)):\n",
    "        result = []\n",
    "        for x in t:\n",
    "            result.append(steepener(x, basepointsShort, basepointsLong))\n",
    "        return result\n",
    "    return -0.65 * shortRateShock(t, basepointsShort)  +  0.9 * longRateShock(t, basepointsLong)\n",
    "\n",
    "\n",
    "def flattener(t, basepointsShort, basepointsLong):\n",
    "    if isinstance(t, (list, np.ndarray)):\n",
    "        result = []\n",
    "        for x in t:\n",
    "            result.append(flattener(x, basepointsShort, basepointsLong))\n",
    "        return result\n",
    "    return 0.8 * shortRateShock(t, basepointsShort)  -  0.6 * longRateShock(t, basepointsLong)\n",
    "\n",
    "\n",
    "def getShockValue(t, shockScenario, parallel = 0, short = 0, long = 0):\n",
    "    \n",
    "    if shockScenario == 'Parallel':\n",
    "        return parallel\n",
    "    \n",
    "    if shockScenario == 'ParallelUp':\n",
    "        return parallel\n",
    "    \n",
    "    if shockScenario == 'ParallelDown':\n",
    "        return -parallel\n",
    "    \n",
    "    \n",
    "    \n",
    "    if shockScenario == 'Short':\n",
    "        return shortRateShock(t, short)\n",
    "    \n",
    "    if shockScenario == 'ShortUp':\n",
    "        return shortRateShock(t, short)\n",
    "    \n",
    "    if shockScenario == 'ShortDown':\n",
    "        return shortRateShock(t, -short)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if shockScenario == 'Long':\n",
    "        return longRateShock(t, long)\n",
    "    \n",
    "    if shockScenario == 'LongUp':\n",
    "        return longRateShock(t, long)\n",
    "    \n",
    "    if shockScenario == 'LongDown':\n",
    "        return longRateShock(t, -long)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if shockScenario == 'Flatten':\n",
    "        return flattener(t, short, long)\n",
    "    \n",
    "    if shockScenario == 'Steepen':\n",
    "        return steepener(t, short, long)\n",
    "    \n",
    "    \n",
    "    raise InvalidArgument('I don\\'t know a scenario of the name \\'' + shockScenario + '\\'')\n",
    "\n",
    "\n",
    "\n",
    "# define the parameters for the shock scenarios\n",
    "\n",
    "shockParams = pd.DataFrame({'Currency': [], 'Parallel': [], 'Short': [], 'Long': []})\n",
    "shockParams = shockParams.append({'Currency': 'EUR', 'Parallel': 200, 'Short': 250, 'Long': 100}, ignore_index = True)\n",
    "shockParams = shockParams.append({'Currency': 'GBP', 'Parallel': 250, 'Short': 300, 'Long': 150}, ignore_index = True)\n",
    "shockParams = shockParams.append({'Currency': 'USD', 'Parallel': 200, 'Short': 300, 'Long': 150}, ignore_index = True)\n",
    "\n",
    "    \n",
    "########################################################################################################### \n",
    "\n",
    "\n",
    "def getShockedInterestRates(\n",
    "        refdate,\n",
    "        dates,\n",
    "        interestRates,\n",
    "        daycounter,\n",
    "        shockScenario,\n",
    "        parallel = 0,\n",
    "        short = 0,\n",
    "        long = 0):\n",
    "    if len(interestRates) != len(dates):\n",
    "        raise RangeMismatch('You need to provide an equal number of discount factors and sampling dates.')\n",
    "    \n",
    "    shockedInterestRates = []\n",
    "    \n",
    "    for i in range(len(dates)):\n",
    "        t = daycounter.yf(refdate, dates[i])\n",
    "        rate = interestRates[i] + getShockValue(t, shockScenario, parallel, short, long)\n",
    "        shockedInterestRates.append(rate)\n",
    "    \n",
    "    #print(shockedInterestRates)\n",
    "    \n",
    "    return shockedInterestRates\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "# Define a function, which shifts a discount curve according to the shock scenarios\n",
    "# We start by defining a function, which shifts a vector of discount factors\n",
    "\n",
    "def getShockedDiscountFactors(\n",
    "        refdate,\n",
    "        dates,\n",
    "        interestRates,\n",
    "        daycounter,\n",
    "        shockScenario,\n",
    "        parallel = 0,\n",
    "        short = 0,\n",
    "        long = 0):\n",
    "    \n",
    "    if len(interestRates) != len(dates):\n",
    "        raise RangeMismatch('You need to provide an equal number of discount factors and sampling dates.')\n",
    "    \n",
    "    shockedInteresRates = getShockedInterestRates(\n",
    "        refdate,\n",
    "        dates,\n",
    "        interestRates,\n",
    "        daycounter,\n",
    "        shockScenario,\n",
    "        parallel,\n",
    "        short,\n",
    "        long\n",
    "    )    \n",
    "    \n",
    "    shockedDiscountFactors = []\n",
    "    #shockedDiscountFactors = analytics.vectorDouble(len(dates))\n",
    "    #print(shockedDiscountFactors)\n",
    "    \n",
    "    for i in range(len(dates)):\n",
    "        t = daycounter.yf(refdate, dates[i])\n",
    "        rate = shockedInteresRates[i]/100 # are given in percent -> convert to decimal number\n",
    "        shockedDiscountFactors.append(math.exp(-t*rate))\n",
    "        #shockedDiscountFactors[i] = discountFactors[i] + getShockValue(t, shockScenario, parallel, short, long)\n",
    "        #print(shockedDiscountFactors)\n",
    "    \n",
    "    #print(\"//////////////////////////////////////////////\")\n",
    "    #print(shockedInteresRates)\n",
    "    #print(\"-----------------------\")\n",
    "    #print(shockedDiscountFactors)\n",
    "    #print(\"//////////////////////////////////////////////\")\n",
    "    \n",
    "    return shockedDiscountFactors\n",
    "    \n",
    "    \n",
    "    \n",
    "# We now use these shifted discount factors to construct shifted discount curves     \n",
    "    \n",
    "\n",
    "def getShockedDiscountCurve(\n",
    "        name,\n",
    "        refdate,\n",
    "        dates,\n",
    "        interestRates,\n",
    "        daycounterType,\n",
    "        interpolationType,\n",
    "        extrapolationType,\n",
    "        shockScenario,\n",
    "        parallel = 0,\n",
    "        short = 0,\n",
    "        long = 0):\n",
    "    \n",
    "    shockedDFs = getShockedDiscountFactors(\n",
    "        refdate,\n",
    "        dates,\n",
    "        interestRates,\n",
    "        analytics.DayCounter(daycounterType),\n",
    "        shockScenario,\n",
    "        parallel,\n",
    "        short,\n",
    "        long\n",
    "    )    \n",
    "    \n",
    "    #print(shockedDFs)\n",
    "    \n",
    "    return analytics.DiscountCurve(\n",
    "        name,\n",
    "        refdate,\n",
    "        dates,\n",
    "        shockedDFs,\n",
    "        daycounterType,\n",
    "        interpolationType,\n",
    "        extrapolationType\n",
    "    )\n",
    "\n",
    "###########################################################################################################    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## The scenarios\n",
    "### Plot the shock scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_shock_scenarios(\n",
    "    samplingPoints,\n",
    "    shockParams\n",
    "):\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    ax = fig.gca()\n",
    "\n",
    "    currency = 'EUR'\n",
    "    parallel = shockParams.loc[shockParams['Currency'] == currency].loc[0]['Parallel']\n",
    "    short = shockParams.loc[shockParams['Currency'] == currency].loc[0]['Short']\n",
    "    long = shockParams.loc[shockParams['Currency'] == currency].loc[0]['Long']\n",
    "\n",
    "    #print(type(samplingPoints))\n",
    "\n",
    "    #print(shortRateShock(short, samplingPoints))\n",
    "    hline = ax.axhline(y=0, xmin=0, xmax=1, linewidth=1, zorder = 30)\n",
    "    ax.plot(samplingPoints, [parallel]*len(samplingPoints), '.-', label = 'ParallelUp', zorder = 20)\n",
    "    ax.plot(samplingPoints, [-parallel]*len(samplingPoints), '.-', label = 'ParallelDown', zorder = 20)\n",
    "    ax.plot(samplingPoints, shortRateShock(samplingPoints, short), '.-', label = 'ShortUp', zorder = 20)\n",
    "    ax.plot(samplingPoints, shortRateShock(samplingPoints, -short), '.-', label = 'ShortDown', zorder = 20)\n",
    "    ax.plot(samplingPoints, longRateShock(samplingPoints, long), '.-', label = 'LongUp', zorder = 20)\n",
    "    ax.plot(samplingPoints, longRateShock(samplingPoints, -long), '.-', label = 'LongDown', zorder = 20)\n",
    "    ax.plot(samplingPoints, flattener(samplingPoints, short, long), '.-', label = 'Flattener', zorder = 20)\n",
    "    ax.plot(samplingPoints, steepener(samplingPoints, short, long), '.-', label = 'Steepener', zorder = 20)\n",
    "\n",
    "\n",
    "    plt.grid(alpha=grid_alpha) \n",
    "    plt.xlabel('Expiry (in years)')\n",
    "    plt.ylabel('Interest rate (in base points)')\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.legend(loc='lower right').set_zorder(100)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_shock_scenarios(sampling_points_EONIA_yf, shockParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_shocked_interest_rates(\n",
    "    refdate,\n",
    "    dates,\n",
    "    interestRates,\n",
    "    daycounter\n",
    "):\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    ax = fig.gca()\n",
    "\n",
    "    currency = 'EUR'\n",
    "    parallel = shockParams.loc[shockParams['Currency'] == currency].loc[0]['Parallel']\n",
    "    short = shockParams.loc[shockParams['Currency'] == currency].loc[0]['Short']\n",
    "    long = shockParams.loc[shockParams['Currency'] == currency].loc[0]['Long']\n",
    "\n",
    "    #print(type(samplingPoints))\n",
    "\n",
    "    #print(shortRateShock(short, samplingPoints))\n",
    "    hline = ax.axhline(y=0, xmin=0, xmax=1, linewidth=1, zorder = 30)\n",
    "    shockScenarios = ['ParallelUp', 'ParallelDown', 'ShortUp', 'ShortDown', 'LongUp', 'LongDown', 'Flatten', 'Steepen']\n",
    "    for shockScenario in shockScenarios:\n",
    "        ir = getShockedInterestRates(\n",
    "            refdate,\n",
    "            dates,\n",
    "            interestRates,\n",
    "            daycounter,\n",
    "            shockScenario,\n",
    "            parallel,\n",
    "            short,\n",
    "            long\n",
    "        )\n",
    "        year_fractions = []\n",
    "        for i in range(len(dates)):\n",
    "            year_fractions.append(daycounter.yf(refdate, dates[i]))\n",
    "        \n",
    "        ax.plot(year_fractions, ir, '.-', label = shockScenario, zorder = 20)\n",
    "\n",
    "    \n",
    "    plt.grid(alpha=grid_alpha)    \n",
    "    plt.xlabel('Expiry (in years)')\n",
    "    plt.ylabel('Interest rate (in base points)')\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.legend(loc='lower right').set_zorder(100)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "interestRates = [0] * len(sampling_points_EONIA_dates)\n",
    "\n",
    "plot_shocked_interest_rates(\n",
    "    refdate,\n",
    "    sampling_points_EONIA_dates,\n",
    "    interestRates,\n",
    "    analytics.DayCounter(daycounter_type_standard)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Plot the shock scenarios (discount factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_shocked_discount_factors(\n",
    "    refdate,\n",
    "    dates,\n",
    "    interestRates,\n",
    "    daycounter\n",
    "):\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    ax = fig.gca()\n",
    "\n",
    "    currency = 'EUR'\n",
    "    parallel = shockParams.loc[shockParams['Currency'] == currency].loc[0]['Parallel']\n",
    "    short = shockParams.loc[shockParams['Currency'] == currency].loc[0]['Short']\n",
    "    long = shockParams.loc[shockParams['Currency'] == currency].loc[0]['Long']\n",
    "\n",
    "    #print(type(samplingPoints))\n",
    "\n",
    "    #print(shortRateShock(short, samplingPoints))\n",
    "    hline = ax.axhline(y=1, xmin=0, xmax=1, linewidth=1, zorder = 30)\n",
    "    shockScenarios = ['ParallelUp', 'ParallelDown', 'ShortUp', 'ShortDown', 'LongUp', 'LongDown', 'Flatten', 'Steepen']\n",
    "    for shockScenario in shockScenarios:\n",
    "        df = getShockedDiscountFactors(\n",
    "            refdate,\n",
    "            dates,\n",
    "            interestRates,\n",
    "            daycounter,\n",
    "            shockScenario,\n",
    "            parallel/100,\n",
    "            short/100,\n",
    "            long/100\n",
    "        )\n",
    "        year_fractions = []\n",
    "        for i in range(len(dates)):\n",
    "            year_fractions.append(daycounter.yf(refdate, dates[i]))\n",
    "        \n",
    "        ax.plot(year_fractions, df, '.-', label = shockScenario, zorder = 20)\n",
    "\n",
    "    \n",
    "    plt.grid(alpha=grid_alpha) \n",
    "    plt.xlabel('Expiry (in years)')\n",
    "    plt.ylabel('Discount factor')\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.legend(loc='lower right').set_zorder(100)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "interestRates = [0] * len(sampling_points_EONIA_dates)\n",
    "\n",
    "plot_shocked_discount_factors(\n",
    "    refdate,\n",
    "    sampling_points_EONIA_dates,\n",
    "    interestRates,\n",
    "    analytics.DayCounter(daycounter_type_standard)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Plot them again, using discount curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_shocked_discount_curves(\n",
    "    name,\n",
    "    refdate,\n",
    "    dates,\n",
    "    discountFactors,\n",
    "    daycounterType,\n",
    "    interpolationType,\n",
    "    extrapolationType,\n",
    "    shockParams\n",
    "):\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    ax = fig.gca()\n",
    "\n",
    "    currency = 'EUR'\n",
    "    parallel = shockParams.loc[shockParams['Currency'] == currency].loc[0]['Parallel']\n",
    "    short = shockParams.loc[shockParams['Currency'] == currency].loc[0]['Short']\n",
    "    long = shockParams.loc[shockParams['Currency'] == currency].loc[0]['Long']\n",
    "    \n",
    "    daycounter = analytics.DayCounter(daycounterType)\n",
    "\n",
    "    #print(type(samplingPoints))\n",
    "\n",
    "    #print(shortRateShock(short, samplingPoints))\n",
    "    hline = ax.axhline(y=1, xmin=0, xmax=1, linewidth=1, zorder = 30)\n",
    "    shockScenarios = ['ParallelUp', 'ParallelDown', 'ShortUp', 'ShortDown', 'LongUp', 'LongDown', 'Flatten', 'Steepen']\n",
    "    for shockScenario in shockScenarios:\n",
    "        dc = getShockedDiscountCurve(\n",
    "            name + '_' + shockScenario,\n",
    "            refdate,\n",
    "            dates,\n",
    "            discountFactors,\n",
    "            daycounterType,\n",
    "            interpolationType,\n",
    "            extrapolationType,\n",
    "            shockScenario,\n",
    "            parallel/100,\n",
    "            short/100,\n",
    "            long/100\n",
    "        )\n",
    "        \n",
    "        year_fractions = []\n",
    "        for i in range(len(dates)):\n",
    "            year_fractions.append(daycounter.yf(refdate, dates[i]))\n",
    "        values = analytics.vectorDouble()\n",
    "        dc.value(values, refdate, dates)\n",
    "            \n",
    "        ax.plot(year_fractions, values, '.-', label = shockScenario, zorder = 20)\n",
    "\n",
    "        \n",
    "    plt.grid(alpha=grid_alpha) \n",
    "    plt.xlabel('Expiry (in years)')\n",
    "    plt.ylabel('Discount factor')\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.legend(loc='lower right').set_zorder(100)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "plot_shocked_discount_curves(\n",
    "    name = 'dc_linear',\n",
    "    refdate = refdate,\n",
    "    dates = sampling_points_EONIA_dates,\n",
    "    discountFactors = [0.0] * len(sampling_points_EONIA_dates),\n",
    "    daycounterType = daycounter_type_standard,\n",
    "    interpolationType = interpolation_type_standard,\n",
    "    extrapolationType = extrapolation_type_standard,\n",
    "    shockParams = shockParams\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Compute the change in value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Compute the price of our portfolio\n",
    "# Repeat for every scenario\n",
    "results_dirty = []\n",
    "results_clean = []\n",
    "results_dirty_bondonly = []\n",
    "results_clean_bondonly = []\n",
    "\n",
    "currency = 'EUR'\n",
    "parallel = shockParams.loc[shockParams['Currency'] == currency].loc[0]['Parallel']\n",
    "short = shockParams.loc[shockParams['Currency'] == currency].loc[0]['Short']\n",
    "long = shockParams.loc[shockParams['Currency'] == currency].loc[0]['Long']\n",
    "    \n",
    "shockScenarios = ['ParallelUp', 'ParallelDown', 'ShortUp', 'ShortDown', 'LongUp', 'LongDown', 'Flatten', 'Steepen']\n",
    "\n",
    "for shockScenario in shockScenarios:\n",
    "    dcEONIA = getShockedDiscountCurve(\n",
    "        'dc_linear',\n",
    "        refdate,\n",
    "        sampling_points_EONIA_dates,\n",
    "        data_EONIA_current,\n",
    "        daycounter_type_standard,\n",
    "        interpolation_type_standard,\n",
    "        extrapolation_type_standard,\n",
    "        shockScenario,\n",
    "        parallel/100,\n",
    "        short/100,\n",
    "        long/100\n",
    "    )\n",
    "    \n",
    "#     dcWithSpread = getShockedDiscountCurve(\n",
    "#          'dc_linear_spread',\n",
    "#          refdate,\n",
    "#          sampling_points_EONIA_dates,\n",
    "#          data_EONIA_current + creditspread,\n",
    "#          daycounter_type_standard,\n",
    "#          interpolation_type_standard,\n",
    "#          extrapolation_type_standard,\n",
    "#          shockScenario,\n",
    "#          parallel/100,\n",
    "#          short/100,\n",
    "#          long/100\n",
    "#      )\n",
    "    \n",
    "    pricing_data_simple.discountCurve = dcEONIA # dcWithSpread\n",
    "    pay_leg_pricing_data.discountCurve = dcEONIA\n",
    "    rec_leg_pricing_data.discountCurve = dcEONIA\n",
    "    rec_leg_pricing_data.fixingCurve = dcEONIA\n",
    "    \n",
    "    prBond = pyvacon.pricing.price(pricing_data_simple)\n",
    "    prSwap = analytics.price(ir_swap_pricing_data)\n",
    "    dirty = prBond.getPrice() + prSwap.getPrice()\n",
    "    clean = prBond.getCleanPrice() + prSwap.getCleanPrice()\n",
    "    results_dirty.append(dirty)\n",
    "    results_clean.append(clean)\n",
    "    results_dirty_bondonly.append(prBond.getPrice())\n",
    "    results_clean_bondonly.append(prBond.getCleanPrice())\n",
    "    #print(pricing_data_simple.spec.getObjectId() + ', dirty price: ' + str(results.getPrice()) + \",  clean price: \" + str(results.getCleanPrice()))\n",
    "#print(results_dirty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Plot the change in value (comparison between our entire portfolio and the bond on its own)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Histogramm of the changes/differences in value\n",
    "valDiffsDirty = np.asarray([res - currentValue for res in results_dirty])\n",
    "valDiffsDirtyBondOnly = np.asarray([res - currentValueBond for res in results_dirty_bondonly])\n",
    "fig, (ax1, ax2) = plt.subplots(2, figsize=(16,8))\n",
    "#fig = plt.figure(figsize=(16,8))\n",
    "#ax = fig.gca()\n",
    "\n",
    "# print(currentValue)\n",
    "# print(currentValueBond)\n",
    "#print(results_clean)\n",
    "#print(results_dirty)\n",
    "\n",
    "ax1.grid(alpha=grid_alpha) \n",
    "ax1.bar(shockScenarios, valDiffsDirty/currentValue*100, color=color_main, zorder = 40)\n",
    "ax1.axhline(y=0, xmin=0, xmax=1, color=color_highlight, zorder = 30)\n",
    "ax1.set_ylabel('Change in portfolio value [%]')\n",
    "\n",
    "ax2.grid(alpha=grid_alpha) \n",
    "ax2.bar(shockScenarios, valDiffsDirtyBondOnly/currentValueBond*100, color=color_main, zorder = 40)\n",
    "ax2.axhline(y=0, xmin=0, xmax=1, color=color_highlight, zorder = 30)\n",
    "ax2.set_ylabel('Change in bond value [%]')\n",
    "\n",
    "#fig.text(0.075, 0.5, 'Change in value [%]', ha='center', va='center', rotation='vertical')\n",
    "fig.text(0.5, 0.06, 'Shock Scenario', ha='center', va='center')\n",
    "#plt.ylabel('Change in portfolio value [%]')\n",
    "#plt.xlabel('Shock Scenario')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bootstrap zero-coupon interest rates\n",
    "- 'Einheit' der Zinssaetze mit an die Funktionen uebergeben (?) (dezimal, percent, basepoint)\n",
    "- Das Wort 'current' als Beschreibung für die jüngste in den Marktdaten vorhandene EONIA-Kurve ueberdenken\n",
    "- Actually order the historical data by date ascending (instead of descending) to avoid confusion\n",
    "- Illustrate the definition of VaR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "268.253px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
